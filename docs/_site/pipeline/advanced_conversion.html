<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="icon" href="/assets/images/favicon.ico" type="image/x-icon"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Advanced conversion | Onboarding tutorial</title> <meta name="generator" content="Jekyll v3.9.3" /> <meta property="og:title" content="Advanced conversion" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Publishing as a Standard Open Linked Data Model This quick start guide will show you how to create a more advanced processing pipeline in the LDIO Workbench for converting our example model to a standard open vocabulary and to publish that as a Linked Data Event Stream (LDES). Please see the introduction for the example data set and pre-requisites, as well as an overview of all examples. Copy &amp; Paste Rules! To kickstart this tutorial we can use the basic setup tutorial. For the server we only will need to change the actual model. Everything else can stay the same: we will still need to volume mount the server configuration file and provide the database connection string (which we have changed to reflect our tutorial). The workbench is where we need to change a few things: we’ll need to transform our custom model to the standard vocabulary. To make it a bit more interesting we’ll start from an actual real-time message which contains more than one state object. In fact, we’ll be checking for changes on a regular basis. Now we have a real linked data event stream! Towards a More Advanced Model As mentioned above, we’ll be using an open vocabulary standard to describe our model. This allows us to attach real semantic meaning to it and create interoperability with other Data Publishers that use the same vocabulary. Understanding and mapping our source model (check out the dataset schema) to the target model is the hard part, in particular if we are missing descriptions for the model structure and its properties. Lucky for us, most of the property names are more-or-less self-explanatory. source property meaning name descriptive name lastupdate timestamp when last updated type type of parking facility, always offStreetParkingGround openingtimesdescription description of opening times isopennow is parking currently open? specified as boolean: yes = 1, no = 0 temporaryclosed is parking temporary closed? (boolean) operatorinformation description of company operating the parking freeparking is parking freely accessable? (boolean) urllinkaddress webpage URL of the parking offering more information numberofspaces total number of spaces (capacity) availablespaces available number of spaces occupancytrend ? occupation amount of occupied spaces expressed as a rounded percentage of the capacity latitude north-south position of the parking longitude east-west position of the parking location.lon same as longitude but expressed as a number location.lat same as latitude but expressed as a number gentse_feesten ? As you can see, except for a few, we have a pretty good idea of the meaning of the properties. Obviously we should double-check our assumptions with the publisher of this data. For this tutorial, we’ll assume that the meaning is correct and that we can ignore the few unclear properties. So, let’s continue by looking into how these properties map onto the Mobivoc model. Looking at the Mobivoc model we notice a central entity named parking facility. It derives from a civic structure inheriting its properties. We can also see that there is an entity parking lot derived from a parking facility and is essentially the same as our offstreet parking ground. Following the civic structure relations we see that it can have a capacity and a real time capacity (derived from capacity). Exactly what we need! Furthermore, a capacity is valid for a vehicle type, a civic structure has an opening hours specification and is operated by an organization. Let’s create a mapping from our source model to the target model based on this knowledge. Note that for readability we use well-known abbreviations for the namespaces used in the properties and values. source target name value rdfs:label value lastupdate value dct:modified value type offStreetParkingGround rdf:type mv:ParkingLot openingtimesdescription value schema:openingHoursSpecification [rdf:type schema:OpeningHoursSpecification; rdfs:label value] isopennow value N/A temporaryclosed value N/A operatorinformation value mv:operatedBy [rdf:type schema:Organization, dct:Agent; rdfs:label value] freeparking value mv:price [rdf:type schema:PriceSpecification; mv:freeOfCharge value] urllinkaddress value mv:url value numberofspaces value mv:capacity [rdf:type mv:Capacity; mv:totalCapacity value] availablespaces value mv:capacity [rdf:type mv:RealTimeCapacity; mv:currentValue value] occupancytrend value N/A occupation value mv:rateOfOccupancy value latitude value geo:lat value longitude value geo:long value location.lon N/A location.lat N/A gentse_feesten value N/A Note that we mark some mappings as not applicable (N/A) because we cannot map a property, we do not known the exact meaning of the property or we do not need it (e.g. duplicates). Great! We have determined what will be mapped and how. We’re done. Well, not quite. There is one more thing we need: an identity for our entity. It has to be an URI and obviously it needs to be unique. In addition, for every update of the available spaces the identity should remain the same (duh!). So, what do we use for the identity? One possible option is to take the urllinkaddress value. It would work as long as the Data Owner does not decide to relocate it. Best option is to check with the Data Owner but for this tutorial we’ll continue on the assumption that the urllinkaddress will not change. To Push or To Pull, That’s the Question As mention above, to make it more interesting we will be retrieving the number of available spaces in our parking lots on a regular interval. To do so we can use a component that can poll one or more URLs using HTTP. To do so, we need to replace the LdioHttpIn component (push model) that listens for incoming HTTP requests by a LdioHttpInPoller component (pull model). For example, to poll our source URL every two minutes we need to configure our pipeline input as: input: name: be.vlaanderen.informatievlaanderen.ldes.ldio.LdioHttpInPoller config: url: https://data.stad.gent/api/explore/v2.1/catalog/datasets/real-time-bezetting-pr-gent/exports/csv?lang=en&amp;timezone=Europe%2FBrussels interval: PT1M This will ensure we receive the actual state of our parking lots at regular time intervals, which may or may not have changed since the last time we checked. Note that we request the data as CSV but alternatively we could have used JSON or GeoJSON. We still need to configure an adapter to convert the received CSV message to linked data. We’ll do that next. With a Little Help From Our Pirates Now that we can get the actual state of our parking lots, we need to convert the source message in semicolon (;) separated CSV format to the linked data models we defined in the mapping. For this we can use a technology called RDF Mapping Language (RML). There are various ways to produce the mapping that we need: directly using linked data which defines the RML mapping rules or indirectly using a more human readable way named Yarrrml. Personally I prefer the real thing but using Matey may be more your thing. Explaining the RML technology is beyond the scope of this tutorial. The technology allows us to convert formats such as CSV, XML and JSON into complex RDF models. However, using it to create these complex models can be quite challenging. The solution is to do a straight forward mapping and create the structure we need in a second phase. Basically we map the properties of our source model one-to-one into an intermediate linked data model and then transform this intermediate model into our final model using another RDF technology (SPARQL construct), which is way easier to use for creating complex structures. We’ll do this in the next section. We start by creating a simple intermediate model where we already set the correct identity and entity type but map everything else as-is onto an intermediate vocabulary (temp: or https://temp.org/ns/advanced-compose# in full). source intermediate name value temp:name value lastupdate value temp:lastupdate value type offStreetParkingGround rdf:type mv:ParkingLot openingtimesdescription value temp:openingtimesdescription value operatorinformation value temp:operatorinformation value freeparking value temp:freeparking value urllinkaddress id id numberofspaces value temp:numberofspaces value availablespaces value temp:availablespaces value occupation value temp:occupation value latitude value temp:latitude value longitude value temp:longitude value To create a RML mapping file we need to write the RML rules in Turtle. All the Turtle prefixes should go at the start of the file but for simplicity we’ll add the prefixes as we go. Let’s start with the most common ones: @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . @prefix rml: &lt;http://semweb.mmlab.be/ns/rml#&gt; . @prefix rr: &lt;http://www.w3.org/ns/r2rml#&gt; . @prefix ql: &lt;http://semweb.mmlab.be/ns/ql#&gt; . @prefix carml: &lt;http://carml.taxonic.com/carml/&gt; . Note that the last one is always needed for our RML adapter component (RmlAdaptor). Now, we start by defining the map which will contain our mapping rules. We define a prefix for our map and rules (:) and tell the RML component that we will be mapping CSV messages. Do not forget that all prefixes go at the start before our mapping and rules. @prefix : &lt;https://example.org/ns/tutorial/advanced-conversion#&gt; . :TriplesMap a rr:TriplesMap; rml:logicalSource [ a rml:LogicalSource; rml:source [ a carml:Stream ]; rml:referenceFormulation ql:CSV ]. Let’s continue now with defining the identity and type of our parking lots. Remember that for the identity we use the URL value and for the type we use parking lot. At the same time we’ll also add each parking lot in its own graph. Say what? We’ll learn about triples and graphs a bit later. For now, just remember that we want to handle each parking lot separately so we instruct the RML component to generate a stream of mv:ParkingLot entities, one for each row in the CSV. @prefix mv: &lt;http://schema.mobivoc.org/#&gt; . :TriplesMap rr:subjectMap [ rr:graphMap [ rr:template &quot;{urllinkaddress}&quot; ]; rml:reference &quot;urllinkaddress&quot;; rr:class mv:ParkingLot ]. Easy enough. No? Let’s continue with one property. We define a rule saying that the entity will have a property (predicate) named temp:name whose value (object) comes from the source property name. @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix temp: &lt;https://temp.org/ns/advanced-compose#&gt; . :TriplesMap rr:predicateObjectMap [ rr:predicate temp:name; rr:objectMap [ rml:reference &quot;name&quot; ] ]. Again, no rocket-science once you get used to the Turtle and RML syntax. Let’s do the other properties as well. We define a rule to map each source property value onto the intermediate property. However, to make our life a bit easier in the next step, where we convert the intermediate to the target model, we can already add the correct value types. :TriplesMap rr:predicateObjectMap [ rr:predicate temp:lastupdate; rr:objectMap [ rml:reference &quot;lastupdate&quot;; rr:datatype xsd:dateTime ] ], [ rr:predicate temp:openingtimesdescription; rr:objectMap [ rml:reference &quot;openingtimesdescription&quot; ] ], [ rr:predicate temp:operatorinformation; rr:objectMap [ rml:reference &quot;operatorinformation&quot; ] ], [ rr:predicate temp:freeparking; rr:objectMap [ rml:reference &quot;freeparking&quot;; rr:datatype xsd:integer ] ], [ rr:predicate temp:numberofspaces; rr:objectMap [ rml:reference &quot;numberofspaces&quot;; rr:datatype xsd:integer ] ], [ rr:predicate temp:availablespaces; rr:objectMap [ rml:reference &quot;availablespaces&quot;; rr:datatype xsd:integer ] ], [ rr:predicate temp:occupation; rr:objectMap [ rml:reference &quot;occupation&quot;; rr:datatype xsd:integer ] ], [ rr:predicate temp:latitude; rr:objectMap [ rml:reference &quot;latitude&quot;; rr:datatype xsd:double ] ], [ rr:predicate temp:longitude; rr:objectMap [ rml:reference &quot;longitude&quot;; rr:datatype xsd:double ] ]. All of the above results in the mapping to intermediate file. In order to make it available to the workbench container we need to use volume mapping again. However, becomes we know we’ll need an additional file for transforming the intermediate to the target format, we choose to map the directory containing the mapping file as a whole: volumes: - ./workbench/config:/ldio/config:ro We also need to change our workbench pipeline to use the above RML mapping file and to include the RML mapping component (RmlAdapter instead of the JsonLdAdapter used in the basic setup). Our workbench pipeline input component is now complete and looks like this: input: name: be.vlaanderen.informatievlaanderen.ldes.ldio.LdioHttpInPoller config: url: https://data.stad.gent/api/explore/v2.1/catalog/datasets/real-time-bezetting-pr-gent/exports/csv?lang=en&amp;timezone=Europe%2FBrussels interval: PT1M adapter: name: be.vlaanderen.informatievlaanderen.ldes.ldi.RmlAdapter config: mapping: ./config/source-to-intermediate.ttl Using the Swiss Army Knife Now that we have an intermediate model as linked data we can use a SPARQL component which allows us to query the values in our intermediate model and construct a target model. If we look at our intermediate model and the target model we see that we need to keep the identity and type of our parking lot, convert the other properties to a different namespace and for some properties introduce the required structure: intermediate target temp:name value rdfs:label value temp:lastupdate value dct:modified value rdf:type mv:ParkingLot as-is temp:openingtimesdescription value schema:openingHoursSpecification [rdf:type schema:OpeningHoursSpecification; rdfs:label value] temp:operatorinformation value mv:operatedBy [rdf:type schema:Organization, dct:Agent; rdfs:label value] temp:freeparking value mv:price [rdf:type schema:PriceSpecification; mv:freeOfCharge value ] id mv:url id temp:numberofspaces value mv:capacity [rdf:type mv:Capacity; mv:totalCapacity value] temp:availablespaces value mv:capacity [rdf:type mv:RealTimeCapacity; mv:currentValue value] temp:occupation value mv:rateOfOccupancy value temp:latitude value geo:lat value temp:longitude value geo:long value So, let’s start with the an empty SPARQL construct query (which is similar to a SPARQL query but the result is a new RDF model not just some values). Again, we use Turtle to do this: # TODO: add our prefixes here CONSTRUCT { # TODO: add our target model here } WHERE { # TODO: select the intermediate model values here } Not too difficult to understand: in, the where part we select values from the intermediate model and put them in variables. We then use those variables to create the target model in the constructpart. Note that the casing of the words construct and where is not important. Let’s take a brief moment and look at the intermediate model. It is now linked data so we can look at this model as being a collection of id-property-value triples, where the id is the id of eack parking lot, the properties being the names in our temp: namespace and the values being values, obviously. In linked data we call this a triple. The S stands for subject (the identity of a thing), the P stands for predicate (a property identified by its unique full name, including the namespace) and the O stands for object (the value which can be literal or a reference to some other subject, with or without an actual identity). Conceptually, a triple is a way to represent a unidirectional, named relation between a subject and an object. In linked data we also define the concept of a graph, which is just a tag for a triple and as such basically a way of grouping a bunch of triples together. It has no implicit meaning. A graph can be named by having an URI which identifies it. There’s also one special unnamed graph (has no URI) which we call the default graph. When we add a graph part to a triple (SPO) we get a quad (SPOG). In fact, triples are just a special case of quads where the 4th component is the default graph. We can use graphs for many purposes, e.g. to identify the source of the triples, to group together all related entities, etc. But, we use graphs in our pipelines to split data containing multiple entities into a stream of entities that are processed one-by-one in the pipeline. Wow, let’s think about this for a moment: in linked data we model everything as a collection of (subject-predicate-object) triples. It allows us to look at SPARQL queries as being filters that select a subset of the triple collection. For example, if we need to select all the identities of our parking lots we can simply state that we look for all the triples for which the predicate is rdf:type and the object is mv:ParkingLot. The subjects of these triples are in fact what we search for: the identities. To express this in a SPARQL query we specify this as follows: ?id &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://schema.mobivoc.org/#ParkingLot&gt; The interesting part is the variable ?id. It represents each result in our query. To make it more readable and in order to not repeat the namespace in every subject, predicate and object, we can again use prefixes: PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; PREFIX mv: &lt;http://schema.mobivoc.org/#&gt; ?id rdf:type mv:ParkingLot Note that the syntax for our prefix definitions is slightly different: we use PREFIX instead of @prefix and there is no dot (.) at the end of the line. The full SPARQL query would be: PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; PREFIX mv: &lt;http://schema.mobivoc.org/#&gt; SELECT ?id WHERE { ?id rdf:type mv:ParkingLot } But we do not need the identities only. Instead we want to create a new collection of triples for each parking lot with the predicates changed to those needed by our target model. Let’s start with simply copying the triple that defines our parking lots and their update timestamp: PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; PREFIX mv: &lt;http://schema.mobivoc.org/#&gt; PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; PREFIX temp: &lt;https://temp.org/ns/advanced-compose#&gt; CONSTRUCT { ?id a mv:ParkingLot . ?id dct:modified ?lastupdate . } WHERE { ?id rdf:type mv:ParkingLot . ?id temp:lastupdate ?lastupdate . } Note that a is a short-hand notation of rdf:type. In the where part we look for each ?id which is a mv:ParkingLot and then we retrieve its value of temp:lastupdate as variable ?lastupdate. Now that we have found these we can use the variables to create a new set of triples listed under the construct part. Not too difficult, is it? Our target model is a bit more structured that our intermediate model, so at times we need to introduce an intermediate relation to some structure. Take for example the capacity. In the model diagram, we see that a civic structure has a relation has capacity to a Capacity object that has a property total capacity. In linked data we model this as triples in this way: &lt;civic-structure-id&gt; rdf:type schema:CivicStructure . &lt;civic-structure-id&gt; mv:capacity &lt;a-capacity&gt; . &lt;a-capacity&gt; rdf:type mv:Capacity . &lt;a-capacity&gt; mv:totalCapacity &quot;some-numeric-value&quot;^^xsd:integer . We see a very interesting difference between the civic structure and its capacity: a civic-structure has some unique identifier such as “https://example.org/id/civic-structure/parking-lot-1” but a capacity is something that does not exist on its own, it is part of the civic structure and does not have an identity of its own. In linked data we call this a blank node because we can represent a triple as two nodes connected by a directed arrow, where the start node is a subject, the arrow is the predicate and the end node is the object. A blank node is a node without an identity and can be both the source and the destination of one or more arrows, representing its relations aka. properties aka. predicates. Because a blank node has no identity, we can write the above a bit more condensed by dropping the meaningless &lt;a-capacity&gt; and separating the predicates of the same subject by a semi-colon (;) - formatted for clarity: &lt;civic-structure-id&gt; rdf:type schema:CivicStructure ; mv:capacity [ rdf:type mv:Capacity ; mv:totalCapacity &quot;some-numeric-value&quot;^^xsd:integer ]. Note that [ ... ] now represents our capacity. Now that we have learned how to introduce structure in our target model we can create the complete mapping using SPARQL construct and we need to add this transformation step in the workbench pipeline: transformers: - name: be.vlaanderen.informatievlaanderen.ldes.ldi.SparqlConstructTransformer config: query: ./config/intermediate-to-target.rq In addition, as our target model has changed, we need to fix the transformation step which creates the version object to: - name: be.vlaanderen.informatievlaanderen.ldes.ldi.VersionObjectCreator config: member-type: http://schema.mobivoc.org/#ParkingLot delimiter: &quot;/&quot; date-observed-property: &lt;http://purl.org/dc/terms/modified&gt; generatedAt-property: http://purl.org/dc/terms/modified versionOf-property: http://purl.org/dc/terms/isVersionOf And also the definition of the LDES to reflect the correct target class: @prefix mv: &lt;http://schema.mobivoc.org/#&gt; &lt;/occupancy&gt; a ldes:EventStream ; tree:shape [ a sh:NodeShape ; sh:targetClass mv:ParkingLot ] ; Note that in the complete mapping when we query the properties from our source model that almost all of these query lines are wrapped by an optional { ... } construct. The reason for this is that any of these triples may be missing. Remember that the WHERE clause is in essence a filter on the collection of source triples, where each query line refines the subset of results from the previous query line. Therefore, if we do not use optional then the query returns no results and hence no target entity is constructed. Enough Talk, Show Me the Members Now that we have set everything up, we can launch our systems. We cannot launch both our LDES server and our workbench at the same time because we are now polling for the data and our workbench pipeline will start immediately. This is a problem because we first need to send the definition of our LDES and view to the LDES server. Actually, it takes our LDES server longer to start than our workbench, so we need to prevent launching the workbench until our LDES server is up and running and we have seeded our definitions. To prevent our workbench to launch when we bring all our other systems up, we can add a profile to the Docker compose file in the workbench service. The actual name of the profile does not matter but we use delay-started to clearly communicate the purpose: ldio-workbench: container_name: advanced-conversion_ldio-workbench image: ghcr.io/informatievlaanderen/ldi-orchestrator:latest volumes: - ./workbench/config:/ldio/config:ro - ./workbench/application.yml:/ldio/application.yml:ro ports: - 9004:80 networks: - advanced-conversion profiles: - delay-started To run the LDES server and its storage service (mongo), wait until its up and running, send definitions and then start the workbench: clear docker compose up -d while ! docker logs $(docker ps -q -f &quot;name=ldes-server$&quot;) 2&gt; /dev/null | grep &#39;Started Application in&#39; ; do sleep 1; done curl -X POST -H &quot;content-type: text/turtle&quot; &quot;http://localhost:9003/ldes/admin/api/v1/eventstreams&quot; -d &quot;@./definitions/occupancy.ttl&quot; curl -X POST -H &quot;content-type: text/turtle&quot; &quot;http://localhost:9003/ldes/admin/api/v1/eventstreams/occupancy/views&quot; -d &quot;@./definitions/occupancy.by-page.ttl&quot; docker compose up ldio-workbench -d while ! docker logs $(docker ps -q -f &quot;name=ldio-workbench$&quot;) 2&gt; /dev/null | grep &#39;Started Application in&#39; ; do sleep 1; done To verify the LDES, view and data: clear curl http://localhost:9003/ldes/occupancy curl http://localhost:9003/ldes/occupancy/by-page curl http://localhost:9003/ldes/occupancy/by-page?pageNumber=1 The last URL will contain our members, looking something like this (limited to one member): @prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix park-and-ride-pr: &lt;https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/&gt; . @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix terms: &lt;http://purl.org/dc/terms/&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt; . @prefix wgs84_pos: &lt;http://www.w3.org/2003/01/geo/wgs84_pos#&gt; . &lt;https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/pr-loopexpo#2023-12-13T12:21:21+01:00&gt; rdf:type &lt;http://schema.mobivoc.org/#ParkingLot&gt; ; rdfs:label &quot;P+R The Loop&quot; ; terms:isVersionOf park-and-ride-pr:pr-loopexpo ; terms:modified &quot;2023-12-13T12:21:21+01:00&quot;^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ; &lt;http://schema.mobivoc.org/#capacity&gt; [ rdf:type &lt;http://schema.mobivoc.org/#Capacity&gt; ; &lt;http://schema.mobivoc.org/#totalCapacity&gt; 168 ] ; &lt;http://schema.mobivoc.org/#capacity&gt; [ rdf:type &lt;http://schema.mobivoc.org/#RealTimeCapacity&gt; ; &lt;http://schema.mobivoc.org/#currentValue&gt; 30 ] ; &lt;http://schema.mobivoc.org/#operatedBy&gt; [ rdf:type terms:Agent , &lt;http://schema.org/Organization&gt; ; rdfs:label &quot;Mobiliteitsbedrijf Gent&quot; ] ; &lt;http://schema.mobivoc.org/#price&gt; [ rdf:type &lt;http://schema.org/PriceSpecification&gt; ; &lt;http://schema.mobivoc.org/#freeOfCharge&gt; 1 ] ; &lt;http://schema.mobivoc.org/#rateOfOccupancy&gt; 82 ; &lt;http://schema.mobivoc.org/#url&gt; park-and-ride-pr:pr-loopexpo ; &lt;http://schema.org/openingHoursSpecification&gt; [ rdf:type &lt;http://schema.org/OpeningHoursSpecification&gt; ; rdfs:label &quot;24/7&quot; ] ; wgs84_pos:lat &quot;51.024483197&quot;^^&lt;http://www.w3.org/2001/XMLSchema#double&gt; ; wgs84_pos:long &quot;3.69519252261&quot;^^&lt;http://www.w3.org/2001/XMLSchema#double&gt; . ... &lt;http://localhost:9003/ldes/occupancy&gt; rdf:type ldes:EventStream ; tree:member &lt;https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/pr-loopexpo#2023-12-13T12:21:21+01:00&gt;, ... ... Note that every two minutes the pipeline will request the latest state of our parking lots and will create additional version objects. The identity of a member depends only on the lastupdate property of our parking lot. If that did not change for a parking lot then the pipeline will create a version object with an identical identity as before. Any such version object will be refused by the LDES server and a warning will be logged in the LDES server log. The new version objects are added to the LDES and become new members. Every End is a New Beginning You should now know some basics about linked data. You learned how to define a mapping from non-linked data to linked data using RML as well as how to transform a linked data model into a different linked data model. In addition, you learned that you can periodically pull data into a workbench pipeline to create a continuous stream of versions of the state of some system. You can now stop all the systems. To bring the containers down and remove the private network: docker compose rm ldio-workbench --stop --force --volumes docker compose down Note that to bring down the workbench we need to stop it and remove the container and associated volumes explicitly because we started it separately." /> <meta property="og:description" content="Publishing as a Standard Open Linked Data Model This quick start guide will show you how to create a more advanced processing pipeline in the LDIO Workbench for converting our example model to a standard open vocabulary and to publish that as a Linked Data Event Stream (LDES). Please see the introduction for the example data set and pre-requisites, as well as an overview of all examples. Copy &amp; Paste Rules! To kickstart this tutorial we can use the basic setup tutorial. For the server we only will need to change the actual model. Everything else can stay the same: we will still need to volume mount the server configuration file and provide the database connection string (which we have changed to reflect our tutorial). The workbench is where we need to change a few things: we’ll need to transform our custom model to the standard vocabulary. To make it a bit more interesting we’ll start from an actual real-time message which contains more than one state object. In fact, we’ll be checking for changes on a regular basis. Now we have a real linked data event stream! Towards a More Advanced Model As mentioned above, we’ll be using an open vocabulary standard to describe our model. This allows us to attach real semantic meaning to it and create interoperability with other Data Publishers that use the same vocabulary. Understanding and mapping our source model (check out the dataset schema) to the target model is the hard part, in particular if we are missing descriptions for the model structure and its properties. Lucky for us, most of the property names are more-or-less self-explanatory. source property meaning name descriptive name lastupdate timestamp when last updated type type of parking facility, always offStreetParkingGround openingtimesdescription description of opening times isopennow is parking currently open? specified as boolean: yes = 1, no = 0 temporaryclosed is parking temporary closed? (boolean) operatorinformation description of company operating the parking freeparking is parking freely accessable? (boolean) urllinkaddress webpage URL of the parking offering more information numberofspaces total number of spaces (capacity) availablespaces available number of spaces occupancytrend ? occupation amount of occupied spaces expressed as a rounded percentage of the capacity latitude north-south position of the parking longitude east-west position of the parking location.lon same as longitude but expressed as a number location.lat same as latitude but expressed as a number gentse_feesten ? As you can see, except for a few, we have a pretty good idea of the meaning of the properties. Obviously we should double-check our assumptions with the publisher of this data. For this tutorial, we’ll assume that the meaning is correct and that we can ignore the few unclear properties. So, let’s continue by looking into how these properties map onto the Mobivoc model. Looking at the Mobivoc model we notice a central entity named parking facility. It derives from a civic structure inheriting its properties. We can also see that there is an entity parking lot derived from a parking facility and is essentially the same as our offstreet parking ground. Following the civic structure relations we see that it can have a capacity and a real time capacity (derived from capacity). Exactly what we need! Furthermore, a capacity is valid for a vehicle type, a civic structure has an opening hours specification and is operated by an organization. Let’s create a mapping from our source model to the target model based on this knowledge. Note that for readability we use well-known abbreviations for the namespaces used in the properties and values. source target name value rdfs:label value lastupdate value dct:modified value type offStreetParkingGround rdf:type mv:ParkingLot openingtimesdescription value schema:openingHoursSpecification [rdf:type schema:OpeningHoursSpecification; rdfs:label value] isopennow value N/A temporaryclosed value N/A operatorinformation value mv:operatedBy [rdf:type schema:Organization, dct:Agent; rdfs:label value] freeparking value mv:price [rdf:type schema:PriceSpecification; mv:freeOfCharge value] urllinkaddress value mv:url value numberofspaces value mv:capacity [rdf:type mv:Capacity; mv:totalCapacity value] availablespaces value mv:capacity [rdf:type mv:RealTimeCapacity; mv:currentValue value] occupancytrend value N/A occupation value mv:rateOfOccupancy value latitude value geo:lat value longitude value geo:long value location.lon N/A location.lat N/A gentse_feesten value N/A Note that we mark some mappings as not applicable (N/A) because we cannot map a property, we do not known the exact meaning of the property or we do not need it (e.g. duplicates). Great! We have determined what will be mapped and how. We’re done. Well, not quite. There is one more thing we need: an identity for our entity. It has to be an URI and obviously it needs to be unique. In addition, for every update of the available spaces the identity should remain the same (duh!). So, what do we use for the identity? One possible option is to take the urllinkaddress value. It would work as long as the Data Owner does not decide to relocate it. Best option is to check with the Data Owner but for this tutorial we’ll continue on the assumption that the urllinkaddress will not change. To Push or To Pull, That’s the Question As mention above, to make it more interesting we will be retrieving the number of available spaces in our parking lots on a regular interval. To do so we can use a component that can poll one or more URLs using HTTP. To do so, we need to replace the LdioHttpIn component (push model) that listens for incoming HTTP requests by a LdioHttpInPoller component (pull model). For example, to poll our source URL every two minutes we need to configure our pipeline input as: input: name: be.vlaanderen.informatievlaanderen.ldes.ldio.LdioHttpInPoller config: url: https://data.stad.gent/api/explore/v2.1/catalog/datasets/real-time-bezetting-pr-gent/exports/csv?lang=en&amp;timezone=Europe%2FBrussels interval: PT1M This will ensure we receive the actual state of our parking lots at regular time intervals, which may or may not have changed since the last time we checked. Note that we request the data as CSV but alternatively we could have used JSON or GeoJSON. We still need to configure an adapter to convert the received CSV message to linked data. We’ll do that next. With a Little Help From Our Pirates Now that we can get the actual state of our parking lots, we need to convert the source message in semicolon (;) separated CSV format to the linked data models we defined in the mapping. For this we can use a technology called RDF Mapping Language (RML). There are various ways to produce the mapping that we need: directly using linked data which defines the RML mapping rules or indirectly using a more human readable way named Yarrrml. Personally I prefer the real thing but using Matey may be more your thing. Explaining the RML technology is beyond the scope of this tutorial. The technology allows us to convert formats such as CSV, XML and JSON into complex RDF models. However, using it to create these complex models can be quite challenging. The solution is to do a straight forward mapping and create the structure we need in a second phase. Basically we map the properties of our source model one-to-one into an intermediate linked data model and then transform this intermediate model into our final model using another RDF technology (SPARQL construct), which is way easier to use for creating complex structures. We’ll do this in the next section. We start by creating a simple intermediate model where we already set the correct identity and entity type but map everything else as-is onto an intermediate vocabulary (temp: or https://temp.org/ns/advanced-compose# in full). source intermediate name value temp:name value lastupdate value temp:lastupdate value type offStreetParkingGround rdf:type mv:ParkingLot openingtimesdescription value temp:openingtimesdescription value operatorinformation value temp:operatorinformation value freeparking value temp:freeparking value urllinkaddress id id numberofspaces value temp:numberofspaces value availablespaces value temp:availablespaces value occupation value temp:occupation value latitude value temp:latitude value longitude value temp:longitude value To create a RML mapping file we need to write the RML rules in Turtle. All the Turtle prefixes should go at the start of the file but for simplicity we’ll add the prefixes as we go. Let’s start with the most common ones: @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . @prefix rml: &lt;http://semweb.mmlab.be/ns/rml#&gt; . @prefix rr: &lt;http://www.w3.org/ns/r2rml#&gt; . @prefix ql: &lt;http://semweb.mmlab.be/ns/ql#&gt; . @prefix carml: &lt;http://carml.taxonic.com/carml/&gt; . Note that the last one is always needed for our RML adapter component (RmlAdaptor). Now, we start by defining the map which will contain our mapping rules. We define a prefix for our map and rules (:) and tell the RML component that we will be mapping CSV messages. Do not forget that all prefixes go at the start before our mapping and rules. @prefix : &lt;https://example.org/ns/tutorial/advanced-conversion#&gt; . :TriplesMap a rr:TriplesMap; rml:logicalSource [ a rml:LogicalSource; rml:source [ a carml:Stream ]; rml:referenceFormulation ql:CSV ]. Let’s continue now with defining the identity and type of our parking lots. Remember that for the identity we use the URL value and for the type we use parking lot. At the same time we’ll also add each parking lot in its own graph. Say what? We’ll learn about triples and graphs a bit later. For now, just remember that we want to handle each parking lot separately so we instruct the RML component to generate a stream of mv:ParkingLot entities, one for each row in the CSV. @prefix mv: &lt;http://schema.mobivoc.org/#&gt; . :TriplesMap rr:subjectMap [ rr:graphMap [ rr:template &quot;{urllinkaddress}&quot; ]; rml:reference &quot;urllinkaddress&quot;; rr:class mv:ParkingLot ]. Easy enough. No? Let’s continue with one property. We define a rule saying that the entity will have a property (predicate) named temp:name whose value (object) comes from the source property name. @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix temp: &lt;https://temp.org/ns/advanced-compose#&gt; . :TriplesMap rr:predicateObjectMap [ rr:predicate temp:name; rr:objectMap [ rml:reference &quot;name&quot; ] ]. Again, no rocket-science once you get used to the Turtle and RML syntax. Let’s do the other properties as well. We define a rule to map each source property value onto the intermediate property. However, to make our life a bit easier in the next step, where we convert the intermediate to the target model, we can already add the correct value types. :TriplesMap rr:predicateObjectMap [ rr:predicate temp:lastupdate; rr:objectMap [ rml:reference &quot;lastupdate&quot;; rr:datatype xsd:dateTime ] ], [ rr:predicate temp:openingtimesdescription; rr:objectMap [ rml:reference &quot;openingtimesdescription&quot; ] ], [ rr:predicate temp:operatorinformation; rr:objectMap [ rml:reference &quot;operatorinformation&quot; ] ], [ rr:predicate temp:freeparking; rr:objectMap [ rml:reference &quot;freeparking&quot;; rr:datatype xsd:integer ] ], [ rr:predicate temp:numberofspaces; rr:objectMap [ rml:reference &quot;numberofspaces&quot;; rr:datatype xsd:integer ] ], [ rr:predicate temp:availablespaces; rr:objectMap [ rml:reference &quot;availablespaces&quot;; rr:datatype xsd:integer ] ], [ rr:predicate temp:occupation; rr:objectMap [ rml:reference &quot;occupation&quot;; rr:datatype xsd:integer ] ], [ rr:predicate temp:latitude; rr:objectMap [ rml:reference &quot;latitude&quot;; rr:datatype xsd:double ] ], [ rr:predicate temp:longitude; rr:objectMap [ rml:reference &quot;longitude&quot;; rr:datatype xsd:double ] ]. All of the above results in the mapping to intermediate file. In order to make it available to the workbench container we need to use volume mapping again. However, becomes we know we’ll need an additional file for transforming the intermediate to the target format, we choose to map the directory containing the mapping file as a whole: volumes: - ./workbench/config:/ldio/config:ro We also need to change our workbench pipeline to use the above RML mapping file and to include the RML mapping component (RmlAdapter instead of the JsonLdAdapter used in the basic setup). Our workbench pipeline input component is now complete and looks like this: input: name: be.vlaanderen.informatievlaanderen.ldes.ldio.LdioHttpInPoller config: url: https://data.stad.gent/api/explore/v2.1/catalog/datasets/real-time-bezetting-pr-gent/exports/csv?lang=en&amp;timezone=Europe%2FBrussels interval: PT1M adapter: name: be.vlaanderen.informatievlaanderen.ldes.ldi.RmlAdapter config: mapping: ./config/source-to-intermediate.ttl Using the Swiss Army Knife Now that we have an intermediate model as linked data we can use a SPARQL component which allows us to query the values in our intermediate model and construct a target model. If we look at our intermediate model and the target model we see that we need to keep the identity and type of our parking lot, convert the other properties to a different namespace and for some properties introduce the required structure: intermediate target temp:name value rdfs:label value temp:lastupdate value dct:modified value rdf:type mv:ParkingLot as-is temp:openingtimesdescription value schema:openingHoursSpecification [rdf:type schema:OpeningHoursSpecification; rdfs:label value] temp:operatorinformation value mv:operatedBy [rdf:type schema:Organization, dct:Agent; rdfs:label value] temp:freeparking value mv:price [rdf:type schema:PriceSpecification; mv:freeOfCharge value ] id mv:url id temp:numberofspaces value mv:capacity [rdf:type mv:Capacity; mv:totalCapacity value] temp:availablespaces value mv:capacity [rdf:type mv:RealTimeCapacity; mv:currentValue value] temp:occupation value mv:rateOfOccupancy value temp:latitude value geo:lat value temp:longitude value geo:long value So, let’s start with the an empty SPARQL construct query (which is similar to a SPARQL query but the result is a new RDF model not just some values). Again, we use Turtle to do this: # TODO: add our prefixes here CONSTRUCT { # TODO: add our target model here } WHERE { # TODO: select the intermediate model values here } Not too difficult to understand: in, the where part we select values from the intermediate model and put them in variables. We then use those variables to create the target model in the constructpart. Note that the casing of the words construct and where is not important. Let’s take a brief moment and look at the intermediate model. It is now linked data so we can look at this model as being a collection of id-property-value triples, where the id is the id of eack parking lot, the properties being the names in our temp: namespace and the values being values, obviously. In linked data we call this a triple. The S stands for subject (the identity of a thing), the P stands for predicate (a property identified by its unique full name, including the namespace) and the O stands for object (the value which can be literal or a reference to some other subject, with or without an actual identity). Conceptually, a triple is a way to represent a unidirectional, named relation between a subject and an object. In linked data we also define the concept of a graph, which is just a tag for a triple and as such basically a way of grouping a bunch of triples together. It has no implicit meaning. A graph can be named by having an URI which identifies it. There’s also one special unnamed graph (has no URI) which we call the default graph. When we add a graph part to a triple (SPO) we get a quad (SPOG). In fact, triples are just a special case of quads where the 4th component is the default graph. We can use graphs for many purposes, e.g. to identify the source of the triples, to group together all related entities, etc. But, we use graphs in our pipelines to split data containing multiple entities into a stream of entities that are processed one-by-one in the pipeline. Wow, let’s think about this for a moment: in linked data we model everything as a collection of (subject-predicate-object) triples. It allows us to look at SPARQL queries as being filters that select a subset of the triple collection. For example, if we need to select all the identities of our parking lots we can simply state that we look for all the triples for which the predicate is rdf:type and the object is mv:ParkingLot. The subjects of these triples are in fact what we search for: the identities. To express this in a SPARQL query we specify this as follows: ?id &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://schema.mobivoc.org/#ParkingLot&gt; The interesting part is the variable ?id. It represents each result in our query. To make it more readable and in order to not repeat the namespace in every subject, predicate and object, we can again use prefixes: PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; PREFIX mv: &lt;http://schema.mobivoc.org/#&gt; ?id rdf:type mv:ParkingLot Note that the syntax for our prefix definitions is slightly different: we use PREFIX instead of @prefix and there is no dot (.) at the end of the line. The full SPARQL query would be: PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; PREFIX mv: &lt;http://schema.mobivoc.org/#&gt; SELECT ?id WHERE { ?id rdf:type mv:ParkingLot } But we do not need the identities only. Instead we want to create a new collection of triples for each parking lot with the predicates changed to those needed by our target model. Let’s start with simply copying the triple that defines our parking lots and their update timestamp: PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; PREFIX mv: &lt;http://schema.mobivoc.org/#&gt; PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; PREFIX temp: &lt;https://temp.org/ns/advanced-compose#&gt; CONSTRUCT { ?id a mv:ParkingLot . ?id dct:modified ?lastupdate . } WHERE { ?id rdf:type mv:ParkingLot . ?id temp:lastupdate ?lastupdate . } Note that a is a short-hand notation of rdf:type. In the where part we look for each ?id which is a mv:ParkingLot and then we retrieve its value of temp:lastupdate as variable ?lastupdate. Now that we have found these we can use the variables to create a new set of triples listed under the construct part. Not too difficult, is it? Our target model is a bit more structured that our intermediate model, so at times we need to introduce an intermediate relation to some structure. Take for example the capacity. In the model diagram, we see that a civic structure has a relation has capacity to a Capacity object that has a property total capacity. In linked data we model this as triples in this way: &lt;civic-structure-id&gt; rdf:type schema:CivicStructure . &lt;civic-structure-id&gt; mv:capacity &lt;a-capacity&gt; . &lt;a-capacity&gt; rdf:type mv:Capacity . &lt;a-capacity&gt; mv:totalCapacity &quot;some-numeric-value&quot;^^xsd:integer . We see a very interesting difference between the civic structure and its capacity: a civic-structure has some unique identifier such as “https://example.org/id/civic-structure/parking-lot-1” but a capacity is something that does not exist on its own, it is part of the civic structure and does not have an identity of its own. In linked data we call this a blank node because we can represent a triple as two nodes connected by a directed arrow, where the start node is a subject, the arrow is the predicate and the end node is the object. A blank node is a node without an identity and can be both the source and the destination of one or more arrows, representing its relations aka. properties aka. predicates. Because a blank node has no identity, we can write the above a bit more condensed by dropping the meaningless &lt;a-capacity&gt; and separating the predicates of the same subject by a semi-colon (;) - formatted for clarity: &lt;civic-structure-id&gt; rdf:type schema:CivicStructure ; mv:capacity [ rdf:type mv:Capacity ; mv:totalCapacity &quot;some-numeric-value&quot;^^xsd:integer ]. Note that [ ... ] now represents our capacity. Now that we have learned how to introduce structure in our target model we can create the complete mapping using SPARQL construct and we need to add this transformation step in the workbench pipeline: transformers: - name: be.vlaanderen.informatievlaanderen.ldes.ldi.SparqlConstructTransformer config: query: ./config/intermediate-to-target.rq In addition, as our target model has changed, we need to fix the transformation step which creates the version object to: - name: be.vlaanderen.informatievlaanderen.ldes.ldi.VersionObjectCreator config: member-type: http://schema.mobivoc.org/#ParkingLot delimiter: &quot;/&quot; date-observed-property: &lt;http://purl.org/dc/terms/modified&gt; generatedAt-property: http://purl.org/dc/terms/modified versionOf-property: http://purl.org/dc/terms/isVersionOf And also the definition of the LDES to reflect the correct target class: @prefix mv: &lt;http://schema.mobivoc.org/#&gt; &lt;/occupancy&gt; a ldes:EventStream ; tree:shape [ a sh:NodeShape ; sh:targetClass mv:ParkingLot ] ; Note that in the complete mapping when we query the properties from our source model that almost all of these query lines are wrapped by an optional { ... } construct. The reason for this is that any of these triples may be missing. Remember that the WHERE clause is in essence a filter on the collection of source triples, where each query line refines the subset of results from the previous query line. Therefore, if we do not use optional then the query returns no results and hence no target entity is constructed. Enough Talk, Show Me the Members Now that we have set everything up, we can launch our systems. We cannot launch both our LDES server and our workbench at the same time because we are now polling for the data and our workbench pipeline will start immediately. This is a problem because we first need to send the definition of our LDES and view to the LDES server. Actually, it takes our LDES server longer to start than our workbench, so we need to prevent launching the workbench until our LDES server is up and running and we have seeded our definitions. To prevent our workbench to launch when we bring all our other systems up, we can add a profile to the Docker compose file in the workbench service. The actual name of the profile does not matter but we use delay-started to clearly communicate the purpose: ldio-workbench: container_name: advanced-conversion_ldio-workbench image: ghcr.io/informatievlaanderen/ldi-orchestrator:latest volumes: - ./workbench/config:/ldio/config:ro - ./workbench/application.yml:/ldio/application.yml:ro ports: - 9004:80 networks: - advanced-conversion profiles: - delay-started To run the LDES server and its storage service (mongo), wait until its up and running, send definitions and then start the workbench: clear docker compose up -d while ! docker logs $(docker ps -q -f &quot;name=ldes-server$&quot;) 2&gt; /dev/null | grep &#39;Started Application in&#39; ; do sleep 1; done curl -X POST -H &quot;content-type: text/turtle&quot; &quot;http://localhost:9003/ldes/admin/api/v1/eventstreams&quot; -d &quot;@./definitions/occupancy.ttl&quot; curl -X POST -H &quot;content-type: text/turtle&quot; &quot;http://localhost:9003/ldes/admin/api/v1/eventstreams/occupancy/views&quot; -d &quot;@./definitions/occupancy.by-page.ttl&quot; docker compose up ldio-workbench -d while ! docker logs $(docker ps -q -f &quot;name=ldio-workbench$&quot;) 2&gt; /dev/null | grep &#39;Started Application in&#39; ; do sleep 1; done To verify the LDES, view and data: clear curl http://localhost:9003/ldes/occupancy curl http://localhost:9003/ldes/occupancy/by-page curl http://localhost:9003/ldes/occupancy/by-page?pageNumber=1 The last URL will contain our members, looking something like this (limited to one member): @prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix park-and-ride-pr: &lt;https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/&gt; . @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix terms: &lt;http://purl.org/dc/terms/&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt; . @prefix wgs84_pos: &lt;http://www.w3.org/2003/01/geo/wgs84_pos#&gt; . &lt;https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/pr-loopexpo#2023-12-13T12:21:21+01:00&gt; rdf:type &lt;http://schema.mobivoc.org/#ParkingLot&gt; ; rdfs:label &quot;P+R The Loop&quot; ; terms:isVersionOf park-and-ride-pr:pr-loopexpo ; terms:modified &quot;2023-12-13T12:21:21+01:00&quot;^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ; &lt;http://schema.mobivoc.org/#capacity&gt; [ rdf:type &lt;http://schema.mobivoc.org/#Capacity&gt; ; &lt;http://schema.mobivoc.org/#totalCapacity&gt; 168 ] ; &lt;http://schema.mobivoc.org/#capacity&gt; [ rdf:type &lt;http://schema.mobivoc.org/#RealTimeCapacity&gt; ; &lt;http://schema.mobivoc.org/#currentValue&gt; 30 ] ; &lt;http://schema.mobivoc.org/#operatedBy&gt; [ rdf:type terms:Agent , &lt;http://schema.org/Organization&gt; ; rdfs:label &quot;Mobiliteitsbedrijf Gent&quot; ] ; &lt;http://schema.mobivoc.org/#price&gt; [ rdf:type &lt;http://schema.org/PriceSpecification&gt; ; &lt;http://schema.mobivoc.org/#freeOfCharge&gt; 1 ] ; &lt;http://schema.mobivoc.org/#rateOfOccupancy&gt; 82 ; &lt;http://schema.mobivoc.org/#url&gt; park-and-ride-pr:pr-loopexpo ; &lt;http://schema.org/openingHoursSpecification&gt; [ rdf:type &lt;http://schema.org/OpeningHoursSpecification&gt; ; rdfs:label &quot;24/7&quot; ] ; wgs84_pos:lat &quot;51.024483197&quot;^^&lt;http://www.w3.org/2001/XMLSchema#double&gt; ; wgs84_pos:long &quot;3.69519252261&quot;^^&lt;http://www.w3.org/2001/XMLSchema#double&gt; . ... &lt;http://localhost:9003/ldes/occupancy&gt; rdf:type ldes:EventStream ; tree:member &lt;https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/pr-loopexpo#2023-12-13T12:21:21+01:00&gt;, ... ... Note that every two minutes the pipeline will request the latest state of our parking lots and will create additional version objects. The identity of a member depends only on the lastupdate property of our parking lot. If that did not change for a parking lot then the pipeline will create a version object with an identical identity as before. Any such version object will be refused by the LDES server and a warning will be logged in the LDES server log. The new version objects are added to the LDES and become new members. Every End is a New Beginning You should now know some basics about linked data. You learned how to define a mapping from non-linked data to linked data using RML as well as how to transform a linked data model into a different linked data model. In addition, you learned that you can periodically pull data into a workbench pipeline to create a continuous stream of versions of the state of some system. You can now stop all the systems. To bring the containers down and remove the private network: docker compose rm ldio-workbench --stop --force --volumes docker compose down Note that to bring down the workbench we need to stop it and remove the container and associated volumes explicitly because we started it separately." /> <link rel="canonical" href="http://localhost:4000/pipeline/advanced_conversion" /> <meta property="og:url" content="http://localhost:4000/pipeline/advanced_conversion" /> <meta property="og:site_name" content="Onboarding tutorial" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2024-02-19T11:01:58+01:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Advanced conversion" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-02-19T11:01:58+01:00","datePublished":"2024-02-19T11:01:58+01:00","description":"Publishing as a Standard Open Linked Data Model This quick start guide will show you how to create a more advanced processing pipeline in the LDIO Workbench for converting our example model to a standard open vocabulary and to publish that as a Linked Data Event Stream (LDES). Please see the introduction for the example data set and pre-requisites, as well as an overview of all examples. Copy &amp; Paste Rules! To kickstart this tutorial we can use the basic setup tutorial. For the server we only will need to change the actual model. Everything else can stay the same: we will still need to volume mount the server configuration file and provide the database connection string (which we have changed to reflect our tutorial). The workbench is where we need to change a few things: we’ll need to transform our custom model to the standard vocabulary. To make it a bit more interesting we’ll start from an actual real-time message which contains more than one state object. In fact, we’ll be checking for changes on a regular basis. Now we have a real linked data event stream! Towards a More Advanced Model As mentioned above, we’ll be using an open vocabulary standard to describe our model. This allows us to attach real semantic meaning to it and create interoperability with other Data Publishers that use the same vocabulary. Understanding and mapping our source model (check out the dataset schema) to the target model is the hard part, in particular if we are missing descriptions for the model structure and its properties. Lucky for us, most of the property names are more-or-less self-explanatory. source property meaning name descriptive name lastupdate timestamp when last updated type type of parking facility, always offStreetParkingGround openingtimesdescription description of opening times isopennow is parking currently open? specified as boolean: yes = 1, no = 0 temporaryclosed is parking temporary closed? (boolean) operatorinformation description of company operating the parking freeparking is parking freely accessable? (boolean) urllinkaddress webpage URL of the parking offering more information numberofspaces total number of spaces (capacity) availablespaces available number of spaces occupancytrend ? occupation amount of occupied spaces expressed as a rounded percentage of the capacity latitude north-south position of the parking longitude east-west position of the parking location.lon same as longitude but expressed as a number location.lat same as latitude but expressed as a number gentse_feesten ? As you can see, except for a few, we have a pretty good idea of the meaning of the properties. Obviously we should double-check our assumptions with the publisher of this data. For this tutorial, we’ll assume that the meaning is correct and that we can ignore the few unclear properties. So, let’s continue by looking into how these properties map onto the Mobivoc model. Looking at the Mobivoc model we notice a central entity named parking facility. It derives from a civic structure inheriting its properties. We can also see that there is an entity parking lot derived from a parking facility and is essentially the same as our offstreet parking ground. Following the civic structure relations we see that it can have a capacity and a real time capacity (derived from capacity). Exactly what we need! Furthermore, a capacity is valid for a vehicle type, a civic structure has an opening hours specification and is operated by an organization. Let’s create a mapping from our source model to the target model based on this knowledge. Note that for readability we use well-known abbreviations for the namespaces used in the properties and values. source target name value rdfs:label value lastupdate value dct:modified value type offStreetParkingGround rdf:type mv:ParkingLot openingtimesdescription value schema:openingHoursSpecification [rdf:type schema:OpeningHoursSpecification; rdfs:label value] isopennow value N/A temporaryclosed value N/A operatorinformation value mv:operatedBy [rdf:type schema:Organization, dct:Agent; rdfs:label value] freeparking value mv:price [rdf:type schema:PriceSpecification; mv:freeOfCharge value] urllinkaddress value mv:url value numberofspaces value mv:capacity [rdf:type mv:Capacity; mv:totalCapacity value] availablespaces value mv:capacity [rdf:type mv:RealTimeCapacity; mv:currentValue value] occupancytrend value N/A occupation value mv:rateOfOccupancy value latitude value geo:lat value longitude value geo:long value location.lon N/A location.lat N/A gentse_feesten value N/A Note that we mark some mappings as not applicable (N/A) because we cannot map a property, we do not known the exact meaning of the property or we do not need it (e.g. duplicates). Great! We have determined what will be mapped and how. We’re done. Well, not quite. There is one more thing we need: an identity for our entity. It has to be an URI and obviously it needs to be unique. In addition, for every update of the available spaces the identity should remain the same (duh!). So, what do we use for the identity? One possible option is to take the urllinkaddress value. It would work as long as the Data Owner does not decide to relocate it. Best option is to check with the Data Owner but for this tutorial we’ll continue on the assumption that the urllinkaddress will not change. To Push or To Pull, That’s the Question As mention above, to make it more interesting we will be retrieving the number of available spaces in our parking lots on a regular interval. To do so we can use a component that can poll one or more URLs using HTTP. To do so, we need to replace the LdioHttpIn component (push model) that listens for incoming HTTP requests by a LdioHttpInPoller component (pull model). For example, to poll our source URL every two minutes we need to configure our pipeline input as: input: name: be.vlaanderen.informatievlaanderen.ldes.ldio.LdioHttpInPoller config: url: https://data.stad.gent/api/explore/v2.1/catalog/datasets/real-time-bezetting-pr-gent/exports/csv?lang=en&amp;timezone=Europe%2FBrussels interval: PT1M This will ensure we receive the actual state of our parking lots at regular time intervals, which may or may not have changed since the last time we checked. Note that we request the data as CSV but alternatively we could have used JSON or GeoJSON. We still need to configure an adapter to convert the received CSV message to linked data. We’ll do that next. With a Little Help From Our Pirates Now that we can get the actual state of our parking lots, we need to convert the source message in semicolon (;) separated CSV format to the linked data models we defined in the mapping. For this we can use a technology called RDF Mapping Language (RML). There are various ways to produce the mapping that we need: directly using linked data which defines the RML mapping rules or indirectly using a more human readable way named Yarrrml. Personally I prefer the real thing but using Matey may be more your thing. Explaining the RML technology is beyond the scope of this tutorial. The technology allows us to convert formats such as CSV, XML and JSON into complex RDF models. However, using it to create these complex models can be quite challenging. The solution is to do a straight forward mapping and create the structure we need in a second phase. Basically we map the properties of our source model one-to-one into an intermediate linked data model and then transform this intermediate model into our final model using another RDF technology (SPARQL construct), which is way easier to use for creating complex structures. We’ll do this in the next section. We start by creating a simple intermediate model where we already set the correct identity and entity type but map everything else as-is onto an intermediate vocabulary (temp: or https://temp.org/ns/advanced-compose# in full). source intermediate name value temp:name value lastupdate value temp:lastupdate value type offStreetParkingGround rdf:type mv:ParkingLot openingtimesdescription value temp:openingtimesdescription value operatorinformation value temp:operatorinformation value freeparking value temp:freeparking value urllinkaddress id id numberofspaces value temp:numberofspaces value availablespaces value temp:availablespaces value occupation value temp:occupation value latitude value temp:latitude value longitude value temp:longitude value To create a RML mapping file we need to write the RML rules in Turtle. All the Turtle prefixes should go at the start of the file but for simplicity we’ll add the prefixes as we go. Let’s start with the most common ones: @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . @prefix rml: &lt;http://semweb.mmlab.be/ns/rml#&gt; . @prefix rr: &lt;http://www.w3.org/ns/r2rml#&gt; . @prefix ql: &lt;http://semweb.mmlab.be/ns/ql#&gt; . @prefix carml: &lt;http://carml.taxonic.com/carml/&gt; . Note that the last one is always needed for our RML adapter component (RmlAdaptor). Now, we start by defining the map which will contain our mapping rules. We define a prefix for our map and rules (:) and tell the RML component that we will be mapping CSV messages. Do not forget that all prefixes go at the start before our mapping and rules. @prefix : &lt;https://example.org/ns/tutorial/advanced-conversion#&gt; . :TriplesMap a rr:TriplesMap; rml:logicalSource [ a rml:LogicalSource; rml:source [ a carml:Stream ]; rml:referenceFormulation ql:CSV ]. Let’s continue now with defining the identity and type of our parking lots. Remember that for the identity we use the URL value and for the type we use parking lot. At the same time we’ll also add each parking lot in its own graph. Say what? We’ll learn about triples and graphs a bit later. For now, just remember that we want to handle each parking lot separately so we instruct the RML component to generate a stream of mv:ParkingLot entities, one for each row in the CSV. @prefix mv: &lt;http://schema.mobivoc.org/#&gt; . :TriplesMap rr:subjectMap [ rr:graphMap [ rr:template &quot;{urllinkaddress}&quot; ]; rml:reference &quot;urllinkaddress&quot;; rr:class mv:ParkingLot ]. Easy enough. No? Let’s continue with one property. We define a rule saying that the entity will have a property (predicate) named temp:name whose value (object) comes from the source property name. @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix temp: &lt;https://temp.org/ns/advanced-compose#&gt; . :TriplesMap rr:predicateObjectMap [ rr:predicate temp:name; rr:objectMap [ rml:reference &quot;name&quot; ] ]. Again, no rocket-science once you get used to the Turtle and RML syntax. Let’s do the other properties as well. We define a rule to map each source property value onto the intermediate property. However, to make our life a bit easier in the next step, where we convert the intermediate to the target model, we can already add the correct value types. :TriplesMap rr:predicateObjectMap [ rr:predicate temp:lastupdate; rr:objectMap [ rml:reference &quot;lastupdate&quot;; rr:datatype xsd:dateTime ] ], [ rr:predicate temp:openingtimesdescription; rr:objectMap [ rml:reference &quot;openingtimesdescription&quot; ] ], [ rr:predicate temp:operatorinformation; rr:objectMap [ rml:reference &quot;operatorinformation&quot; ] ], [ rr:predicate temp:freeparking; rr:objectMap [ rml:reference &quot;freeparking&quot;; rr:datatype xsd:integer ] ], [ rr:predicate temp:numberofspaces; rr:objectMap [ rml:reference &quot;numberofspaces&quot;; rr:datatype xsd:integer ] ], [ rr:predicate temp:availablespaces; rr:objectMap [ rml:reference &quot;availablespaces&quot;; rr:datatype xsd:integer ] ], [ rr:predicate temp:occupation; rr:objectMap [ rml:reference &quot;occupation&quot;; rr:datatype xsd:integer ] ], [ rr:predicate temp:latitude; rr:objectMap [ rml:reference &quot;latitude&quot;; rr:datatype xsd:double ] ], [ rr:predicate temp:longitude; rr:objectMap [ rml:reference &quot;longitude&quot;; rr:datatype xsd:double ] ]. All of the above results in the mapping to intermediate file. In order to make it available to the workbench container we need to use volume mapping again. However, becomes we know we’ll need an additional file for transforming the intermediate to the target format, we choose to map the directory containing the mapping file as a whole: volumes: - ./workbench/config:/ldio/config:ro We also need to change our workbench pipeline to use the above RML mapping file and to include the RML mapping component (RmlAdapter instead of the JsonLdAdapter used in the basic setup). Our workbench pipeline input component is now complete and looks like this: input: name: be.vlaanderen.informatievlaanderen.ldes.ldio.LdioHttpInPoller config: url: https://data.stad.gent/api/explore/v2.1/catalog/datasets/real-time-bezetting-pr-gent/exports/csv?lang=en&amp;timezone=Europe%2FBrussels interval: PT1M adapter: name: be.vlaanderen.informatievlaanderen.ldes.ldi.RmlAdapter config: mapping: ./config/source-to-intermediate.ttl Using the Swiss Army Knife Now that we have an intermediate model as linked data we can use a SPARQL component which allows us to query the values in our intermediate model and construct a target model. If we look at our intermediate model and the target model we see that we need to keep the identity and type of our parking lot, convert the other properties to a different namespace and for some properties introduce the required structure: intermediate target temp:name value rdfs:label value temp:lastupdate value dct:modified value rdf:type mv:ParkingLot as-is temp:openingtimesdescription value schema:openingHoursSpecification [rdf:type schema:OpeningHoursSpecification; rdfs:label value] temp:operatorinformation value mv:operatedBy [rdf:type schema:Organization, dct:Agent; rdfs:label value] temp:freeparking value mv:price [rdf:type schema:PriceSpecification; mv:freeOfCharge value ] id mv:url id temp:numberofspaces value mv:capacity [rdf:type mv:Capacity; mv:totalCapacity value] temp:availablespaces value mv:capacity [rdf:type mv:RealTimeCapacity; mv:currentValue value] temp:occupation value mv:rateOfOccupancy value temp:latitude value geo:lat value temp:longitude value geo:long value So, let’s start with the an empty SPARQL construct query (which is similar to a SPARQL query but the result is a new RDF model not just some values). Again, we use Turtle to do this: # TODO: add our prefixes here CONSTRUCT { # TODO: add our target model here } WHERE { # TODO: select the intermediate model values here } Not too difficult to understand: in, the where part we select values from the intermediate model and put them in variables. We then use those variables to create the target model in the constructpart. Note that the casing of the words construct and where is not important. Let’s take a brief moment and look at the intermediate model. It is now linked data so we can look at this model as being a collection of id-property-value triples, where the id is the id of eack parking lot, the properties being the names in our temp: namespace and the values being values, obviously. In linked data we call this a triple. The S stands for subject (the identity of a thing), the P stands for predicate (a property identified by its unique full name, including the namespace) and the O stands for object (the value which can be literal or a reference to some other subject, with or without an actual identity). Conceptually, a triple is a way to represent a unidirectional, named relation between a subject and an object. In linked data we also define the concept of a graph, which is just a tag for a triple and as such basically a way of grouping a bunch of triples together. It has no implicit meaning. A graph can be named by having an URI which identifies it. There’s also one special unnamed graph (has no URI) which we call the default graph. When we add a graph part to a triple (SPO) we get a quad (SPOG). In fact, triples are just a special case of quads where the 4th component is the default graph. We can use graphs for many purposes, e.g. to identify the source of the triples, to group together all related entities, etc. But, we use graphs in our pipelines to split data containing multiple entities into a stream of entities that are processed one-by-one in the pipeline. Wow, let’s think about this for a moment: in linked data we model everything as a collection of (subject-predicate-object) triples. It allows us to look at SPARQL queries as being filters that select a subset of the triple collection. For example, if we need to select all the identities of our parking lots we can simply state that we look for all the triples for which the predicate is rdf:type and the object is mv:ParkingLot. The subjects of these triples are in fact what we search for: the identities. To express this in a SPARQL query we specify this as follows: ?id &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://schema.mobivoc.org/#ParkingLot&gt; The interesting part is the variable ?id. It represents each result in our query. To make it more readable and in order to not repeat the namespace in every subject, predicate and object, we can again use prefixes: PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; PREFIX mv: &lt;http://schema.mobivoc.org/#&gt; ?id rdf:type mv:ParkingLot Note that the syntax for our prefix definitions is slightly different: we use PREFIX instead of @prefix and there is no dot (.) at the end of the line. The full SPARQL query would be: PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; PREFIX mv: &lt;http://schema.mobivoc.org/#&gt; SELECT ?id WHERE { ?id rdf:type mv:ParkingLot } But we do not need the identities only. Instead we want to create a new collection of triples for each parking lot with the predicates changed to those needed by our target model. Let’s start with simply copying the triple that defines our parking lots and their update timestamp: PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; PREFIX mv: &lt;http://schema.mobivoc.org/#&gt; PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; PREFIX temp: &lt;https://temp.org/ns/advanced-compose#&gt; CONSTRUCT { ?id a mv:ParkingLot . ?id dct:modified ?lastupdate . } WHERE { ?id rdf:type mv:ParkingLot . ?id temp:lastupdate ?lastupdate . } Note that a is a short-hand notation of rdf:type. In the where part we look for each ?id which is a mv:ParkingLot and then we retrieve its value of temp:lastupdate as variable ?lastupdate. Now that we have found these we can use the variables to create a new set of triples listed under the construct part. Not too difficult, is it? Our target model is a bit more structured that our intermediate model, so at times we need to introduce an intermediate relation to some structure. Take for example the capacity. In the model diagram, we see that a civic structure has a relation has capacity to a Capacity object that has a property total capacity. In linked data we model this as triples in this way: &lt;civic-structure-id&gt; rdf:type schema:CivicStructure . &lt;civic-structure-id&gt; mv:capacity &lt;a-capacity&gt; . &lt;a-capacity&gt; rdf:type mv:Capacity . &lt;a-capacity&gt; mv:totalCapacity &quot;some-numeric-value&quot;^^xsd:integer . We see a very interesting difference between the civic structure and its capacity: a civic-structure has some unique identifier such as “https://example.org/id/civic-structure/parking-lot-1” but a capacity is something that does not exist on its own, it is part of the civic structure and does not have an identity of its own. In linked data we call this a blank node because we can represent a triple as two nodes connected by a directed arrow, where the start node is a subject, the arrow is the predicate and the end node is the object. A blank node is a node without an identity and can be both the source and the destination of one or more arrows, representing its relations aka. properties aka. predicates. Because a blank node has no identity, we can write the above a bit more condensed by dropping the meaningless &lt;a-capacity&gt; and separating the predicates of the same subject by a semi-colon (;) - formatted for clarity: &lt;civic-structure-id&gt; rdf:type schema:CivicStructure ; mv:capacity [ rdf:type mv:Capacity ; mv:totalCapacity &quot;some-numeric-value&quot;^^xsd:integer ]. Note that [ ... ] now represents our capacity. Now that we have learned how to introduce structure in our target model we can create the complete mapping using SPARQL construct and we need to add this transformation step in the workbench pipeline: transformers: - name: be.vlaanderen.informatievlaanderen.ldes.ldi.SparqlConstructTransformer config: query: ./config/intermediate-to-target.rq In addition, as our target model has changed, we need to fix the transformation step which creates the version object to: - name: be.vlaanderen.informatievlaanderen.ldes.ldi.VersionObjectCreator config: member-type: http://schema.mobivoc.org/#ParkingLot delimiter: &quot;/&quot; date-observed-property: &lt;http://purl.org/dc/terms/modified&gt; generatedAt-property: http://purl.org/dc/terms/modified versionOf-property: http://purl.org/dc/terms/isVersionOf And also the definition of the LDES to reflect the correct target class: @prefix mv: &lt;http://schema.mobivoc.org/#&gt; &lt;/occupancy&gt; a ldes:EventStream ; tree:shape [ a sh:NodeShape ; sh:targetClass mv:ParkingLot ] ; Note that in the complete mapping when we query the properties from our source model that almost all of these query lines are wrapped by an optional { ... } construct. The reason for this is that any of these triples may be missing. Remember that the WHERE clause is in essence a filter on the collection of source triples, where each query line refines the subset of results from the previous query line. Therefore, if we do not use optional then the query returns no results and hence no target entity is constructed. Enough Talk, Show Me the Members Now that we have set everything up, we can launch our systems. We cannot launch both our LDES server and our workbench at the same time because we are now polling for the data and our workbench pipeline will start immediately. This is a problem because we first need to send the definition of our LDES and view to the LDES server. Actually, it takes our LDES server longer to start than our workbench, so we need to prevent launching the workbench until our LDES server is up and running and we have seeded our definitions. To prevent our workbench to launch when we bring all our other systems up, we can add a profile to the Docker compose file in the workbench service. The actual name of the profile does not matter but we use delay-started to clearly communicate the purpose: ldio-workbench: container_name: advanced-conversion_ldio-workbench image: ghcr.io/informatievlaanderen/ldi-orchestrator:latest volumes: - ./workbench/config:/ldio/config:ro - ./workbench/application.yml:/ldio/application.yml:ro ports: - 9004:80 networks: - advanced-conversion profiles: - delay-started To run the LDES server and its storage service (mongo), wait until its up and running, send definitions and then start the workbench: clear docker compose up -d while ! docker logs $(docker ps -q -f &quot;name=ldes-server$&quot;) 2&gt; /dev/null | grep &#39;Started Application in&#39; ; do sleep 1; done curl -X POST -H &quot;content-type: text/turtle&quot; &quot;http://localhost:9003/ldes/admin/api/v1/eventstreams&quot; -d &quot;@./definitions/occupancy.ttl&quot; curl -X POST -H &quot;content-type: text/turtle&quot; &quot;http://localhost:9003/ldes/admin/api/v1/eventstreams/occupancy/views&quot; -d &quot;@./definitions/occupancy.by-page.ttl&quot; docker compose up ldio-workbench -d while ! docker logs $(docker ps -q -f &quot;name=ldio-workbench$&quot;) 2&gt; /dev/null | grep &#39;Started Application in&#39; ; do sleep 1; done To verify the LDES, view and data: clear curl http://localhost:9003/ldes/occupancy curl http://localhost:9003/ldes/occupancy/by-page curl http://localhost:9003/ldes/occupancy/by-page?pageNumber=1 The last URL will contain our members, looking something like this (limited to one member): @prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix park-and-ride-pr: &lt;https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/&gt; . @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix terms: &lt;http://purl.org/dc/terms/&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt; . @prefix wgs84_pos: &lt;http://www.w3.org/2003/01/geo/wgs84_pos#&gt; . &lt;https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/pr-loopexpo#2023-12-13T12:21:21+01:00&gt; rdf:type &lt;http://schema.mobivoc.org/#ParkingLot&gt; ; rdfs:label &quot;P+R The Loop&quot; ; terms:isVersionOf park-and-ride-pr:pr-loopexpo ; terms:modified &quot;2023-12-13T12:21:21+01:00&quot;^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ; &lt;http://schema.mobivoc.org/#capacity&gt; [ rdf:type &lt;http://schema.mobivoc.org/#Capacity&gt; ; &lt;http://schema.mobivoc.org/#totalCapacity&gt; 168 ] ; &lt;http://schema.mobivoc.org/#capacity&gt; [ rdf:type &lt;http://schema.mobivoc.org/#RealTimeCapacity&gt; ; &lt;http://schema.mobivoc.org/#currentValue&gt; 30 ] ; &lt;http://schema.mobivoc.org/#operatedBy&gt; [ rdf:type terms:Agent , &lt;http://schema.org/Organization&gt; ; rdfs:label &quot;Mobiliteitsbedrijf Gent&quot; ] ; &lt;http://schema.mobivoc.org/#price&gt; [ rdf:type &lt;http://schema.org/PriceSpecification&gt; ; &lt;http://schema.mobivoc.org/#freeOfCharge&gt; 1 ] ; &lt;http://schema.mobivoc.org/#rateOfOccupancy&gt; 82 ; &lt;http://schema.mobivoc.org/#url&gt; park-and-ride-pr:pr-loopexpo ; &lt;http://schema.org/openingHoursSpecification&gt; [ rdf:type &lt;http://schema.org/OpeningHoursSpecification&gt; ; rdfs:label &quot;24/7&quot; ] ; wgs84_pos:lat &quot;51.024483197&quot;^^&lt;http://www.w3.org/2001/XMLSchema#double&gt; ; wgs84_pos:long &quot;3.69519252261&quot;^^&lt;http://www.w3.org/2001/XMLSchema#double&gt; . ... &lt;http://localhost:9003/ldes/occupancy&gt; rdf:type ldes:EventStream ; tree:member &lt;https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/pr-loopexpo#2023-12-13T12:21:21+01:00&gt;, ... ... Note that every two minutes the pipeline will request the latest state of our parking lots and will create additional version objects. The identity of a member depends only on the lastupdate property of our parking lot. If that did not change for a parking lot then the pipeline will create a version object with an identical identity as before. Any such version object will be refused by the LDES server and a warning will be logged in the LDES server log. The new version objects are added to the LDES and become new members. Every End is a New Beginning You should now know some basics about linked data. You learned how to define a mapping from non-linked data to linked data using RML as well as how to transform a linked data model into a different linked data model. In addition, you learned that you can periodically pull data into a workbench pipeline to create a continuous stream of versions of the state of some system. You can now stop all the systems. To bring the containers down and remove the private network: docker compose rm ldio-workbench --stop --force --volumes docker compose down Note that to bring down the workbench we need to stop it and remove the container and associated volumes explicitly because we started it separately.","headline":"Advanced conversion","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/pipeline/advanced_conversion"},"url":"http://localhost:4000/pipeline/advanced_conversion"}</script> <!-- End Jekyll SEO tag --> <div class="test-header" data-v-df634f57="" data-v-7a7a37b1=""> <div class="global-header" data-v-df634f57=""><img src="/assets/images/LOGO_Vlaanderen_NIV2.svg" alt="Vlaanderen" data-v-df634f57=""><img class="small-global-header-divider" src="/assets/images/divider_globalheader.svg" alt="divider" data-v-df634f57=""><span class="header-title header-font" data-v-df634f57="">Vlaamse Smart Data Space</span><span class="spacer" data-v-df634f57=""></span> <div id="help-needed" data-v-df634f57=""><span class="header-font" data-v-df634f57="">Hulp nodig</span><a id="help" href="https://www.vlaanderen.be/vlaamse-smart-data-space-portaal/contact"><img class="question-menu-item" src="/assets/images/question-mark.svg" alt="?" data-v-df634f57=""></a></div> </div> <hr class="divider" data-v-df634f57=""> </div> <div class="navbar navbar-fixed-top"> <div class="navbar-inner"> <div class="container-fluid"> <a href="https://informatievlaanderen.github.io/VSDS-Tech-Docs/"> <button type="button" class="btn btn-navbar custom-border" > <span class="nav-icon-bar">Home</span> </button> </a> <a href="https://github.com/Informatievlaanderen/VSDS-Onboarding-Example"> <button type="button" class="btn btn-navbar custom-border-activated" > <span class="nav-icon-bar">Tutorial</span> </button> </a> <a href="https://informatievlaanderen.github.io/VSDS-Linked-Data-Interactions/"> <button type="button" class="btn btn-navbar custom-border" > <span class="nav-icon-bar">Linked Data Interactions</span> </button> </a> <a href="https://informatievlaanderen.github.io/VSDS-LDESServer4J/"> <button type="button" class="btn btn-navbar custom-border" > <span class="nav-icon-bar">LDES server</span> </button> </a> <a href="https://informatievlaanderen.github.io/VSDS-Linked-Data-Interactions/ldio/ldio-inputs/ldio-ldes-client"> <button type="button" class="btn btn-navbar custom-border" > <span class="nav-icon-bar">LDES client</span> </button> </a> <a href="https://www.vlaanderen.be/vlaamse-smart-data-space-portaal/blog"> <button type="button" class="btn btn-navbar custom-border" > <span class="nav-icon-bar">Blog posts</span> </button> </a> <a href="https://informatievlaanderen.github.io/OSLO-mapping/"> <button type="button" class="btn btn-navbar custom-border" > <span class="nav-icon-bar">OSLO mapping</span> </button> </a> </div> </div> </div> <style> #help-needed[data-v-df634f57] { display: flex; align-items: center; gap: 5px; background-image: url(https://informatievlaanderen.github.io/OSLO-mapping/assets/images/hulp_nodig.svg); width: 144px; height: 44px; } .custom-border-activated { background-color: rgb(255, 230, 21); } .custom-border { border: 0.3px solid rgb(0, 200, 171); /* Randkleur */ /* Voeg hier eventueel andere stijlelementen toe */ } .navbar-fixed-top .navbar-inner { margin-top:30px; height: 35px; padding: 0; border-top: 1px solid #d4d4d4; z-index: 1000; position: fixed; right:0px; } @media (min-width: 50rem) .navbar-fixed-top { position: relative !important; width: auto !important; height: 100% !important; padding: 0; transition: none; } .global-header[data-v-df634f57] { display: flex; margin-left: 0px; align-items: center; position: fixed; width: 100%; background-color: white; z-index: 100; } .test-header{ margin-bottom: 20px; overflow:hidden; z-index: 100; } element.style { } .small-global-header-divider[data-v-df634f57] { margin: 0 8px 0 4px; } .header-font { color: #333332; font-family: Flanders Art Sans, sans-serif; font-size: 12px; font-style: normal; font-weight: 500; line-height: normal; letter-spacing: 0.5px; text-transform: uppercase; } .spacer { flex-grow: 1; } .divider { border: 1px solid #8f8f8f66; margin: 0; } #help-needed > span[data-v-df634f57] { margin-left: 25px; } /* title of the site */ #header { height: 80px; } #header hgroup { position: absolute; top: 10px; left: 20px; } #header h1 { margin: 0; font-size: 1.75em; font-weight: bold; } #header h2 { color: #ccc; margin: 0 0 4px 16px; line-height: 0.8; font-size: 1.0em; font-weight: normal; } #header a, #header a:hover, #header a:visited { text-decoration: none; } .navbar .nav > .active > a, .navbar .nav > .active > a:hover, .navbar .nav > .active > a:focus { box-shadow: none; } .navbar .btn-navbar { margin-bottom: 5px; } </style> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> Onboarding tutorial </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </a> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">Onboarding tutorial</a></li></ul> <div class="nav-category">Publishing an LDES</div> <ul class="nav-list"><li class="nav-list-item"><a href="/publishing/publishing_protected_LDES" class="nav-list-link">Publishing a protected LDES</a></li><li class="nav-list-item"><a href="/publishing/basic_setup" class="nav-list-link">Basic setup</a></li><li class="nav-list-item"><a href="/publishing/minimal_server" class="nav-list-link">Minimal Server</a></li></ul> <div class="nav-category">Data pipeline</div> <ul class="nav-list"><li class="nav-list-item"><a href="/pipeline/minimal_workbench" class="nav-list-link">Minimal workbench</a></li><li class="nav-list-item active"><a href="/pipeline/advanced_conversion" class="nav-list-link active">Advanced conversion</a></li></ul> <div class="nav-category">Consuming an LDES</div> <ul class="nav-list"><li class="nav-list-item"><a href="/consuming/consuming" class="nav-list-link">Setup a minimal LDES client</a></li></ul> <div class="nav-category">Protected LDES</div> <ul class="nav-list"><li class="nav-list-item"><a href="/protected/publishing_protected_LDES" class="nav-list-link">Publishing a protected LDES</a></li></ul> </nav> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Onboarding tutorial" aria-label="Search Onboarding tutorial" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div id="main-content-wrap" class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <h1 id="publishing-as-a-standard-open-linked-data-model"> <a href="#publishing-as-a-standard-open-linked-data-model" class="anchor-heading" aria-labelledby="publishing-as-a-standard-open-linked-data-model"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Publishing as a Standard Open Linked Data Model </h1> <p>This quick start guide will show you how to create a more advanced processing pipeline in the <a href="https://informatievlaanderen.github.io/VSDS-Linked-Data-Interactions/">LDIO Workbench</a> for converting our example model to a <a href="https://github.com/vocol/mobivoc">standard open vocabulary</a> and to publish that as a <a href="https://semiceu.github.io/LinkedDataEventStreams/">Linked Data Event Stream (LDES)</a>.</p> <p>Please see the <a href="../README.md">introduction</a> for the example data set and pre-requisites, as well as an overview of all examples.</p> <h2 id="copy--paste-rules"> <a href="#copy--paste-rules" class="anchor-heading" aria-labelledby="copy--paste-rules"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Copy &amp; Paste Rules! </h2> <p>To kickstart this tutorial we can use the <a href="./basic-setup/README.md">basic setup tutorial</a>.</p> <p>For the server we only will need to change the actual model. Everything else can stay the same: we will still need to volume mount the server configuration file and provide the database connection string (which we have changed to reflect our tutorial).</p> <p>The workbench is where we need to change a few things: we’ll need to transform our custom model to the standard vocabulary. To make it a bit more interesting we’ll start from an actual real-time message which contains more than one state object. In fact, we’ll be checking for changes on a regular basis. Now we have a real linked data event stream!</p> <h2 id="towards-a-more-advanced-model"> <a href="#towards-a-more-advanced-model" class="anchor-heading" aria-labelledby="towards-a-more-advanced-model"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Towards a More Advanced Model </h2> <p>As mentioned above, we’ll be using an open vocabulary standard to describe our model. This allows us to attach real semantic meaning to it and create interoperability with other Data Publishers that use the same vocabulary.</p> <p>Understanding and mapping our <a href="https://data.stad.gent/explore/dataset/real-time-bezetting-pr-gent/information/">source model</a> (check out the dataset schema) to the <a href="https://raw.githubusercontent.com/vocol/mobivoc/develop/diagrams/mobivoc_v1.1.4.png">target model</a> is the hard part, in particular if we are missing descriptions for the model structure and its properties. Lucky for us, most of the property names are more-or-less self-explanatory.</p> <div class="table-wrapper"><table> <thead> <tr> <th>source property</th> <th>meaning</th> </tr> </thead> <tbody> <tr> <td>name</td> <td>descriptive name</td> </tr> <tr> <td>lastupdate</td> <td>timestamp when last updated</td> </tr> <tr> <td>type</td> <td>type of parking facility, always <code class="language-plaintext highlighter-rouge">offStreetParkingGround</code></td> </tr> <tr> <td>openingtimesdescription</td> <td>description of opening times</td> </tr> <tr> <td>isopennow</td> <td>is parking currently open? specified as boolean: yes = 1, no = 0</td> </tr> <tr> <td>temporaryclosed</td> <td>is parking temporary closed? (boolean)</td> </tr> <tr> <td>operatorinformation</td> <td>description of company operating the parking</td> </tr> <tr> <td>freeparking</td> <td>is parking freely accessable? (boolean)</td> </tr> <tr> <td>urllinkaddress</td> <td>webpage URL of the parking offering more information</td> </tr> <tr> <td>numberofspaces</td> <td>total number of spaces (capacity)</td> </tr> <tr> <td>availablespaces</td> <td>available number of spaces</td> </tr> <tr> <td>occupancytrend</td> <td>?</td> </tr> <tr> <td>occupation</td> <td>amount of occupied spaces expressed as a rounded percentage of the capacity</td> </tr> <tr> <td>latitude</td> <td>north-south position of the parking</td> </tr> <tr> <td>longitude</td> <td>east-west position of the parking</td> </tr> <tr> <td>location.lon</td> <td>same as longitude but expressed as a number</td> </tr> <tr> <td>location.lat</td> <td>same as latitude but expressed as a number</td> </tr> <tr> <td>gentse_feesten</td> <td>?</td> </tr> </tbody> </table></div> <p>As you can see, except for a few, we have a pretty good idea of the meaning of the properties. Obviously we should double-check our assumptions with the publisher of this data. For this tutorial, we’ll assume that the meaning is correct and that we can ignore the few unclear properties. So, let’s continue by looking into how these properties map onto the Mobivoc model.</p> <p>Looking at the Mobivoc model we notice a central entity named <em>parking facility</em>. It derives from a <em>civic structure</em> inheriting its properties. We can also see that there is an entity <em>parking lot</em> derived from a <em>parking facility</em> and is essentially the same as our offstreet parking ground. Following the <em>civic structure</em> relations we see that it can have a <em>capacity</em> and a <em>real time capacity</em> (derived from <em>capacity</em>). Exactly what we need! Furthermore, a <em>capacity</em> is valid for a vehicle type, a <em>civic structure</em> has an <em>opening hours specification</em> and is operated by an <em>organization</em>. Let’s create a mapping from our source model to the target model based on this knowledge.</p> <blockquote> <p><strong>Note</strong> that for readability we use well-known abbreviations for the namespaces used in the properties and values.</p> </blockquote> <div class="table-wrapper"><table> <thead> <tr> <th>source</th> <th>target</th> </tr> </thead> <tbody> <tr> <td>name <em>value</em></td> <td>rdfs:label <em>value</em></td> </tr> <tr> <td>lastupdate <em>value</em></td> <td>dct:modified <em>value</em></td> </tr> <tr> <td>type offStreetParkingGround</td> <td>rdf:type mv:ParkingLot</td> </tr> <tr> <td>openingtimesdescription <em>value</em></td> <td>schema:openingHoursSpecification [rdf:type schema:OpeningHoursSpecification; rdfs:label <em>value</em>]</td> </tr> <tr> <td>isopennow <em>value</em></td> <td><em>N/A</em></td> </tr> <tr> <td>temporaryclosed <em>value</em></td> <td><em>N/A</em></td> </tr> <tr> <td>operatorinformation <em>value</em></td> <td>mv:operatedBy [rdf:type schema:Organization, dct:Agent; rdfs:label <em>value</em>]</td> </tr> <tr> <td>freeparking <em>value</em></td> <td>mv:price [rdf:type schema:PriceSpecification; mv:freeOfCharge <em>value</em>]</td> </tr> <tr> <td>urllinkaddress <em>value</em></td> <td>mv:url <em>value</em></td> </tr> <tr> <td>numberofspaces <em>value</em></td> <td>mv:capacity [rdf:type mv:Capacity; mv:totalCapacity <em>value</em>]</td> </tr> <tr> <td>availablespaces <em>value</em></td> <td>mv:capacity [rdf:type mv:RealTimeCapacity; mv:currentValue <em>value</em>]</td> </tr> <tr> <td>occupancytrend <em>value</em></td> <td><em>N/A</em></td> </tr> <tr> <td>occupation <em>value</em></td> <td>mv:rateOfOccupancy <em>value</em></td> </tr> <tr> <td>latitude <em>value</em></td> <td>geo:lat <em>value</em></td> </tr> <tr> <td>longitude <em>value</em></td> <td>geo:long <em>value</em></td> </tr> <tr> <td>location.lon</td> <td><em>N/A</em></td> </tr> <tr> <td>location.lat</td> <td><em>N/A</em></td> </tr> <tr> <td>gentse_feesten <em>value</em></td> <td><em>N/A</em></td> </tr> </tbody> </table></div> <blockquote> <p><strong>Note</strong> that we mark some mappings as not applicable (<em>N/A</em>) because we cannot map a property, we do not known the exact meaning of the property or we do not need it (e.g. duplicates).</p> </blockquote> <p>Great! We have determined what will be mapped and how. We’re done. Well, not quite. There is one more thing we need: an identity for our entity. It has to be an URI and obviously it needs to be unique. In addition, for every update of the available spaces the identity should remain the same (duh!). So, what do we use for the identity? One possible option is to take the <code class="language-plaintext highlighter-rouge">urllinkaddress</code> value. It would work as long as the Data Owner does not decide to relocate it. Best option is to check with the Data Owner but for this tutorial we’ll continue on the assumption that the <code class="language-plaintext highlighter-rouge">urllinkaddress</code> will not change.</p> <h2 id="to-push-or-to-pull-thats-the-question"> <a href="#to-push-or-to-pull-thats-the-question" class="anchor-heading" aria-labelledby="to-push-or-to-pull-thats-the-question"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> To Push or To Pull, That’s the Question </h2> <p>As mention above, to make it more interesting we will be retrieving the number of available spaces in our parking lots on a regular interval. To do so we can use a component that can poll one or more URLs using HTTP. To do so, we need to replace the <code class="language-plaintext highlighter-rouge">LdioHttpIn</code> component (push model) that listens for incoming HTTP requests by a <code class="language-plaintext highlighter-rouge">LdioHttpInPoller</code> component (pull model).</p> <p>For example, to poll our source URL every two minutes we need to configure our pipeline input as:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">input</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">be.vlaanderen.informatievlaanderen.ldes.ldio.LdioHttpInPoller</span>
  <span class="na">config</span><span class="pi">:</span>
    <span class="na">url</span><span class="pi">:</span> <span class="s">https://data.stad.gent/api/explore/v2.1/catalog/datasets/real-time-bezetting-pr-gent/exports/csv?lang=en&amp;timezone=Europe%2FBrussels</span>
    <span class="na">interval</span><span class="pi">:</span> <span class="s">PT1M</span>
</code></pre></div></div> <p>This will ensure we receive the actual state of our parking lots at regular time intervals, which may or may not have changed since the last time we checked.</p> <blockquote> <p><strong>Note</strong> that we request the data as CSV but alternatively we could have used JSON or GeoJSON. We still need to configure an adapter to convert the received CSV message to linked data. We’ll do that next.</p> </blockquote> <h2 id="with-a-little-help-from-our-pirates"> <a href="#with-a-little-help-from-our-pirates" class="anchor-heading" aria-labelledby="with-a-little-help-from-our-pirates"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> With a Little Help From Our Pirates </h2> <p>Now that we can get the actual state of our parking lots, we need to convert the source message in semicolon (<code class="language-plaintext highlighter-rouge">;</code>) separated CSV format to the linked data models we defined in the mapping. For this we can use a technology called <a href="https://rml.io">RDF Mapping Language</a> (RML). There are various ways to produce the mapping that we need: directly using linked data which defines the <a href="https://rml.io/specs/rml/">RML mapping rules</a> or indirectly using a more human readable way named <a href="https://rml.io/yarrrml/">Yarrrml</a>. Personally I prefer the real thing but using <a href="https://rml.io/yarrrml/matey/">Matey</a> may be more your thing.</p> <p>Explaining the RML technology is beyond the scope of this tutorial. The technology allows us to convert formats such as CSV, XML and JSON into complex RDF models. However, using it to create these complex models can be quite challenging. The solution is to do a straight forward mapping and create the structure we need in a second phase. Basically we map the properties of our source model one-to-one into an intermediate linked data model and then transform this intermediate model into our final model using another RDF technology (SPARQL construct), which is way easier to use for creating complex structures. We’ll do this in the next section.</p> <p>We start by creating a simple intermediate model where we already set the correct identity and entity type but map everything else as-is onto an intermediate vocabulary (<code class="language-plaintext highlighter-rouge">temp:</code> or <code class="language-plaintext highlighter-rouge">https://temp.org/ns/advanced-compose#</code> in full).</p> <div class="table-wrapper"><table> <thead> <tr> <th>source</th> <th>intermediate</th> </tr> </thead> <tbody> <tr> <td>name <em>value</em></td> <td>temp:name <em>value</em></td> </tr> <tr> <td>lastupdate <em>value</em></td> <td>temp:lastupdate <em>value</em></td> </tr> <tr> <td>type offStreetParkingGround</td> <td>rdf:type mv:ParkingLot</td> </tr> <tr> <td>openingtimesdescription <em>value</em></td> <td>temp:openingtimesdescription <em>value</em></td> </tr> <tr> <td>operatorinformation <em>value</em></td> <td>temp:operatorinformation <em>value</em></td> </tr> <tr> <td>freeparking <em>value</em></td> <td>temp:freeparking <em>value</em></td> </tr> <tr> <td>urllinkaddress <em>id</em></td> <td><em>id</em></td> </tr> <tr> <td>numberofspaces <em>value</em></td> <td>temp:numberofspaces <em>value</em></td> </tr> <tr> <td>availablespaces <em>value</em></td> <td>temp:availablespaces <em>value</em></td> </tr> <tr> <td>occupation <em>value</em></td> <td>temp:occupation <em>value</em></td> </tr> <tr> <td>latitude <em>value</em></td> <td>temp:latitude <em>value</em></td> </tr> <tr> <td>longitude <em>value</em></td> <td>temp:longitude <em>value</em></td> </tr> </tbody> </table></div> <p>To create a RML mapping file we need to write the RML rules in <a href="https://www.w3.org/TR/turtle/">Turtle</a>. All the Turtle prefixes should go at the start of the file but for simplicity we’ll add the prefixes as we go. Let’s start with the most common ones:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@prefix rdf:    &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .
@prefix rml:    &lt;http://semweb.mmlab.be/ns/rml#&gt; .
@prefix rr:     &lt;http://www.w3.org/ns/r2rml#&gt; .
@prefix ql:     &lt;http://semweb.mmlab.be/ns/ql#&gt; .
@prefix carml:  &lt;http://carml.taxonic.com/carml/&gt; .
</code></pre></div></div> <blockquote> <p><strong>Note</strong> that the last one is always needed for our RML adapter component (<code class="language-plaintext highlighter-rouge">RmlAdaptor</code>).</p> </blockquote> <p>Now, we start by defining the map which will contain our mapping rules. We define a prefix for our map and rules (<code class="language-plaintext highlighter-rouge">:</code>) and tell the RML component that we will be mapping CSV messages. Do not forget that all prefixes go at the start before our mapping and rules.</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@prefix :       &lt;https://example.org/ns/tutorial/advanced-conversion#&gt; .

:TriplesMap a rr:TriplesMap;
  rml:logicalSource [
    a rml:LogicalSource;
    rml:source [ a carml:Stream ];
    rml:referenceFormulation ql:CSV
  ].
</code></pre></div></div> <p>Let’s continue now with defining the identity and type of our parking lots. Remember that for the identity we use the URL value and for the type we use <em>parking lot</em>. At the same time we’ll also add each <em>parking lot</em> in its own graph. Say what? We’ll learn about triples and graphs a bit later. For now, just remember that we want to handle each <em>parking lot</em> separately so we instruct the RML component to generate a stream of <code class="language-plaintext highlighter-rouge">mv:ParkingLot</code> entities, one for each row in the CSV.</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@prefix mv:     &lt;http://schema.mobivoc.org/#&gt; .

:TriplesMap rr:subjectMap [
  rr:graphMap [ rr:template "{urllinkaddress}" ];
  rml:reference "urllinkaddress";
  rr:class mv:ParkingLot
].
</code></pre></div></div> <p>Easy enough. No? Let’s continue with one property. We define a rule saying that the entity will have a property (predicate) named <code class="language-plaintext highlighter-rouge">temp:name</code> whose value (object) comes from the source property <code class="language-plaintext highlighter-rouge">name</code>.</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@prefix rdfs:   &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .
@prefix temp:   &lt;https://temp.org/ns/advanced-compose#&gt; .

:TriplesMap rr:predicateObjectMap [
  rr:predicate temp:name;
  rr:objectMap [ rml:reference "name" ]
].
</code></pre></div></div> <p>Again, no rocket-science once you get used to the Turtle and RML syntax.</p> <p>Let’s do the other properties as well. We define a rule to map each source property value onto the intermediate property. However, to make our life a bit easier in the next step, where we convert the intermediate to the target model, we can already add the correct value types.</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>:TriplesMap rr:predicateObjectMap [
  rr:predicate temp:lastupdate;
  rr:objectMap [ rml:reference "lastupdate"; rr:datatype xsd:dateTime ]
], [
  rr:predicate temp:openingtimesdescription;
  rr:objectMap [ rml:reference "openingtimesdescription" ]
], [
  rr:predicate temp:operatorinformation;
  rr:objectMap [ rml:reference "operatorinformation" ]
], [
  rr:predicate temp:freeparking;
  rr:objectMap [ rml:reference "freeparking"; rr:datatype xsd:integer ]
], [
  rr:predicate temp:numberofspaces;
  rr:objectMap [ rml:reference "numberofspaces"; rr:datatype xsd:integer ]
], [
  rr:predicate temp:availablespaces;
  rr:objectMap [ rml:reference "availablespaces"; rr:datatype xsd:integer ]
], [
  rr:predicate temp:occupation;
  rr:objectMap [ rml:reference "occupation"; rr:datatype xsd:integer ]
], [
  rr:predicate temp:latitude;
  rr:objectMap [ rml:reference "latitude"; rr:datatype xsd:double ]
], [
  rr:predicate temp:longitude;
  rr:objectMap [ rml:reference "longitude"; rr:datatype xsd:double ]
].
</code></pre></div></div> <p>All of the above results in the <a href="./workbench/config/source-to-intermediate.ttl">mapping to intermediate</a> file. In order to make it available to the workbench container we need to use volume mapping again. However, becomes we know we’ll need an additional file for transforming the intermediate to the target format, we choose to map the directory containing the mapping file as a whole:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">volumes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">./workbench/config:/ldio/config:ro</span>
</code></pre></div></div> <p>We also need to change our workbench pipeline to use the above RML mapping file and to include the RML mapping component (<code class="language-plaintext highlighter-rouge">RmlAdapter</code> instead of the <code class="language-plaintext highlighter-rouge">JsonLdAdapter</code> used in the basic setup). Our workbench pipeline input component is now complete and looks like this:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">input</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">be.vlaanderen.informatievlaanderen.ldes.ldio.LdioHttpInPoller</span>
  <span class="na">config</span><span class="pi">:</span>
    <span class="na">url</span><span class="pi">:</span> <span class="s">https://data.stad.gent/api/explore/v2.1/catalog/datasets/real-time-bezetting-pr-gent/exports/csv?lang=en&amp;timezone=Europe%2FBrussels</span>
    <span class="na">interval</span><span class="pi">:</span> <span class="s">PT1M</span>
  <span class="na">adapter</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">be.vlaanderen.informatievlaanderen.ldes.ldi.RmlAdapter</span>
    <span class="na">config</span><span class="pi">:</span>
      <span class="na">mapping</span><span class="pi">:</span> <span class="s">./config/source-to-intermediate.ttl</span>
</code></pre></div></div> <h2 id="using-the-swiss-army-knife"> <a href="#using-the-swiss-army-knife" class="anchor-heading" aria-labelledby="using-the-swiss-army-knife"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Using the Swiss Army Knife </h2> <p>Now that we have an intermediate model as linked data we can use a <a href="https://en.wikipedia.org/wiki/SPARQL">SPARQL</a> component which allows us to query the values in our intermediate model and <a href="https://www.w3.org/TR/rdf-sparql-query/#construct">construct</a> a target model.</p> <p>If we look at our intermediate model and the target model we see that we need to keep the identity and type of our parking lot, convert the other properties to a different namespace and for some properties introduce the required structure:</p> <div class="table-wrapper"><table> <thead> <tr> <th>intermediate</th> <th>target</th> </tr> </thead> <tbody> <tr> <td>temp:name <em>value</em></td> <td>rdfs:label <em>value</em></td> </tr> <tr> <td>temp:lastupdate <em>value</em></td> <td>dct:modified <em>value</em></td> </tr> <tr> <td>rdf:type mv:ParkingLot</td> <td><em>as-is</em></td> </tr> <tr> <td>temp:openingtimesdescription <em>value</em></td> <td>schema:openingHoursSpecification [rdf:type schema:OpeningHoursSpecification; rdfs:label <em>value</em>]</td> </tr> <tr> <td>temp:operatorinformation <em>value</em></td> <td>mv:operatedBy [rdf:type schema:Organization, dct:Agent; rdfs:label <em>value</em>]</td> </tr> <tr> <td>temp:freeparking <em>value</em></td> <td>mv:price [rdf:type schema:PriceSpecification; mv:freeOfCharge <em>value</em> ]</td> </tr> <tr> <td><em>id</em></td> <td>mv:url <em>id</em></td> </tr> <tr> <td>temp:numberofspaces <em>value</em></td> <td>mv:capacity [rdf:type mv:Capacity; mv:totalCapacity <em>value</em>]</td> </tr> <tr> <td>temp:availablespaces <em>value</em></td> <td>mv:capacity [rdf:type mv:RealTimeCapacity; mv:currentValue <em>value</em>]</td> </tr> <tr> <td>temp:occupation <em>value</em></td> <td>mv:rateOfOccupancy <em>value</em></td> </tr> <tr> <td>temp:latitude <em>value</em></td> <td>geo:lat <em>value</em></td> </tr> <tr> <td>temp:longitude <em>value</em></td> <td>geo:long <em>value</em></td> </tr> </tbody> </table></div> <p>So, let’s start with the an empty SPARQL construct query (which is similar to a SPARQL query but the result is a new RDF model not just some values). Again, we use Turtle to do this:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code># TODO: add our prefixes here
CONSTRUCT {
  # TODO: add our target model here
} WHERE {
  # TODO: select the intermediate model values here
}
</code></pre></div></div> <p>Not too difficult to understand: in, the <code class="language-plaintext highlighter-rouge">where</code> part we select values from the intermediate model and put them in variables. We then use those variables to create the target model in the <code class="language-plaintext highlighter-rouge">construct</code>part.</p> <blockquote> <p><strong>Note</strong> that the casing of the words <code class="language-plaintext highlighter-rouge">construct</code> and <code class="language-plaintext highlighter-rouge">where</code> is not important.</p> </blockquote> <p>Let’s take a brief moment and look at the intermediate model. It is now linked data so we can look at this model as being a collection of id-property-value triples, where the id is the id of eack parking lot, the properties being the names in our <code class="language-plaintext highlighter-rouge">temp:</code> namespace and the values being values, obviously. In linked data we call this a <code class="language-plaintext highlighter-rouge">triple</code>. The <code class="language-plaintext highlighter-rouge">S</code> stands for <code class="language-plaintext highlighter-rouge">subject</code> (the identity of a thing), the <code class="language-plaintext highlighter-rouge">P</code> stands for <code class="language-plaintext highlighter-rouge">predicate</code> (a property identified by its unique full name, including the namespace) and the <code class="language-plaintext highlighter-rouge">O</code> stands for <code class="language-plaintext highlighter-rouge">object</code> (the value which can be literal or a reference to some other subject, with or without an actual identity). Conceptually, a triple is a way to represent a unidirectional, named relation between a subject and an object.</p> <p>In linked data we also define the concept of a <code class="language-plaintext highlighter-rouge">graph</code>, which is just a tag for a triple and as such basically a way of grouping a bunch of triples together. It has no implicit meaning. A graph can be named by having an URI which identifies it. There’s also one special unnamed graph (has no URI) which we call the default graph. When we add a graph part to a triple (<code class="language-plaintext highlighter-rouge">SPO</code>) we get a <code class="language-plaintext highlighter-rouge">quad</code> (<code class="language-plaintext highlighter-rouge">SPOG</code>). In fact, triples are just a special case of quads where the 4th component is the default graph. We can use graphs for many purposes, e.g. to identify the source of the triples, to group together all related entities, etc. But, we use graphs in our pipelines to split data containing multiple entities into a stream of entities that are processed one-by-one in the pipeline.</p> <p>Wow, let’s think about this for a moment: in linked data we model everything as a collection of (subject-predicate-object) triples. It allows us to look at SPARQL queries as being filters that select a subset of the triple collection. For example, if we need to select all the identities of our parking lots we can simply state that we look for all the triples for which the predicate is <code class="language-plaintext highlighter-rouge">rdf:type</code> and the object is <code class="language-plaintext highlighter-rouge">mv:ParkingLot</code>. The subjects of these triples are in fact what we search for: the identities. To express this in a SPARQL query we specify this as follows:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>?id &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://schema.mobivoc.org/#ParkingLot&gt;
</code></pre></div></div> <p>The interesting part is the variable <code class="language-plaintext highlighter-rouge">?id</code>. It represents each result in our query.</p> <p>To make it more readable and in order to not repeat the namespace in every subject, predicate and object, we can again use prefixes:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;
PREFIX mv:  &lt;http://schema.mobivoc.org/#&gt;

?id rdf:type mv:ParkingLot
</code></pre></div></div> <blockquote> <p><strong>Note</strong> that the syntax for our prefix definitions is slightly different: we use <code class="language-plaintext highlighter-rouge">PREFIX</code> instead of <code class="language-plaintext highlighter-rouge">@prefix</code> and there is no dot (<code class="language-plaintext highlighter-rouge">.</code>) at the end of the line.</p> </blockquote> <p>The full SPARQL query would be:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;
PREFIX mv:  &lt;http://schema.mobivoc.org/#&gt;

SELECT ?id WHERE { ?id rdf:type mv:ParkingLot }
</code></pre></div></div> <p>But we do not need the identities only. Instead we want to create a new collection of triples for each parking lot with the predicates changed to those needed by our target model. Let’s start with simply copying the triple that defines our parking lots and their update timestamp:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PREFIX rdf:  &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;
PREFIX mv:   &lt;http://schema.mobivoc.org/#&gt;
PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;
PREFIX temp: &lt;https://temp.org/ns/advanced-compose#&gt;

CONSTRUCT {
  ?id a mv:ParkingLot .
  ?id dct:modified ?lastupdate .
} WHERE {
  ?id rdf:type mv:ParkingLot .
  ?id temp:lastupdate ?lastupdate .
}
</code></pre></div></div> <blockquote> <p><strong>Note</strong> that <code class="language-plaintext highlighter-rouge">a</code> is a short-hand notation of <code class="language-plaintext highlighter-rouge">rdf:type</code>.</p> </blockquote> <p>In the <code class="language-plaintext highlighter-rouge">where</code> part we look for each <code class="language-plaintext highlighter-rouge">?id</code> which is a <code class="language-plaintext highlighter-rouge">mv:ParkingLot</code> and then we retrieve its value of <code class="language-plaintext highlighter-rouge">temp:lastupdate</code> as variable <code class="language-plaintext highlighter-rouge">?lastupdate</code>. Now that we have found these we can use the variables to create a new set of triples listed under the <code class="language-plaintext highlighter-rouge">construct</code> part. Not too difficult, is it?</p> <p>Our target model is a bit more structured that our intermediate model, so at times we need to introduce an intermediate relation to some structure. Take for example the capacity. In the <a href="https://raw.githubusercontent.com/vocol/mobivoc/develop/diagrams/mobivoc_v1.1.4.png">model diagram</a>, we see that a <em>civic structure</em> has a relation <em>has capacity</em> to a <em>Capacity</em> object that has a property <em>total capacity</em>. In linked data we model this as triples in this way:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;civic-structure-id&gt; rdf:type schema:CivicStructure .
&lt;civic-structure-id&gt; mv:capacity &lt;a-capacity&gt; .
&lt;a-capacity&gt; rdf:type mv:Capacity .
&lt;a-capacity&gt; mv:totalCapacity "some-numeric-value"^^xsd:integer .
</code></pre></div></div> <p>We see a very interesting difference between the <em>civic structure</em> and its <em>capacity</em>: a <em>civic-structure</em> has some unique identifier such as “https://example.org/id/civic-structure/parking-lot-1” but a <em>capacity</em> is something that does not exist on its own, it is part of the <em>civic structure</em> and does not have an identity of its own. In linked data we call this a <em>blank node</em> because we can represent a triple as two nodes connected by a directed arrow, where the start node is a subject, the arrow is the predicate and the end node is the object. A <em>blank node</em> is a node without an identity and can be both the source and the destination of one or more arrows, representing its relations aka. properties aka. predicates.</p> <p>Because a blank node has no identity, we can write the above a bit more condensed by dropping the meaningless <code class="language-plaintext highlighter-rouge">&lt;a-capacity&gt;</code> and separating the predicates of the same subject by a semi-colon (<code class="language-plaintext highlighter-rouge">;</code>) - formatted for clarity:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;civic-structure-id&gt; 
  rdf:type schema:CivicStructure ; 
  mv:capacity [ 
    rdf:type mv:Capacity ; 
    mv:totalCapacity "some-numeric-value"^^xsd:integer 
  ].
</code></pre></div></div> <blockquote> <p><strong>Note</strong> that <code class="language-plaintext highlighter-rouge">[ ... ]</code> now represents our <em>capacity</em>.</p> </blockquote> <p>Now that we have learned how to introduce structure in our target model we can create the <a href="./workbench/config/intermediate-to-target.rq">complete mapping</a> using SPARQL construct and we need to add this transformation step in the workbench pipeline:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">transformers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">be.vlaanderen.informatievlaanderen.ldes.ldi.SparqlConstructTransformer</span>
    <span class="na">config</span><span class="pi">:</span>
      <span class="na">query</span><span class="pi">:</span> <span class="s">./config/intermediate-to-target.rq</span>
</code></pre></div></div> <p>In addition, as our target model has changed, we need to fix the transformation step which creates the version object to:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">be.vlaanderen.informatievlaanderen.ldes.ldi.VersionObjectCreator</span>
    <span class="na">config</span><span class="pi">:</span>
      <span class="na">member-type</span><span class="pi">:</span> <span class="s">http://schema.mobivoc.org/#ParkingLot</span>
      <span class="na">delimiter</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/"</span>
      <span class="na">date-observed-property</span><span class="pi">:</span> <span class="s">&lt;http://purl.org/dc/terms/modified&gt;</span>
      <span class="na">generatedAt-property</span><span class="pi">:</span> <span class="s">http://purl.org/dc/terms/modified</span>
      <span class="na">versionOf-property</span><span class="pi">:</span> <span class="s">http://purl.org/dc/terms/isVersionOf</span>
</code></pre></div></div> <p>And also the <a href="./definitions/occupancy.ttl">definition of the LDES</a> to reflect the correct target class:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@prefix mv:          &lt;http://schema.mobivoc.org/#&gt;

&lt;/occupancy&gt; a ldes:EventStream ;
	tree:shape [ a sh:NodeShape ; sh:targetClass mv:ParkingLot ] ;
</code></pre></div></div> <blockquote> <p><strong>Note</strong> that in the <a href="./workbench/config/intermediate-to-target.rq">complete mapping</a> when we query the properties from our source model that almost all of these query lines are wrapped by an <code class="language-plaintext highlighter-rouge">optional { ... }</code> construct. The reason for this is that any of these triples may be missing. Remember that the <code class="language-plaintext highlighter-rouge">WHERE</code> clause is in essence a filter on the collection of source triples, where each query line refines the subset of results from the previous query line. Therefore, if we do not use <code class="language-plaintext highlighter-rouge">optional</code> then the query returns no results and hence no target entity is constructed.</p> </blockquote> <h2 id="enough-talk-show-me-the-members"> <a href="#enough-talk-show-me-the-members" class="anchor-heading" aria-labelledby="enough-talk-show-me-the-members"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Enough Talk, Show Me the Members </h2> <p>Now that we have set everything up, we can launch our systems. We cannot launch both our LDES server and our workbench at the same time because we are now polling for the data and our workbench pipeline will start immediately. This is a problem because we first need to send the definition of our LDES and view to the LDES server. Actually, it takes our LDES server longer to start than our workbench, so we need to prevent launching the workbench until our LDES server is up and running and we have seeded our definitions. To prevent our workbench to launch when we bring all our other systems up, we can add a <code class="language-plaintext highlighter-rouge">profile</code> to the <a href="./docker-compose.yml">Docker compose</a> file in the workbench service. The actual name of the profile does not matter but we use <code class="language-plaintext highlighter-rouge">delay-started</code> to clearly communicate the purpose:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">ldio-workbench</span><span class="pi">:</span>
  <span class="na">container_name</span><span class="pi">:</span> <span class="s">advanced-conversion_ldio-workbench</span>
  <span class="na">image</span><span class="pi">:</span> <span class="s">ghcr.io/informatievlaanderen/ldi-orchestrator:latest</span>
  <span class="na">volumes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">./workbench/config:/ldio/config:ro</span>
    <span class="pi">-</span> <span class="s">./workbench/application.yml:/ldio/application.yml:ro</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">9004:80</span>
  <span class="na">networks</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">advanced-conversion</span> 
  <span class="na">profiles</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">delay-started</span>
</code></pre></div></div> <p>To run the LDES server and its storage service (mongo), wait until its up and running, send definitions and then start the workbench:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>clear

docker compose up <span class="nt">-d</span>
<span class="k">while</span> <span class="o">!</span> docker logs <span class="si">$(</span>docker ps <span class="nt">-q</span> <span class="nt">-f</span> <span class="s2">"name=ldes-server$"</span><span class="si">)</span> 2&gt; /dev/null | <span class="nb">grep</span> <span class="s1">'Started Application in'</span> <span class="p">;</span> <span class="k">do </span><span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done

</span>curl <span class="nt">-X</span> POST <span class="nt">-H</span> <span class="s2">"content-type: text/turtle"</span> <span class="s2">"http://localhost:9003/ldes/admin/api/v1/eventstreams"</span> <span class="nt">-d</span> <span class="s2">"@./definitions/occupancy.ttl"</span>
curl <span class="nt">-X</span> POST <span class="nt">-H</span> <span class="s2">"content-type: text/turtle"</span> <span class="s2">"http://localhost:9003/ldes/admin/api/v1/eventstreams/occupancy/views"</span> <span class="nt">-d</span> <span class="s2">"@./definitions/occupancy.by-page.ttl"</span>

docker compose up ldio-workbench <span class="nt">-d</span>
<span class="k">while</span> <span class="o">!</span> docker logs <span class="si">$(</span>docker ps <span class="nt">-q</span> <span class="nt">-f</span> <span class="s2">"name=ldio-workbench$"</span><span class="si">)</span> 2&gt; /dev/null | <span class="nb">grep</span> <span class="s1">'Started Application in'</span> <span class="p">;</span> <span class="k">do </span><span class="nb">sleep </span>1<span class="p">;</span> <span class="k">done</span>
</code></pre></div></div> <p>To verify the LDES, view and data:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>clear
curl http://localhost:9003/ldes/occupancy
curl http://localhost:9003/ldes/occupancy/by-page
curl http://localhost:9003/ldes/occupancy/by-page?pageNumber<span class="o">=</span>1
</code></pre></div></div> <p>The last URL will contain our members, looking something like this (limited to one member):</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@prefix ldes:             &lt;https://w3id.org/ldes#&gt; .
@prefix park-and-ride-pr: &lt;https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/&gt; .
@prefix rdf:              &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .
@prefix rdfs:             &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .
@prefix terms:            &lt;http://purl.org/dc/terms/&gt; .
@prefix tree:             &lt;https://w3id.org/tree#&gt; .
@prefix wgs84_pos:        &lt;http://www.w3.org/2003/01/geo/wgs84_pos#&gt; .

&lt;https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/pr-loopexpo#2023-12-13T12:21:21+01:00&gt;
        rdf:type           &lt;http://schema.mobivoc.org/#ParkingLot&gt; ;
        rdfs:label         "P+R The Loop" ;
        terms:isVersionOf  park-and-ride-pr:pr-loopexpo ;
        terms:modified     "2023-12-13T12:21:21+01:00"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;
        &lt;http://schema.mobivoc.org/#capacity&gt;
                [ rdf:type  &lt;http://schema.mobivoc.org/#Capacity&gt; ;
                  &lt;http://schema.mobivoc.org/#totalCapacity&gt;
                          168
                ] ;
        &lt;http://schema.mobivoc.org/#capacity&gt;
                [ rdf:type  &lt;http://schema.mobivoc.org/#RealTimeCapacity&gt; ;
                  &lt;http://schema.mobivoc.org/#currentValue&gt;
                          30
                ] ;
        &lt;http://schema.mobivoc.org/#operatedBy&gt;
                [ rdf:type    terms:Agent , &lt;http://schema.org/Organization&gt; ;
                  rdfs:label  "Mobiliteitsbedrijf Gent"
                ] ;
        &lt;http://schema.mobivoc.org/#price&gt;
                [ rdf:type  &lt;http://schema.org/PriceSpecification&gt; ;
                  &lt;http://schema.mobivoc.org/#freeOfCharge&gt;
                          1
                ] ;
        &lt;http://schema.mobivoc.org/#rateOfOccupancy&gt;
                82 ;
        &lt;http://schema.mobivoc.org/#url&gt;
                park-and-ride-pr:pr-loopexpo ;
        &lt;http://schema.org/openingHoursSpecification&gt;
                [ rdf:type    &lt;http://schema.org/OpeningHoursSpecification&gt; ;
                  rdfs:label  "24/7"
                ] ;
        wgs84_pos:lat      "51.024483197"^^&lt;http://www.w3.org/2001/XMLSchema#double&gt; ;
        wgs84_pos:long     "3.69519252261"^^&lt;http://www.w3.org/2001/XMLSchema#double&gt; .

...

&lt;http://localhost:9003/ldes/occupancy&gt;
        rdf:type     ldes:EventStream ;
        tree:member  &lt;https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/pr-loopexpo#2023-12-13T12:21:21+01:00&gt;, ...
...
</code></pre></div></div> <blockquote> <p><strong>Note</strong> that every two minutes the pipeline will request the latest state of our parking lots and will create additional version objects. The identity of a member depends only on the <code class="language-plaintext highlighter-rouge">lastupdate</code> property of our parking lot. If that did not change for a parking lot then the pipeline will create a version object with an identical identity as before. Any such version object will be refused by the LDES server and a warning will be logged in the LDES server log. The new version objects are added to the LDES and become new members.</p> </blockquote> <h2 id="every-end-is-a-new-beginning"> <a href="#every-end-is-a-new-beginning" class="anchor-heading" aria-labelledby="every-end-is-a-new-beginning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Every End is a New Beginning </h2> <p>You should now know some basics about linked data. You learned how to define a mapping from non-linked data to linked data using RML as well as how to transform a linked data model into a different linked data model. In addition, you learned that you can periodically pull data into a workbench pipeline to create a continuous stream of versions of the state of some system. You can now stop all the systems.</p> <p>To bring the containers down and remove the private network:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker compose <span class="nb">rm </span>ldio-workbench <span class="nt">--stop</span> <span class="nt">--force</span> <span class="nt">--volumes</span>
docker compose down
</code></pre></div></div> <blockquote> <p><strong>Note</strong> that to bring down the workbench we need to stop it and remove the container and associated volumes explicitly because we started it separately.</p> </blockquote> </main> <hr> <footer> <p class="text-small text-grey-dk-100 mb-0"></p> <div class="d-flex mt-2"> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> <script src="https://cdn.jsdelivr.net/npm/mermaid@9.1.3/dist/mermaid.min.js"></script> <script> var config = {} ; mermaid.initialize(config); window.mermaid.init(undefined, document.querySelectorAll('.language-mermaid')); </script> </body> </html>
