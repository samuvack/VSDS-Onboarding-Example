{"0": {
    "doc": "Onboarding tutorial",
    "title": "Onboarding tutorial",
    "content": "Onboarding tutorial . Based on a few use cases, we try to teach you in a light-hearted way how to publish or consume an LDES. We do not go into depth about the different possible parameters in these tech docs, but we show a working configuration for the proposed use case. Detailed information about the technical parameters or configuration options can be found individually under sections of the building blocks. ",
    "url": "/",
    
    "relUrl": "/"
  },"1": {
    "doc": "Basic setup",
    "title": "Publishing a Simple Data Set With a Basic Setup",
    "content": "This quick start guide will show you how to combine a LDIO Workbench and a LDES Server to create a basic setup for publishing linked data as a Linked Data Event Stream (LDES). Please see the introduction for the example data set and pre-requisites, as well as an overview of all examples. ",
    "url": "/publishing/basic_setup#publishing-a-simple-data-set-with-a-basic-setup",
    
    "relUrl": "/publishing/basic_setup#publishing-a-simple-data-set-with-a-basic-setup"
  },"2": {
    "doc": "Basic setup",
    "title": "All the Things We Need",
    "content": "In order to publish your data set as a LDES you will need to setup and configure a few systems. To start with you need a LDES Server. It will accepts, store and serve the data set. Next you will need a workbench which at the least creates version objects from your data set which typically consists of state objects. In addition, as your data set will typically not be linked data, you will have to create a small pipeline in the workbench to transform your custom data model formatted in whatever format that you expose to a linked data model. The LDES server can ingest the resulting linked data model from several RDF serializations and serve the event stream in any of those RDF formats. Let’s start by creating a Docker Compose file containing a LDES server, its MongoDB storage container and a LDIO Workbench. First, we start by naming the file docker-compose.yml and add the file version and a private network which allows our three systems to interact: . version: '2.0' networks: basic-setup: name: basic-setup_ldes-network . We add the MongoDB system as our first service. We simply use the latest mongo image from Docker Hub and expose the default MongoDB port. This allows use to use a tool such as MongoDB Compass to examine the database if needed: . services: ldes-mongodb: container_name: basic-setup_ldes-mongodb image: mongo:latest ports: - 27017:27017 networks: - basic-setup . After that we add a LDES Server as a service, point it to its configuration file using volume mapping, expose its port so we can retrieve the event stream and set it to depend on the storage container in order to delay starting the server container until after the storage container: . ldes-server: container_name: basic-setup_ldes-server image: ldes/ldes-server:2.5.0-SNAPSHOT # you can safely change this to the latest 2.x.y version volumes: - ./server/application.yml:/application.yml:ro ports: - 9003:80 networks: - basic-setup depends_on: - ldes-mongodb . Finally, we add a LDIO Workbench as a service. It too needs to have access to its configuration file which we again provide using volume mapping. We also need to expose the workbench listener port so we can feed it with models from our custom data set. ldio-workbench: container_name: basic-setup_ldio-workbench image: ldes/ldi-orchestrator:1.11.0-SNAPSHOT # you can safely change this to the latest 1.x.y version volumes: - ./workbench/application.yml:/ldio/application.yml:ro ports: - 9004:80 networks: - basic-setup . We end up with this Docker compose file. At this point we cannot start the containers yet as we do refer to the LDES Server and the LDIO Workbench configuration files but we still need to create them. ",
    "url": "/publishing/basic_setup#all-the-things-we-need",
    
    "relUrl": "/publishing/basic_setup#all-the-things-we-need"
  },"3": {
    "doc": "Basic setup",
    "title": "Create the LDES Server Configuration File",
    "content": "Let’s continue by creating a configuration file for the LDES Server. But before we do we need to think about and decide on a few things: . | in which database do we store the LDES and related information? | on what sub-path will we serve our LDES? | on what port will we run our server? | . For this tutorial we can pick any name for the database. Let’s go for basic-setup. As within our private Docker network the containers can be reached by using their service name, the MongoDB connection string becomes mongodb://ldes-mongodb/basic-setup. We do not need to specify the default port 27017. For the port (server.port) and sub-path (server.servlet.context-path) on which the LDES Server is available we’ll go for 80 respectively /ldes. Note that we had to specify that indexes should be created automatically by the application (auto-index-creation: true). Note that we also need to specify the external base URL of the LDES so that we can follow the event stream from our local system (host-name: http://localhost:9003/ldes) because we set a sub-path (context-path: /ldes). Of course if we would change the server’s external port number in the Docker compose file, we need to change it here as well. ",
    "url": "/publishing/basic_setup#create-the-ldes-server-configuration-file",
    
    "relUrl": "/publishing/basic_setup#create-the-ldes-server-configuration-file"
  },"4": {
    "doc": "Basic setup",
    "title": "Create the LDIO Workbench Configuration File",
    "content": "For the workbench configuration file we can start from the configuration we used for the minimal workbench tutorial but serve the pipelines on a different port (80). As we are now creating an integrated setup we will not send the generated members to the container log using the LdioConsoleout component, but instead we use the LdioHttpOut component. This component allows us to send the member to the LDES server ingest endpoint over HTTP. How do we determine this HTTP ingest endpoint? Because the LDIO Workbench and the LDES Server share the same private network, the workbench can address the server using its service name ldes-server as server path http://ldes-server. As we have set the sub-path to serve all event streams from /ldes we append that to the server path. Finally, as we define our LDES in the same way as we did in the minimal server tutorial, we need to append the name of the LDES (/occupancy). Putting all of this together, in this tutorial the HTTP ingest endpoint for our LDES becomes http://ldes-server/ldes/occupancy. The configuration for our output thus becomes: . outputs: - name: be.vlaanderen.informatievlaanderen.ldes.ldio.LdioHttpOut config: endpoint: http://ldes-server/ldes/occupancy rdf-writer: content-type: application/n-quads . Note that we can use the rdf-writer.content-type setting to change the RDF format used when sending the member to the LDES Server ingest endpoint. By default it is Turtle (text/turtle) but here we choose N-quads (application/n-quads) as this is faster than any other RDF serialization both when writing and parsing. In our minimal workbench tutorial we assumed that we had linked data and we POST’ed a message formatted as JSON-LD to the workbench. Usually, you will have data in a more traditional (non-linked data) model. Typically there will be an API that you can poll or maybe the source system will notify you of changes using messages. No matter if the interface is pull-driven or push-driven, the data will be format using JSON, XML, CSV or similar. Now, let’s assume that on the input side we have a JSON message that is pushed into the workbench pipeline. We need to turn this non-linked data into linked-data. To accomplish this we can attach a JSON-LD context to the message. To do so, we need to use a JsonToLdAdapter in the LdioHttpIn component and configure to use our context. We also need to map the context in the container as follows: . volumes: - ./workbench/context.jsonld:/ldio/context.jsonld:ro . Now we can change the workbench input configuration to: . input: name: be.vlaanderen.informatievlaanderen.ldes.ldio.LdioHttpIn adapter: name: be.vlaanderen.informatievlaanderen.ldes.ldi.JsonToLdAdapter config: core-context: file:///ldio/context.jsonld . Note that we have to use the URI notation for the internal container path (/ldio/context.jsonld). Alternatively, we could use some pre-existing context somewhere online ar refer to it by URL. For the transformations steps we keep the same configuration for the version object creation and end up with the resulting configuration file . ",
    "url": "/publishing/basic_setup#create-the-ldio-workbench-configuration-file",
    
    "relUrl": "/publishing/basic_setup#create-the-ldio-workbench-configuration-file"
  },"5": {
    "doc": "Basic setup",
    "title": "Bringing it All Together",
    "content": "Now that we have everything set up, let’s test the systems. We need to bring all systems up, wait for both the LDIO Workbench and LDES Server to be available, send the LDES and view definitions to the server and finally send the JSON message to the workbench. Then we can retrieve the LDEs, the view and the page containing the actual member. To run the systems, wait, send definitions and message: . clear # bring the systems up docker compose up -d # wait for the workbench while ! docker logs $(docker ps -q -f \"name=ldio-workbench$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done # wait for the server while ! docker logs $(docker ps -q -f \"name=ldes-server$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done # define the LDES curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/ldes/admin/api/v1/eventstreams\" -d \"@./definitions/occupancy.ttl\" # define the view curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/ldes/admin/api/v1/eventstreams/occupancy/views\" -d \"@./definitions/occupancy.by-page.ttl\" # send the message curl -X POST -H \"Content-Type: application/json\" \"http://localhost:9004/p+r-pipeline\" -d \"@./data/message.json\" . Note that we send the definitions to http://localhost:9003/ldes because we have defined server.servlet.context-path: /ldes. Note that we send a JSON message now and therefore specify a header Content-Type: application/json. To verify the LDES, view and data: . clear # get the LDES curl http://localhost:9003/ldes/occupancy # get the view curl http://localhost:9003/ldes/occupancy/by-page # get the data curl http://localhost:9003/ldes/occupancy/by-page?pageNumber=1 . Note that we explicitly noted the three steps to get to the data. Typically a system that wants to replicate and synchronize a LDES only needs access to the LDES itself and can discover the view and subsequently the pages of that view by following the links in the LDES and view. To do so, we can use a LDES Client but that is a different tutorial. ",
    "url": "/publishing/basic_setup#bringing-it-all-together",
    
    "relUrl": "/publishing/basic_setup#bringing-it-all-together"
  },"6": {
    "doc": "Basic setup",
    "title": "The Party is Over, Let’s Go Home",
    "content": "You should now know how to publish a simple data set using a LDES Workbench and use a LDES Server to serve this data set using LDES. You learned how to setup a Docker compose file from scratch, how to configure the LDES Server on a different path and port, how to configure a LDIO Workbench to accept non-linked data and send it to the LDES Server. You can now stop all the systems. To bring the containers down and remove the private network: . docker compose down . ",
    "url": "/publishing/basic_setup#the-party-is-over-lets-go-home",
    
    "relUrl": "/publishing/basic_setup#the-party-is-over-lets-go-home"
  },"7": {
    "doc": "Basic setup",
    "title": "Basic setup",
    "content": " ",
    "url": "/publishing/basic_setup",
    
    "relUrl": "/publishing/basic_setup"
  },"8": {
    "doc": "Minimal Server",
    "title": "Setting Up a Minimal LDES Server",
    "content": "This quick start guide will show you how to setup a minimal LDES server to accept linked data members. Please see the introduction for the example data set and pre-requisites, as well as an overview of all examples. ",
    "url": "/publishing/minimal_server#setting-up-a-minimal-ldes-server",
    
    "relUrl": "/publishing/minimal_server#setting-up-a-minimal-ldes-server"
  },"9": {
    "doc": "Minimal Server",
    "title": "Enter the LDES Server",
    "content": "The LDES Server is one of the components built as part of a large project on creating a data space for Flanders (sorry, Dutch only) for Data Owners, Data Publishers and Data Clients. Its main purpose is to accept data set members, store them in external storage and allow to retrieve the data set as a LDES for replication as a whole or just a subset of the data set. However, the real benefit of LDES lies in its ability to allow Data Clients to keep in sync with changes that occur on the data set. Data Clients are typically not only interested in the current state of a data set but also in the historical changes that occurred, e.g. in order to understand the evolution of air quality in a city you need the analyze the measurements of some sensors over time. The LDES Server expects the data that it receives to be a so called version object, that is, the state of something at some point in time. This allows you to retrieve the changes of the object state from initial state (first version) to the current state (latest version) and due to the ability to keep in sync, even the future state (future versions). Pretty cool! Don’t you think? . ",
    "url": "/publishing/minimal_server#enter-the-ldes-server",
    
    "relUrl": "/publishing/minimal_server#enter-the-ldes-server"
  },"10": {
    "doc": "Minimal Server",
    "title": "Where Do We Keep This Gem?",
    "content": "As each Data Publisher needs to publish its own data set, the LDES Server is offered as a Docker image that can be configured to get the job done in a custom way and according to the needs of the Data Publisher. The Docker images are available on Docker Hub. Here you can find the stable releases. Notice that some tags end with -SNAPSHOT. These images can safely be used. The only difference with the ones without this label is that validity with the official specification did not yet occur. Functionality, they are exactly the same. Every two weeks (our sprint duration) a new images is created and labeled with semantic versioning. You can expect backwards compatible images to keep the same major version number and have an increased minor version number. Very occasionally we create a patch release with a non-zero patch number. Note that our Github repository contains many more images which are created of our development branch. Please use these only if really needed as they can and will contain some issues. ",
    "url": "/publishing/minimal_server#where-do-we-keep-this-gem",
    
    "relUrl": "/publishing/minimal_server#where-do-we-keep-this-gem"
  },"11": {
    "doc": "Minimal Server",
    "title": "Setup Up the Basics",
    "content": "As mentioned in the pre-requisites, you need some minimal knowledge on Docker Compose as we use it to run both the LDES Server and the required data storage system as a container. Currently we only support MongoDB for storing the LDES members and other data. On Docker Hub you can found the container images. Depending on your host hardware (e.g. a Raspberry Pi) you may need an older version, otherwise feel free to use the latest. Some other MongoDB compatible (cloud) databases can also be used (e.g. CosmosDB and DocumentDB). In the future we may add other databases. If you look into the Docker Compose file, you will see that we define one (private) network for our two services: the LDES Server and the MongoDB. This allows the LDES Server to refer to the MongoDB by its service name ldes-mongodb but also requires us to map the internal port numbers (respectively 8080 and 27017) to external port numbers (respectively 9003 and 27017) in order to be able to reach the services (respectively http://localhost:9003 and mongodb://localhost:27017, or even mongodb://locahost). We’ll see in a minute where the internal ports come from. Furthermore the compose file contains names for our containers, the images and tags to use as a definition (i.e. a cookie-cutter) for our containers, a dependency on the MongoDB so the LDES Server is started after the database container, and finally a (read-only) volume mapping of our LDES Server configuration file into the container file-system to allow our LDES Server to use it. The LDES Server configuration file is pretty straight-forward as well. It contains the URI (spring.data.mongodb.uri), basically the database connection string, of our MongoDB database. Note that the format is &lt;scheme&gt;://&lt;server&gt;:&lt;port&gt;/&lt;database&gt; where the &lt;scheme&gt; is always mongodb, the &lt;server&gt; is the service name in the composer file (i.e. ldes-mongodb), the &lt;port&gt; is the default port number 27017 and has been omitted, and finally, &lt;database&gt; is the name of the database that will hold our LDES data, for which we chose minimal-server. In addition, we need to specify that our LDES Server uses the MongoDB driver to automatically create the necessary indexes (even on CosmosDB!). We also need to specify the external base path of our LDES Server so that the links within the LDES can be followed. Because we do port mapping in the compose file, we set our LDES Server to be available at http://localhost:9003 in this tutorial. But not yet, we first need to start our containers. Ready for some action? Let’s get our hands dirty! . ",
    "url": "/publishing/minimal_server#setup-up-the-basics",
    
    "relUrl": "/publishing/minimal_server#setup-up-the-basics"
  },"12": {
    "doc": "Minimal Server",
    "title": "Systems Ready? 3, 2, 1 .. Ignition!",
    "content": "Well, enough theory! Let’s get this thing going and see how we can actually feed the LDES Server with some data. To run the commands below you need to use a bash command shell. This allows us to keep the commands the same across the main three operating systems: Linux, Windows and MacOS. But, if you cloned this repository locally you already have a bash shell! It comes with your git installation. To launch the LDES Server and the MongoDB containers: . docker compose up -d while ! docker logs $(docker ps -q -f \"name=ldes-server$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done . Note that we start the containers as deamons and then wait for the LDES server to be available by checking every second that the container log file contains the magic string Started Application in. You could also simply start it with docker compose up and wait until you actually see this magic string, but then you need to open a new command shell to execute the commands in the next sections. ",
    "url": "/publishing/minimal_server#systems-ready-3-2-1--ignition",
    
    "relUrl": "/publishing/minimal_server#systems-ready-3-2-1--ignition"
  },"13": {
    "doc": "Minimal Server",
    "title": "Defining Our First LDES",
    "content": "As soon as the systems are started and ready we need to tell our LDES Server what data set we want to store because the LDES Server can host more than one data set. To manage this the LDES Server offers an administration API. We will explore this API later but for now we simply send the LDES definition to this API so that we can store some data in our data set. To define the LDES: . curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/admin/api/v1/eventstreams\" -d \"@./definitions/occupancy.ttl\" . This may look like a bit of hocus-pocus but it is truly very simple. The LDES definition is a turtle file which is a way of serializing RDF. The definition file starts with a few declarations allowing for some short-hand notation so we do not get lost in translation. The other lines are the interesting part. First we start by saying that we define a LDES (&lt;/occupancy&gt; a ldes:EventStream) and that we want the server to accept members and fetching the LDES on a sub-path (/occupancy). Then, we tell it what type our members will be (sh:targetClass :offStreetParkingGround, or http://www.w3.org/ns/shacl#targetClass https://example.org/ns/mobility#offStreetParkingGround in full). Finally, we tell it that these members are version objects of the state object identified by the member property dcterms:isVersionOf and can be distinguised by the member property prov:generatedAtTime. Essentially, these properties allow us to group members to find all versions of something respectively to order them to know which member precedes another member. We can verify that the LDES is actually known to the server by requesting it by its endpoint. This endpoint depends on several things: the port mapping, the base path of the server and the data set sub-path we defined in the LDES definition. In this example it is http://localhost:9003/occupancy. To check out our LDES: . curl http://localhost:9003/occupancy . ",
    "url": "/publishing/minimal_server#defining-our-first-ldes",
    
    "relUrl": "/publishing/minimal_server#defining-our-first-ldes"
  },"14": {
    "doc": "Minimal Server",
    "title": "Storing Our First Member",
    "content": "Once we have defined our LDES we can finally send our first member to the endpoint we defined and the LDES Server can store it. To ingest a member: . curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/occupancy\" -d \"@./data/member.ttl\" . Note that the member is some linked data serialized as a turtle file for which the default file extension is .ttl and the mime type is text/turtle. The LDES server can accept a few more RDF serialization formats, each identified by their own mime type. ",
    "url": "/publishing/minimal_server#storing-our-first-member",
    
    "relUrl": "/publishing/minimal_server#storing-our-first-member"
  },"15": {
    "doc": "Minimal Server",
    "title": "Creating Our First View",
    "content": "Now that we have ingested a member, you may wonder how we can check that it is actually in there? And how will Data Clients retrieve our small data set. The LDES Server can offer various views on each data set so we need to tell it how we want the LDES to be available for consumption. We do this again using the administration API by sending a view definition to the LDES Server and attaching it to our LDES. The LDES Server will start a background process to create this LDES view and offer the data set in fragments. Such a fragment is a number of members as well as information about the LDES and ways to navigate to the other members from the data set. See the Tree specification on which the LDES specification is built for details. Essentially, the fragments create a (typically hierarchical but possibly a graph) structure for navigating between them by defining the relation between themselves and their related fragments. Data Clients can then navigate to all or a part of these relations and therefore reach all or a subset of the data set. But enough theory! Let’s define the view and request the member. To define the LDES view: . curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/admin/api/v1/eventstreams/occupancy/views\" -d \"@./definitions/occupancy.by-page.ttl\" . The view definition is a turtle file very similar to the LDES definition. It contains the view definition (it is a tree node) and how to get to this view (&lt;/occupancy/by-page&gt; a tree:Node). It set the number of members per fragment (tree:pageSize \"50\"^^xsd:integer). That’s all folks! . To check out our LDES: . curl http://localhost:9003/occupancy/by-page . Note that you can create more than one view of a LDES, even for simple pagination by specifying a different view URI and page size. Later when we learn about retention policies and different fragmentation strategies, this may make more sense. For now remember that you can create a view before or after you ingest data, You can delete views and re-create them with different options. For this, you will need to use the administration API. Later, we will show you how to enable the swagger to explore this API. ",
    "url": "/publishing/minimal_server#creating-our-first-view",
    
    "relUrl": "/publishing/minimal_server#creating-our-first-view"
  },"16": {
    "doc": "Minimal Server",
    "title": "Show Me the Data!",
    "content": "Depending on the size of the data set the LDES Server magic may take a while to make all members available, but you can already get the first members almost immediately. To retrieve the data set: . curl http://localhost:9003/occupancy/by-page?pageNumber=1 . ",
    "url": "/publishing/minimal_server#show-me-the-data",
    
    "relUrl": "/publishing/minimal_server#show-me-the-data"
  },"17": {
    "doc": "Minimal Server",
    "title": "All Good Things Must Come To an End",
    "content": "That’s it. Now you have an understanding what the LDES Server is about, where you can find it and how to set it up. You have learned how to create a LDES and a view for it as well as how to ingest and retrieve your data set. It’s time now to stop the LDES Server and its storage. To bring the containers down and remove the private network: . docker compose down . ",
    "url": "/publishing/minimal_server#all-good-things-must-come-to-an-end",
    
    "relUrl": "/publishing/minimal_server#all-good-things-must-come-to-an-end"
  },"18": {
    "doc": "Minimal Server",
    "title": "Minimal Server",
    "content": " ",
    "url": "/publishing/minimal_server",
    
    "relUrl": "/publishing/minimal_server"
  },"19": {
    "doc": "Publishing a protected LDES",
    "title": "Publishing and Accessing a Protected LDES",
    "content": "This tutorial will show you how to protect a LDES in order to prevent unauthorized access to a proprietary (or a public) data collection. It will also show how to expose the available LDES Server API as well as add and expose some metadata (using DCAT). In addition it will show you how to access such a protected LDES. Please see the introduction for the example data set and pre-requisites, as well as an overview of all examples. ",
    "url": "/publishing/publishing_protected_LDES#publishing-and-accessing-a-protected-ldes",
    
    "relUrl": "/publishing/publishing_protected_LDES#publishing-and-accessing-a-protected-ldes"
  },"20": {
    "doc": "Publishing a protected LDES",
    "title": "I’ll Protect You From the Hooded Claw",
    "content": "The LDES Server allows you to ingest a data collection and offers one or more views which allows replicating the data collection in whole or a part of it. However, not all data collection can be made publicly available. You need to protect those data collection in some way to prevent unauthorized access. What you will typically do is configure some security system which expects a Data Client to identify itself (authentication) after which the security system verifies it the Data Client has access to the requested data (authorization). The LDES Server does not include such a security system because there are various ways for doing authentication anad authorization and no one-size fits all. You may want to protect the data simply with an API key (secret key shared with a Data Client), using OAuth2 (authorization only) and/or OpenID (based on OAuth2 adding authentication), etc. There are many open-source and commercial implementations available for these and other protocols, so we decided to not include this aspect into the LDES Server. This keeps the server lean and simple for serving open data collections. But, even if you have a open data collection you may want to add a security layer to it to keep usage statistics per Data Client, e.g. to enforce fair usage policies, etc. So, we will look into how you can protect a LDES using a simple API key per Data Client and how the Data Client can access such a LDES. This technique may also apply to a commercial data collection: as a Data Publisher you may also want to offer a part of your data to the public for publicity and marketing reasons but of course you will want to track its usage. We will show how you can protect a LDES and what changes are needed to retrieve such a LDES. We will use existing tutorials to kickstart our setup. To create and feed our LDES we can use the advanced conversion setup and to retrieve it we can use the minimal client. Later we will add a reverse proxy to shield the LDES Server from the outside and configure things in such a way that we have controlled access to both LDES Server administration and LDES replication &amp; synchronization. Obviously we will have to make changes on both the Data Publisher and the Data Client side. But first, let us setup the system without access limitation first to ensure everything works fine. As usual, we start by creating a docker compose file containing the services that we need. At the Data Publisher side we need a database for the LDES Server (ldes-mongodb), the LDES Server itself (ldes-server) and a workbench to feed the LDES Server (server-workbench). We can basically copy/paste the services from the advanced conversion docker compose file: . ldes-mongodb: container_name: protected-setup_ldes-mongodb image: mongo:latest ports: - 27017:27017 networks: - protected-setup ldes-server: container_name: protected-setup_ldes-server image: ldes/ldes-server:2.10.0-SNAPSHOT # you can safely change this to the latest 2.x.y version volumes: - ./ldes-server/application.yml:/application.yml:ro ports: - 9003:80 networks: - protected-setup depends_on: - ldes-mongodb environment: - MANAGEMENT_TRACING_ENABLED=false # TODO: remove this when pull-based tracing implemented - LDES_SERVER_HOST_NAME=${LDES_SERVER_HOST_NAME:-http://localhost:9003/ldes} server-workbench: container_name: protected-setup_server-workbench image: ldes/ldi-orchestrator:2.0.0-SNAPSHOT # you can safely change this to the latest 1.x.y version volumes: - ./server-workbench/config:/ldio/config:ro - ./server-workbench/application.yml:/ldio/application.yml:ro ports: - 9004:80 networks: - protected-setup profiles: - delay-started . Notes: . | for clarity we renamed the network as well as the container names | we also renamed the workbench in order to stress that this is the workbench which feeds the LDES Server | we moved the configuration files to organize the setup a bit | we added an environment variable LDES_SERVER_HOST_NAME to allow changing the ldes-server.host-name in the server configuration easier | . At the Data Client side we only need a workbench (client-workbench) which we can borrow from the minimal client docker compose file: . client-workbench: container_name: protected-setup_client-workbench image: ldes/ldi-orchestrator:2.0.0-SNAPSHOT # you can safely change this to the latest 1.x.y version environment: - LDES_SERVER_URL=${LDES_SERVER_URL:-http://localhost:9003/ldes/occupancy/by-page} - SINK_URL=${SINK_URL} - MAX_REQUESTS_PER_MINUTE=${MAX_REQUESTS_PER_MINUTE:-50} volumes: - ./client-workbench/application.yml:/ldio/application.yml:ro network_mode: \"host\" profiles: - delay-started . Notes . | we renamed the container and the service | we moved the configuration files | we added a profile to prevent the client workbench to start ahead of time | . Note that the client workbench uses the network of the host which is completely disconnected from the internal docker network used by the LDES Server and its database and workbench. The LDES Client component therefore needs to use the host name (localhost) and exposed server port (9003) to access the LDES. That is why we have configured both the LDES_SERVER_HOST_NAME and the LDES_SERVER_URL to start with http://localhost:9003/. What we have now is illustrated in the following system container diagram: . Fig. 1 - Unprotected setup . At this point we can run all the systems and verify that we receive the LDES members in the sink. Please open https://webhook.site/ in a browser windows and copy your unique URL. Copy the .env file to user.env and fill in your unique URL as the sink URL. Now start all systems using (in a bash shell): . clear # start and wait for the server and database systems docker compose up -d while ! docker logs $(docker ps -q -f \"name=ldes-server$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done # define the LDES and the view curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/ldes/admin/api/v1/eventstreams\" -d \"@./ldes-server/definitions/occupancy.ttl\" curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/ldes/admin/api/v1/eventstreams/occupancy/views\" -d \"@./ldes-server/definitions/occupancy.by-page.ttl\" # start and wait for the server workbench docker compose up server-workbench -d while ! docker logs $(docker ps -q -f \"name=server-workbench$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done # start and wait for the client workbench docker compose --env-file user.env up client-workbench -d while ! docker logs $(docker ps -q -f \"name=client-workbench$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done . Once you have verified that the members appear in the sink you can shutdown the systems and remove the private network for now using: . docker compose rm client-workbench --stop --force --volumes docker compose rm server-workbench --stop --force --volumes docker compose down . ",
    "url": "/publishing/publishing_protected_LDES#ill-protect-you-from-the-hooded-claw",
    
    "relUrl": "/publishing/publishing_protected_LDES#ill-protect-you-from-the-hooded-claw"
  },"21": {
    "doc": "Publishing a protected LDES",
    "title": "It’s a Well Kept Secret",
    "content": "Now that we have a unprotected but working setup we can make the necessary changes to enforce security. We will need to do a few things: . | add a reverse proxy that will accept the LDES requests on the server’s behalf, check authentication &amp; authorization and forward the request or return an access error | not expose the LDES Server outside of the internal docker network so that the only way to access it is through the reverse proxy | change the Data Client (and Data Publisher) configuration to retrieve the LDES through the reverse proxy | . The following illustration shows the setup after adding such a reverse proxy: . Fig. 2 - Protected setup . The first thing we need to asks ourselves is which endpoints we need to protect but before we can answer that question we need to know what endpoints are available. By default, the LDES Server does not make that immediately apparant but by exposing the so called Swagger UI we can make the available API visible. If we add the following to the server configuration and launch it, we should see it: . springdoc: swagger-ui: path: /admin/doc/v1/swagger urlsPrimaryName: base . Note that we expose our swagger UI on the ‘admin’ API so we can protect it together with the rest of the admin API later. The path allows us to define the endpoint where the API information is visualized and the urlsPrimaryName allows us to choose which collection of APIs are displayed by default when we browse to the swagger UI endpoint. Now, in order to see it, we need to launch the server again: . clear docker compose up -d while ! docker logs $(docker ps -q -f \"name=ldes-server$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done . Once started point your browser to http://localhost:9003/ldes/admin/doc/v1/swagger. You will be redirected to http://localhost:9003/ldes/admin/doc/v1/swagger-ui/index.html and in the top right corner you should see the base API collection selected and the base API displayed in the main window. When we look at this base API, we see that there is one endpoint for the ingest that expects a POST to an endpoint. Obviously we do not want anybody else than the server workbench to push members to our ingest endpoint so we need to disallow this through the reverse proxy. We can do that by disallowing POST requests through the reverse proxy but we need to ensure that we can still seed the LDES definitions, which also use POST requests towards the administrative API. Next we see that the LDES Server exposes an endpoint to retrieve metadata at the root / as well as the various data collections at /{collection-name} and their respective views at /{collection-name}/{view-name}, all using GET requests. Most likely we want the metadata to be publicly accessible so that our available data collections can be discovered. In other words, we want metadata crawlers to be able to retrieve the metadata (typically DCAT information) in an unsecured way so that we get some exposure for our data collections. Now, as for the collections and views themselves, we can setup the accessibility as required by our use cases. For this tutorial we will assume that only a few clients can access our LDES and view so we will protect them with an API key. In fact, we will assign one API key per client so we can distinguish them for statistical reasons (e.g. to enforce a fair use policy). Now, if we switch to the admin API (select admin in the top right dropdown) we see a lot more available endpoints for managing the LDES Server. Obviously, we want to protect the whole admin API (including the swagger UI) so that only we are allowed to manage it. We will do this by protecting it with yet another API key. Note that an API key is not a very secure way of protecting an API. If somebody gets a hold of it, it can be misused. Therefore you should keep it a secret and use HTTPS instead of HTTP communication to prevent somebody sniffing the network and gaining access to the API key. Currently we do not expose any metadata for our LDES. Without going into details of DCAT, we will simply add the metadata (catalog, LDES metadata and view metadata) to our LDES Server by means of the admin API. We have kept the DCAT itself to the bare minimum as that is beyond the scope of the tutorial. It will be sufficient for our purpose. To try this please run: . # upload LDES &amp; view definitions curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/ldes/admin/api/v1/eventstreams\" -d \"@./ldes-server/definitions/occupancy.ttl\" curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/ldes/admin/api/v1/eventstreams/occupancy/views\" -d \"@./ldes-server/definitions/occupancy.by-page.ttl\" # upload metadata definitions curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/ldes/admin/api/v1/dcat\" -d \"@./ldes-server/metadata/catalog.ttl\" curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/ldes/admin/api/v1/eventstreams/occupancy/dcat\" -d \"@./ldes-server/metadata/occupancy.ttl\" curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/ldes/admin/api/v1/eventstreams/occupancy/views/by-page/dcat\" -d \"@./ldes-server/metadata/occupancy.by-page.ttl\" . Now you can get the full DCAT if you request the root http://localhost:9003/ldes. It is a mix of the metadata definitions which we uploaded and server generated data, resulting in something like this: . @prefix by-page: &lt;http://localhost:9003/ldes/occupancy/by-page/&gt; . @prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; . @prefix ldes: &lt;http://localhost:9003/ldes/&gt; . @prefix occupancy: &lt;http://localhost:9003/ldes/occupancy/&gt; . @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix terms: &lt;http://purl.org/dc/terms/&gt; . @prefix tree: &lt;https://w3id.org/tree/&gt; . ldes:occupancy rdf:type dcat:Dataset ; terms:conformsTo tree:specification , &lt;https://w3id.org/ldes/specification&gt; ; terms:description \"LDES containing the occupancy of the various park+rides in Ghent in real time\"@en ; terms:identifier \"http://localhost:9003/ldes/occupancy\"^^rdfs:Literal ; terms:title \"Real time occupancy P+R (Gent) as LDES\"@en . &lt;https://w3id.org/ldes/specification&gt; rdf:type terms:Standard . occupancy:by-page rdf:type rdfs:Resource . &lt;http://localhost:9003/ldes&gt; rdf:type dcat:Catalog ; terms:description \"Offers an overview of the dataset(s) and data service(s) needed for the tutorial 'Publishing And Accessing a Protected LDES'.\"@en ; terms:identifier \"c403cbbd-9e4d-47a2-8bb5-41a7642701ba\"^^rdfs:Literal ; terms:title \"Catalog for Publishing And Accessing a Protected LDES\"@en ; dcat:dataset ldes:occupancy ; dcat:service by-page:description . by-page:description rdf:type dcat:DataService ; terms:description \"Paged view for the occupancy of the various park+rides in Ghent in real time\"@en ; terms:identifier \"http://localhost:9003/ldes/occupancy/by-page\"^^rdfs:Literal ; terms:title \"Real time occupancy P+R (Gent) by page\"@en ; dcat:endpointDescription &lt;https://semiceu.github.io/LinkedDataEventStreams/&gt; ; dcat:endpointURL occupancy:by-page ; dcat:servesDataset ldes:occupancy . tree:specification rdf:type terms:Standard . &lt;https://semiceu.github.io/LinkedDataEventStreams/&gt; rdf:type rdfs:Resource . As said before, we want this metadata to be publicly available, while limiting access to the admin API only to ourselves and the LDES &amp; the view to a couple of well-known clients, all by means of a unique API key. You can create these keys using one of the free online GUID generators (e.g. https://www.uuidgenerator.net/guid) or a password generator (e.g. https://www.avast.com/random-password-generator), etc. Add a Reverse Proxy . First we need to add the reverse proxy service to the docker compose file: . reverse-proxy: image: nginx:stable container_name: protected-setup_reverse-proxy ports: - 9005:8080 volumes: - ./reverse-proxy/protect-ldes-server.conf:/etc/nginx/conf.d/protect-ldes-server.conf:ro depends_on: - ldes-server networks: - protected-setup . Here we chose a well-known freely available component that we can setup as a reverse proxy. There are many options out there open-source and commercial. The configuration is highly dependent on the component that you use but the principles are mostly the same: you allow or disallow access to some URL for some HTTP verbs (GET, HEAD, POST, etc) based on some conditions. We’ll be using these three API keys for demonstration purposes: . | API key | Purpose | . | admin-secret | admin | . | client-one-secret | client 1 | . | client-two-secret | client 2 | . This translates to a mapping for our reverse proxy. Next we define an API key validation location, meaning that for the URLs we need to protect we use this internal virtual URL for verifying the presence and validity of the given API key. Finally we add our access rules. As previously said we need to: . | allow access to metadata to the public | allow access to admin API (including the swagger API) only to administrators | allow access to the LDES, the view and all view nodes for registered clients only | do not allow access to the ingest endpoint (no POST to LDES endpoint allowed from outside) | . For this particular reverse proxy we end up with this configuration and can start the reverse proxy to test if the rules allow or disallow access correctly: . docker compose up reverse-proxy -d . We have setup the reverse proxy to remap the LDES Server endpoints (all based at /ldes) a bit. The reverse proxy serves: . | the metadata at / | the admin API at /admin | the LDES, the view and its nodes at /feed | . If we do not pass an API key we can retrieve only the metadata and not the LDES, the view, the admin API and the swagger UI: . clear curl -I http://localhost:9005/ curl -I http://localhost:9005/feed/occupancy curl -I http://localhost:9005/feed/occupancy/by-page curl -I http://localhost:9005/admin/api/v1/eventstreams . Public access is only allowed (HTTP 200) for the first call, all other calls are unauthenticated (HTTP 401). Note that we pass -I in order to only retrieve the headers, not the actual content. If we pass a client API key we can retrieve the metadata, the LDES and the view but we cannot use the admin API: . clear curl -I -H \"x-api-key: client-one-secret\" http://localhost:9005/ curl -I -H \"x-api-key: client-one-secret\" http://localhost:9005/feed/occupancy curl -I -H \"x-api-key: client-one-secret\" http://localhost:9005/feed/occupancy/by-page curl -I -H \"x-api-key: client-one-secret\" http://localhost:9005/admin/api/v1/eventstreams . All but the last call should succeed (HTTP 200) while the last one is forbidden (HTTP 403) because we are authenticated but not authorized to use the admin API. Note that we need to pass the API key using the header x-api-key: &lt;well-known-key&gt;. Finally, if we pass the admin API key all calls should be possible: . clear curl -I -H \"x-api-key: admin-secret\" http://localhost:9005/ curl -I -H \"x-api-key: admin-secret\" http://localhost:9005/feed/occupancy curl -I -H \"x-api-key: admin-secret\" http://localhost:9005/feed/occupancy/by-page curl -I -H \"x-api-key: admin-secret\" http://localhost:9005/admin/api/v1/eventstreams . Now all calls succeed. Great! . We need to verify one more rule: nobody (not even an administrator!) should we able to send data to the ingest endpoint of the LDES server: . clear curl -X POST -i -H \"content-type: text/turtle\" -d @./data/member.ttl http://localhost:9005/feed/occupancy curl -X POST -i -H \"content-type: text/turtle\" -d @./data/member.ttl http://localhost:9005/feed/occupancy -H \"x-api-key: client-one-secret\" curl -X POST -i -H \"content-type: text/turtle\" -d @./data/member.ttl http://localhost:9005/feed/occupancy -H \"x-api-key: admin-secret\" . All calls should fail with a forbidden (HTTP 403). Note that it would be better to return method not allowed (HTTP 405) but that seems to be a challenge in this specific reverse proxy configuration. One final thing to test is if we can POST to the admin endpoint, e.g. to define a LDES or add metadata. curl -X POST -d \"@./ldes-server/metadata/catalog.ttl\" -H \"content-type: text/turtle\" \"http://localhost:9005/admin/api/v1/dcat\" -H \"x-api-key: admin-secret\" . Should return an error similar to Resource of type: dcat-catalog with id: c403cbbd-9e4d-47a2-8bb5-41a7642701ba already exists. because we have already defined the catalog. But, this is great news because this means the reverse proxy let the request go through and the LDES Server returns an error response. Do Not Expose the LDES Server . Now that everything is working great we can simply remove (or comment out) the port mapping of the server in the docker compose file as we do not need and do not want any direct access to it: . ldes-server: container_name: protected-setup_ldes-server image: ldes/ldes-server:2.10.0-SNAPSHOT # you can safely change this to the latest 2.x.y version volumes: - ./ldes-server/application.yml:/application.yml:ro # ports: # - 9003:80 networks: - protected-setup depends_on: - ldes-mongodb environment: - MANAGEMENT_TRACING_ENABLED=false # TODO: remove this when pull-based tracing implemented - LDES_SERVER_HOST_NAME=${LDES_SERVER_HOST_NAME:-http://localhost:9003/ldes} . Access the LDES Server Through the Reverse Proxy . Once the LDES Server is not directly accessible anymore, we need to define some environment variables to use the reverse proxy instead: . LDES_SERVER_HOST_NAME=http://localhost:9005/feed LDES_SERVER_URL=http://localhost:9005/feed/occupancy . and we need to pass our user.env file to all our docker compose commands. Of course, we should not forget the most important part: configure the LDES Client to pass a API key when requesting the LDES nodes. In the client workbench we need to change the LDES CLient component configuration to include this API key: . input: name: Ldio:LdesClient config: urls: - ${LDES_SERVER_URL} sourceFormat: application/n-quads auth: type: API_KEY api-key-header: x-api-key api-key: client-two-secret . Show time! But first bring down all systems so we can start with a clean slate: . docker compose down . ",
    "url": "/publishing/publishing_protected_LDES#its-a-well-kept-secret",
    
    "relUrl": "/publishing/publishing_protected_LDES#its-a-well-kept-secret"
  },"22": {
    "doc": "Publishing a protected LDES",
    "title": "Putting It All Together",
    "content": "To launch all the systems and configure it all you can run the following: . clear # start and wait for the server and database systems docker compose --env-file user.env up -d while ! docker logs $(docker ps -q -f \"name=ldes-server$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done # upload LDES &amp; view definitions curl -X POST -H \"x-api-key: admin-secret\" -H \"content-type: text/turtle\" \"http://localhost:9005/admin/api/v1/eventstreams\" -d \"@./ldes-server/definitions/occupancy.ttl\" curl -X POST -H \"x-api-key: admin-secret\" -H \"content-type: text/turtle\" \"http://localhost:9005/admin/api/v1/eventstreams/occupancy/views\" -d \"@./ldes-server/definitions/occupancy.by-page.ttl\" # upload metadata definitions curl -X POST -H \"x-api-key: admin-secret\" -H \"content-type: text/turtle\" \"http://localhost:9005/admin/api/v1/dcat\" -d \"@./ldes-server/metadata/catalog.ttl\" curl -X POST -H \"x-api-key: admin-secret\" -H \"content-type: text/turtle\" \"http://localhost:9005/admin/api/v1/eventstreams/occupancy/dcat\" -d \"@./ldes-server/metadata/occupancy.ttl\" curl -X POST -H \"x-api-key: admin-secret\" -H \"content-type: text/turtle\" \"http://localhost:9005/admin/api/v1/eventstreams/occupancy/views/by-page/dcat\" -d \"@./ldes-server/metadata/occupancy.by-page.ttl\" # start and wait for the server workbench docker compose --env-file user.env up server-workbench -d while ! docker logs $(docker ps -q -f \"name=server-workbench$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done # start and wait for the client workbench docker compose --env-file user.env up client-workbench -d while ! docker logs $(docker ps -q -f \"name=client-workbench$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done . It all goes well (and it should!) you will see the LDES members appear in the sink. ",
    "url": "/publishing/publishing_protected_LDES#putting-it-all-together",
    
    "relUrl": "/publishing/publishing_protected_LDES#putting-it-all-together"
  },"23": {
    "doc": "Publishing a protected LDES",
    "title": "It’s Been a Long Day",
    "content": "We have shown you how to enable the swagger UI, how to provide metadata for your LDES views and, of course, how to access a protected LDES. In addition we have shown you how you can protect a LDES using a API key but if you require a stronger way of securing access have a look at other authentication and authorization mechanisms. The documentation explains how to configure the LDES client in case you need to access an OAuth2/OpenID protected LDES. Now that you have verified that the members appear in the sink you can shutdown the systems and remove the private network using: . docker compose rm client-workbench --stop --force --volumes docker compose rm server-workbench --stop --force --volumes docker compose down . ",
    "url": "/publishing/publishing_protected_LDES#its-been-a-long-day",
    
    "relUrl": "/publishing/publishing_protected_LDES#its-been-a-long-day"
  },"24": {
    "doc": "Publishing a protected LDES",
    "title": "Publishing a protected LDES",
    "content": " ",
    "url": "/publishing/publishing_protected_LDES",
    
    "relUrl": "/publishing/publishing_protected_LDES"
  },"25": {
    "doc": "Advanced conversion",
    "title": "Publishing as a Standard Open Linked Data Model",
    "content": "This quick start guide will show you how to create a more advanced processing pipeline in the LDIO Workbench for converting our example model to a standard open vocabulary and to publish that as a Linked Data Event Stream (LDES). Please see the introduction for the example data set and pre-requisites, as well as an overview of all examples. ",
    "url": "/pipeline/advanced_conversion#publishing-as-a-standard-open-linked-data-model",
    
    "relUrl": "/pipeline/advanced_conversion#publishing-as-a-standard-open-linked-data-model"
  },"26": {
    "doc": "Advanced conversion",
    "title": "Copy &amp; Paste Rules!",
    "content": "To kickstart this tutorial we can use the basic setup tutorial. For the server we only will need to change the actual model. Everything else can stay the same: we will still need to volume mount the server configuration file and provide the database connection string (which we have changed to reflect our tutorial). The workbench is where we need to change a few things: we’ll need to transform our custom model to the standard vocabulary. To make it a bit more interesting we’ll start from an actual real-time message which contains more than one state object. In fact, we’ll be checking for changes on a regular basis. Now we have a real linked data event stream! . ",
    "url": "/pipeline/advanced_conversion#copy--paste-rules",
    
    "relUrl": "/pipeline/advanced_conversion#copy--paste-rules"
  },"27": {
    "doc": "Advanced conversion",
    "title": "Towards a More Advanced Model",
    "content": "As mentioned above, we’ll be using an open vocabulary standard to describe our model. This allows us to attach real semantic meaning to it and create interoperability with other Data Publishers that use the same vocabulary. Understanding and mapping our source model (check out the dataset schema) to the target model is the hard part, in particular if we are missing descriptions for the model structure and its properties. Lucky for us, most of the property names are more-or-less self-explanatory. | source property | meaning | . | name | descriptive name | . | lastupdate | timestamp when last updated | . | type | type of parking facility, always offStreetParkingGround | . | openingtimesdescription | description of opening times | . | isopennow | is parking currently open? specified as boolean: yes = 1, no = 0 | . | temporaryclosed | is parking temporary closed? (boolean) | . | operatorinformation | description of company operating the parking | . | freeparking | is parking freely accessable? (boolean) | . | urllinkaddress | webpage URL of the parking offering more information | . | numberofspaces | total number of spaces (capacity) | . | availablespaces | available number of spaces | . | occupancytrend | ? | . | occupation | amount of occupied spaces expressed as a rounded percentage of the capacity | . | latitude | north-south position of the parking | . | longitude | east-west position of the parking | . | location.lon | same as longitude but expressed as a number | . | location.lat | same as latitude but expressed as a number | . | gentse_feesten | ? | . As you can see, except for a few, we have a pretty good idea of the meaning of the properties. Obviously we should double-check our assumptions with the publisher of this data. For this tutorial, we’ll assume that the meaning is correct and that we can ignore the few unclear properties. So, let’s continue by looking into how these properties map onto the Mobivoc model. Looking at the Mobivoc model we notice a central entity named parking facility. It derives from a civic structure inheriting its properties. We can also see that there is an entity parking lot derived from a parking facility and is essentially the same as our offstreet parking ground. Following the civic structure relations we see that it can have a capacity and a real time capacity (derived from capacity). Exactly what we need! Furthermore, a capacity is valid for a vehicle type, a civic structure has an opening hours specification and is operated by an organization. Let’s create a mapping from our source model to the target model based on this knowledge. Note that for readability we use well-known abbreviations for the namespaces used in the properties and values. | source | target | . | name value | rdfs:label value | . | lastupdate value | dct:modified value | . | type offStreetParkingGround | rdf:type mv:ParkingLot | . | openingtimesdescription value | schema:openingHoursSpecification [rdf:type schema:OpeningHoursSpecification; rdfs:label value] | . | isopennow value | N/A | . | temporaryclosed value | N/A | . | operatorinformation value | mv:operatedBy [rdf:type schema:Organization, dct:Agent; rdfs:label value] | . | freeparking value | mv:price [rdf:type schema:PriceSpecification; mv:freeOfCharge value] | . | urllinkaddress value | mv:url value | . | numberofspaces value | mv:capacity [rdf:type mv:Capacity; mv:totalCapacity value] | . | availablespaces value | mv:capacity [rdf:type mv:RealTimeCapacity; mv:currentValue value] | . | occupancytrend value | N/A | . | occupation value | mv:rateOfOccupancy value | . | latitude value | geo:lat value | . | longitude value | geo:long value | . | location.lon | N/A | . | location.lat | N/A | . | gentse_feesten value | N/A | . Note that we mark some mappings as not applicable (N/A) because we cannot map a property, we do not known the exact meaning of the property or we do not need it (e.g. duplicates). Great! We have determined what will be mapped and how. We’re done. Well, not quite. There is one more thing we need: an identity for our entity. It has to be an URI and obviously it needs to be unique. In addition, for every update of the available spaces the identity should remain the same (duh!). So, what do we use for the identity? One possible option is to take the urllinkaddress value. It would work as long as the Data Owner does not decide to relocate it. Best option is to check with the Data Owner but for this tutorial we’ll continue on the assumption that the urllinkaddress will not change. ",
    "url": "/pipeline/advanced_conversion#towards-a-more-advanced-model",
    
    "relUrl": "/pipeline/advanced_conversion#towards-a-more-advanced-model"
  },"28": {
    "doc": "Advanced conversion",
    "title": "To Push or To Pull, That’s the Question",
    "content": "As mention above, to make it more interesting we will be retrieving the number of available spaces in our parking lots on a regular interval. To do so we can use a component that can poll one or more URLs using HTTP. To do so, we need to replace the LdioHttpIn component (push model) that listens for incoming HTTP requests by a LdioHttpInPoller component (pull model). For example, to poll our source URL every two minutes we need to configure our pipeline input as: . input: name: be.vlaanderen.informatievlaanderen.ldes.ldio.LdioHttpInPoller config: url: https://data.stad.gent/api/explore/v2.1/catalog/datasets/real-time-bezetting-pr-gent/exports/csv?lang=en&amp;timezone=Europe%2FBrussels interval: PT1M . This will ensure we receive the actual state of our parking lots at regular time intervals, which may or may not have changed since the last time we checked. Note that we request the data as CSV but alternatively we could have used JSON or GeoJSON. We still need to configure an adapter to convert the received CSV message to linked data. We’ll do that next. ",
    "url": "/pipeline/advanced_conversion#to-push-or-to-pull-thats-the-question",
    
    "relUrl": "/pipeline/advanced_conversion#to-push-or-to-pull-thats-the-question"
  },"29": {
    "doc": "Advanced conversion",
    "title": "With a Little Help From Our Pirates",
    "content": "Now that we can get the actual state of our parking lots, we need to convert the source message in semicolon (;) separated CSV format to the linked data models we defined in the mapping. For this we can use a technology called RDF Mapping Language (RML). There are various ways to produce the mapping that we need: directly using linked data which defines the RML mapping rules or indirectly using a more human readable way named Yarrrml. Personally I prefer the real thing but using Matey may be more your thing. Explaining the RML technology is beyond the scope of this tutorial. The technology allows us to convert formats such as CSV, XML and JSON into complex RDF models. However, using it to create these complex models can be quite challenging. The solution is to do a straight forward mapping and create the structure we need in a second phase. Basically we map the properties of our source model one-to-one into an intermediate linked data model and then transform this intermediate model into our final model using another RDF technology (SPARQL construct), which is way easier to use for creating complex structures. We’ll do this in the next section. We start by creating a simple intermediate model where we already set the correct identity and entity type but map everything else as-is onto an intermediate vocabulary (temp: or https://temp.org/ns/advanced-compose# in full). | source | intermediate | . | name value | temp:name value | . | lastupdate value | temp:lastupdate value | . | type offStreetParkingGround | rdf:type mv:ParkingLot | . | openingtimesdescription value | temp:openingtimesdescription value | . | operatorinformation value | temp:operatorinformation value | . | freeparking value | temp:freeparking value | . | urllinkaddress id | id | . | numberofspaces value | temp:numberofspaces value | . | availablespaces value | temp:availablespaces value | . | occupation value | temp:occupation value | . | latitude value | temp:latitude value | . | longitude value | temp:longitude value | . To create a RML mapping file we need to write the RML rules in Turtle. All the Turtle prefixes should go at the start of the file but for simplicity we’ll add the prefixes as we go. Let’s start with the most common ones: . @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . @prefix rml: &lt;http://semweb.mmlab.be/ns/rml#&gt; . @prefix rr: &lt;http://www.w3.org/ns/r2rml#&gt; . @prefix ql: &lt;http://semweb.mmlab.be/ns/ql#&gt; . @prefix carml: &lt;http://carml.taxonic.com/carml/&gt; . Note that the last one is always needed for our RML adapter component (RmlAdaptor). Now, we start by defining the map which will contain our mapping rules. We define a prefix for our map and rules (:) and tell the RML component that we will be mapping CSV messages. Do not forget that all prefixes go at the start before our mapping and rules. @prefix : &lt;https://example.org/ns/tutorial/advanced-conversion#&gt; . :TriplesMap a rr:TriplesMap; rml:logicalSource [ a rml:LogicalSource; rml:source [ a carml:Stream ]; rml:referenceFormulation ql:CSV ]. Let’s continue now with defining the identity and type of our parking lots. Remember that for the identity we use the URL value and for the type we use parking lot. At the same time we’ll also add each parking lot in its own graph. Say what? We’ll learn about triples and graphs a bit later. For now, just remember that we want to handle each parking lot separately so we instruct the RML component to generate a stream of mv:ParkingLot entities, one for each row in the CSV. @prefix mv: &lt;http://schema.mobivoc.org/#&gt; . :TriplesMap rr:subjectMap [ rr:graphMap [ rr:template \"{urllinkaddress}\" ]; rml:reference \"urllinkaddress\"; rr:class mv:ParkingLot ]. Easy enough. No? Let’s continue with one property. We define a rule saying that the entity will have a property (predicate) named temp:name whose value (object) comes from the source property name. @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix temp: &lt;https://temp.org/ns/advanced-compose#&gt; . :TriplesMap rr:predicateObjectMap [ rr:predicate temp:name; rr:objectMap [ rml:reference \"name\" ] ]. Again, no rocket-science once you get used to the Turtle and RML syntax. Let’s do the other properties as well. We define a rule to map each source property value onto the intermediate property. However, to make our life a bit easier in the next step, where we convert the intermediate to the target model, we can already add the correct value types. :TriplesMap rr:predicateObjectMap [ rr:predicate temp:lastupdate; rr:objectMap [ rml:reference \"lastupdate\"; rr:datatype xsd:dateTime ] ], [ rr:predicate temp:openingtimesdescription; rr:objectMap [ rml:reference \"openingtimesdescription\" ] ], [ rr:predicate temp:operatorinformation; rr:objectMap [ rml:reference \"operatorinformation\" ] ], [ rr:predicate temp:freeparking; rr:objectMap [ rml:reference \"freeparking\"; rr:datatype xsd:integer ] ], [ rr:predicate temp:numberofspaces; rr:objectMap [ rml:reference \"numberofspaces\"; rr:datatype xsd:integer ] ], [ rr:predicate temp:availablespaces; rr:objectMap [ rml:reference \"availablespaces\"; rr:datatype xsd:integer ] ], [ rr:predicate temp:occupation; rr:objectMap [ rml:reference \"occupation\"; rr:datatype xsd:integer ] ], [ rr:predicate temp:latitude; rr:objectMap [ rml:reference \"latitude\"; rr:datatype xsd:double ] ], [ rr:predicate temp:longitude; rr:objectMap [ rml:reference \"longitude\"; rr:datatype xsd:double ] ]. All of the above results in the mapping to intermediate file. In order to make it available to the workbench container we need to use volume mapping again. However, becomes we know we’ll need an additional file for transforming the intermediate to the target format, we choose to map the directory containing the mapping file as a whole: . volumes: - ./workbench/config:/ldio/config:ro . We also need to change our workbench pipeline to use the above RML mapping file and to include the RML mapping component (RmlAdapter instead of the JsonLdAdapter used in the basic setup). Our workbench pipeline input component is now complete and looks like this: . input: name: be.vlaanderen.informatievlaanderen.ldes.ldio.LdioHttpInPoller config: url: https://data.stad.gent/api/explore/v2.1/catalog/datasets/real-time-bezetting-pr-gent/exports/csv?lang=en&amp;timezone=Europe%2FBrussels interval: PT1M adapter: name: be.vlaanderen.informatievlaanderen.ldes.ldi.RmlAdapter config: mapping: ./config/source-to-intermediate.ttl . ",
    "url": "/pipeline/advanced_conversion#with-a-little-help-from-our-pirates",
    
    "relUrl": "/pipeline/advanced_conversion#with-a-little-help-from-our-pirates"
  },"30": {
    "doc": "Advanced conversion",
    "title": "Using the Swiss Army Knife",
    "content": "Now that we have an intermediate model as linked data we can use a SPARQL component which allows us to query the values in our intermediate model and construct a target model. If we look at our intermediate model and the target model we see that we need to keep the identity and type of our parking lot, convert the other properties to a different namespace and for some properties introduce the required structure: . | intermediate | target | . | temp:name value | rdfs:label value | . | temp:lastupdate value | dct:modified value | . | rdf:type mv:ParkingLot | as-is | . | temp:openingtimesdescription value | schema:openingHoursSpecification [rdf:type schema:OpeningHoursSpecification; rdfs:label value] | . | temp:operatorinformation value | mv:operatedBy [rdf:type schema:Organization, dct:Agent; rdfs:label value] | . | temp:freeparking value | mv:price [rdf:type schema:PriceSpecification; mv:freeOfCharge value ] | . | id | mv:url id | . | temp:numberofspaces value | mv:capacity [rdf:type mv:Capacity; mv:totalCapacity value] | . | temp:availablespaces value | mv:capacity [rdf:type mv:RealTimeCapacity; mv:currentValue value] | . | temp:occupation value | mv:rateOfOccupancy value | . | temp:latitude value | geo:lat value | . | temp:longitude value | geo:long value | . So, let’s start with the an empty SPARQL construct query (which is similar to a SPARQL query but the result is a new RDF model not just some values). Again, we use Turtle to do this: . # TODO: add our prefixes here CONSTRUCT { # TODO: add our target model here } WHERE { # TODO: select the intermediate model values here } . Not too difficult to understand: in, the where part we select values from the intermediate model and put them in variables. We then use those variables to create the target model in the constructpart. Note that the casing of the words construct and where is not important. Let’s take a brief moment and look at the intermediate model. It is now linked data so we can look at this model as being a collection of id-property-value triples, where the id is the id of eack parking lot, the properties being the names in our temp: namespace and the values being values, obviously. In linked data we call this a triple. The S stands for subject (the identity of a thing), the P stands for predicate (a property identified by its unique full name, including the namespace) and the O stands for object (the value which can be literal or a reference to some other subject, with or without an actual identity). Conceptually, a triple is a way to represent a unidirectional, named relation between a subject and an object. In linked data we also define the concept of a graph, which is just a tag for a triple and as such basically a way of grouping a bunch of triples together. It has no implicit meaning. A graph can be named by having an URI which identifies it. There’s also one special unnamed graph (has no URI) which we call the default graph. When we add a graph part to a triple (SPO) we get a quad (SPOG). In fact, triples are just a special case of quads where the 4th component is the default graph. We can use graphs for many purposes, e.g. to identify the source of the triples, to group together all related entities, etc. But, we use graphs in our pipelines to split data containing multiple entities into a stream of entities that are processed one-by-one in the pipeline. Wow, let’s think about this for a moment: in linked data we model everything as a collection of (subject-predicate-object) triples. It allows us to look at SPARQL queries as being filters that select a subset of the triple collection. For example, if we need to select all the identities of our parking lots we can simply state that we look for all the triples for which the predicate is rdf:type and the object is mv:ParkingLot. The subjects of these triples are in fact what we search for: the identities. To express this in a SPARQL query we specify this as follows: . ?id &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://schema.mobivoc.org/#ParkingLot&gt; . The interesting part is the variable ?id. It represents each result in our query. To make it more readable and in order to not repeat the namespace in every subject, predicate and object, we can again use prefixes: . PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; PREFIX mv: &lt;http://schema.mobivoc.org/#&gt; ?id rdf:type mv:ParkingLot . Note that the syntax for our prefix definitions is slightly different: we use PREFIX instead of @prefix and there is no dot (.) at the end of the line. The full SPARQL query would be: . PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; PREFIX mv: &lt;http://schema.mobivoc.org/#&gt; SELECT ?id WHERE { ?id rdf:type mv:ParkingLot } . But we do not need the identities only. Instead we want to create a new collection of triples for each parking lot with the predicates changed to those needed by our target model. Let’s start with simply copying the triple that defines our parking lots and their update timestamp: . PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; PREFIX mv: &lt;http://schema.mobivoc.org/#&gt; PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; PREFIX temp: &lt;https://temp.org/ns/advanced-compose#&gt; CONSTRUCT { ?id a mv:ParkingLot . ?id dct:modified ?lastupdate . } WHERE { ?id rdf:type mv:ParkingLot . ?id temp:lastupdate ?lastupdate . } . Note that a is a short-hand notation of rdf:type. In the where part we look for each ?id which is a mv:ParkingLot and then we retrieve its value of temp:lastupdate as variable ?lastupdate. Now that we have found these we can use the variables to create a new set of triples listed under the construct part. Not too difficult, is it? . Our target model is a bit more structured that our intermediate model, so at times we need to introduce an intermediate relation to some structure. Take for example the capacity. In the model diagram, we see that a civic structure has a relation has capacity to a Capacity object that has a property total capacity. In linked data we model this as triples in this way: . &lt;civic-structure-id&gt; rdf:type schema:CivicStructure . &lt;civic-structure-id&gt; mv:capacity &lt;a-capacity&gt; . &lt;a-capacity&gt; rdf:type mv:Capacity . &lt;a-capacity&gt; mv:totalCapacity \"some-numeric-value\"^^xsd:integer . We see a very interesting difference between the civic structure and its capacity: a civic-structure has some unique identifier such as “https://example.org/id/civic-structure/parking-lot-1” but a capacity is something that does not exist on its own, it is part of the civic structure and does not have an identity of its own. In linked data we call this a blank node because we can represent a triple as two nodes connected by a directed arrow, where the start node is a subject, the arrow is the predicate and the end node is the object. A blank node is a node without an identity and can be both the source and the destination of one or more arrows, representing its relations aka. properties aka. predicates. Because a blank node has no identity, we can write the above a bit more condensed by dropping the meaningless &lt;a-capacity&gt; and separating the predicates of the same subject by a semi-colon (;) - formatted for clarity: . &lt;civic-structure-id&gt; rdf:type schema:CivicStructure ; mv:capacity [ rdf:type mv:Capacity ; mv:totalCapacity \"some-numeric-value\"^^xsd:integer ]. Note that [ ... ] now represents our capacity. Now that we have learned how to introduce structure in our target model we can create the complete mapping using SPARQL construct and we need to add this transformation step in the workbench pipeline: . transformers: - name: be.vlaanderen.informatievlaanderen.ldes.ldi.SparqlConstructTransformer config: query: ./config/intermediate-to-target.rq . In addition, as our target model has changed, we need to fix the transformation step which creates the version object to: . - name: be.vlaanderen.informatievlaanderen.ldes.ldi.VersionObjectCreator config: member-type: http://schema.mobivoc.org/#ParkingLot delimiter: \"/\" date-observed-property: &lt;http://purl.org/dc/terms/modified&gt; generatedAt-property: http://purl.org/dc/terms/modified versionOf-property: http://purl.org/dc/terms/isVersionOf . And also the definition of the LDES to reflect the correct target class: . @prefix mv: &lt;http://schema.mobivoc.org/#&gt; &lt;/occupancy&gt; a ldes:EventStream ; tree:shape [ a sh:NodeShape ; sh:targetClass mv:ParkingLot ] ; . Note that in the complete mapping when we query the properties from our source model that almost all of these query lines are wrapped by an optional { ... } construct. The reason for this is that any of these triples may be missing. Remember that the WHERE clause is in essence a filter on the collection of source triples, where each query line refines the subset of results from the previous query line. Therefore, if we do not use optional then the query returns no results and hence no target entity is constructed. ",
    "url": "/pipeline/advanced_conversion#using-the-swiss-army-knife",
    
    "relUrl": "/pipeline/advanced_conversion#using-the-swiss-army-knife"
  },"31": {
    "doc": "Advanced conversion",
    "title": "Enough Talk, Show Me the Members",
    "content": "Now that we have set everything up, we can launch our systems. We cannot launch both our LDES server and our workbench at the same time because we are now polling for the data and our workbench pipeline will start immediately. This is a problem because we first need to send the definition of our LDES and view to the LDES server. Actually, it takes our LDES server longer to start than our workbench, so we need to prevent launching the workbench until our LDES server is up and running and we have seeded our definitions. To prevent our workbench to launch when we bring all our other systems up, we can add a profile to the Docker compose file in the workbench service. The actual name of the profile does not matter but we use delay-started to clearly communicate the purpose: . ldio-workbench: container_name: advanced-conversion_ldio-workbench image: ghcr.io/informatievlaanderen/ldi-orchestrator:latest volumes: - ./workbench/config:/ldio/config:ro - ./workbench/application.yml:/ldio/application.yml:ro ports: - 9004:80 networks: - advanced-conversion profiles: - delay-started . To run the LDES server and its storage service (mongo), wait until its up and running, send definitions and then start the workbench: . clear docker compose up -d while ! docker logs $(docker ps -q -f \"name=ldes-server$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/ldes/admin/api/v1/eventstreams\" -d \"@./definitions/occupancy.ttl\" curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/ldes/admin/api/v1/eventstreams/occupancy/views\" -d \"@./definitions/occupancy.by-page.ttl\" docker compose up ldio-workbench -d while ! docker logs $(docker ps -q -f \"name=ldio-workbench$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done . To verify the LDES, view and data: . clear curl http://localhost:9003/ldes/occupancy curl http://localhost:9003/ldes/occupancy/by-page curl http://localhost:9003/ldes/occupancy/by-page?pageNumber=1 . The last URL will contain our members, looking something like this (limited to one member): . @prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix park-and-ride-pr: &lt;https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/&gt; . @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix terms: &lt;http://purl.org/dc/terms/&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt; . @prefix wgs84_pos: &lt;http://www.w3.org/2003/01/geo/wgs84_pos#&gt; . &lt;https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/pr-loopexpo#2023-12-13T12:21:21+01:00&gt; rdf:type &lt;http://schema.mobivoc.org/#ParkingLot&gt; ; rdfs:label \"P+R The Loop\" ; terms:isVersionOf park-and-ride-pr:pr-loopexpo ; terms:modified \"2023-12-13T12:21:21+01:00\"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ; &lt;http://schema.mobivoc.org/#capacity&gt; [ rdf:type &lt;http://schema.mobivoc.org/#Capacity&gt; ; &lt;http://schema.mobivoc.org/#totalCapacity&gt; 168 ] ; &lt;http://schema.mobivoc.org/#capacity&gt; [ rdf:type &lt;http://schema.mobivoc.org/#RealTimeCapacity&gt; ; &lt;http://schema.mobivoc.org/#currentValue&gt; 30 ] ; &lt;http://schema.mobivoc.org/#operatedBy&gt; [ rdf:type terms:Agent , &lt;http://schema.org/Organization&gt; ; rdfs:label \"Mobiliteitsbedrijf Gent\" ] ; &lt;http://schema.mobivoc.org/#price&gt; [ rdf:type &lt;http://schema.org/PriceSpecification&gt; ; &lt;http://schema.mobivoc.org/#freeOfCharge&gt; 1 ] ; &lt;http://schema.mobivoc.org/#rateOfOccupancy&gt; 82 ; &lt;http://schema.mobivoc.org/#url&gt; park-and-ride-pr:pr-loopexpo ; &lt;http://schema.org/openingHoursSpecification&gt; [ rdf:type &lt;http://schema.org/OpeningHoursSpecification&gt; ; rdfs:label \"24/7\" ] ; wgs84_pos:lat \"51.024483197\"^^&lt;http://www.w3.org/2001/XMLSchema#double&gt; ; wgs84_pos:long \"3.69519252261\"^^&lt;http://www.w3.org/2001/XMLSchema#double&gt; ... &lt;http://localhost:9003/ldes/occupancy&gt; rdf:type ldes:EventStream ; tree:member &lt;https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/pr-loopexpo#2023-12-13T12:21:21+01:00&gt;, ..... Note that every two minutes the pipeline will request the latest state of our parking lots and will create additional version objects. The identity of a member depends only on the lastupdate property of our parking lot. If that did not change for a parking lot then the pipeline will create a version object with an identical identity as before. Any such version object will be refused by the LDES server and a warning will be logged in the LDES server log. The new version objects are added to the LDES and become new members. ",
    "url": "/pipeline/advanced_conversion#enough-talk-show-me-the-members",
    
    "relUrl": "/pipeline/advanced_conversion#enough-talk-show-me-the-members"
  },"32": {
    "doc": "Advanced conversion",
    "title": "Every End is a New Beginning",
    "content": "You should now know some basics about linked data. You learned how to define a mapping from non-linked data to linked data using RML as well as how to transform a linked data model into a different linked data model. In addition, you learned that you can periodically pull data into a workbench pipeline to create a continuous stream of versions of the state of some system. You can now stop all the systems. To bring the containers down and remove the private network: . docker compose rm ldio-workbench --stop --force --volumes docker compose down . Note that to bring down the workbench we need to stop it and remove the container and associated volumes explicitly because we started it separately. ",
    "url": "/pipeline/advanced_conversion#every-end-is-a-new-beginning",
    
    "relUrl": "/pipeline/advanced_conversion#every-end-is-a-new-beginning"
  },"33": {
    "doc": "Advanced conversion",
    "title": "Advanced conversion",
    "content": " ",
    "url": "/pipeline/advanced_conversion",
    
    "relUrl": "/pipeline/advanced_conversion"
  },"34": {
    "doc": "Minimal workbench",
    "title": "Setting Up a Minimal LDIO Workbench",
    "content": "This quick start guide will show you how to setup a minimal workbench to create version objects from state objects. Please see the introduction for the example data set and pre-requisites, as well as an overview of all examples. ",
    "url": "/pipeline/minimal_workbench#setting-up-a-minimal-ldio-workbench",
    
    "relUrl": "/pipeline/minimal_workbench#setting-up-a-minimal-ldio-workbench"
  },"35": {
    "doc": "Minimal workbench",
    "title": "Show Me Your Workbench",
    "content": "The LDES Server allows to ingest version objects but typically the source data represents the state of an object and not a version in time of this state. That is why a data transformation is needed to create such a version object. Such a data transformation can be standalone or as part of a data transformation pipeline which can be build in various ways with many different data processing systems. One such a workbench that allows to create data pipelines is Apache NiFi. This is a mature and solid open-source solution that comes with many features, allows for horizontal scaling and comes with many standard processors for creating and monitoring complex data pipelines. However, it also comes with a steep learning curve and some other drawbacks. The Linked Data Interactions Orchestrator (LDIO is a simple and more light-weight solution that eases the process of creating more straightforward, linear data transformations while requiring minimal resources and attempting to keep the learning curve as low as possible. It is by no means a silver bullet but experience has learned us that most data publishing use cases can easy be covered with a simple linear pipeline and as such LDIO usually suffies. LDIO allows to create one or more synchronous linear pipelines that convert linked and non-linked data to version objects that can be ingested by an LDES Server. It is centered around the concept of one input source with an adaptor to convert to linked data, one or more in-memory transformation steps and sending the result to one or more output sinks. Various input components are available for starting a pipeline such as: accepting HTTP messages both using a push model (HTTP listener) and a pull model (HTTP poller), reading from Kafka, etc. If the source data is already linked data you can use a simple RDF adaptor which allows to parse various RDF serializations. If the source data is not yet linked data you can use either a JSON-LD adaptor to attach a JSON-LD context to a JSON message or alternatively a RML adaptor, which allows to create linked data from various other message formats, such as JSON, XML, CSV, etc. On the output side we also provide several possibilities such as POST-ing using HTTP, writing to Kafka, etc. We provide several transformation components for manipulating linked data but most data transformations can be done using a SPARQL construct transformation step. In addition, we also provide some components for more specific tasks such as enriching the data from some external HTTP source, converting GeoJson to Well Known Text (WKT), etc. All these components are provided as part of the LDIO workbench which is packaged as a Docker image. The Docker images are available on Docker Hub. The stable releases can be found here. ",
    "url": "/pipeline/minimal_workbench#show-me-your-workbench",
    
    "relUrl": "/pipeline/minimal_workbench#show-me-your-workbench"
  },"36": {
    "doc": "Minimal workbench",
    "title": "Configure Your First Pipeline",
    "content": "The example docker compose file only contains a LDIO service which runs in a private network and uses volume mapping to have its configuration file available in the container. As we will see in a minute, the pipeline starts with a HTTP listener and therefore we need a port mapping to allow the workbench to receive HTTP messages. The workbench configuration file starts with specifying the port on which the HTTP listener will accept requests. We have used the default port number 8080 and could have easily omitted it from te configuration. Other than that, we only need to specify the actual pipeline definition. Note that the workbench can contain more than one pipeline if needed. The pipeline definition starts with a name and a description. The latter is purely for documentation purposes, but the former is used as the base path on which the HTTP listener receives requests. In our case this is (based on the docker compose port mapping): http://localhost:9004/p+r-pipeline. After that the definition continues with the input component and associated adapter, the transformation steps and the output(s). Let’s look at these in more detail. The input component simply states that it is a HTTP listener which uses a RDF adaptor and as such is expecting Linked Data: . input: name: be.vlaanderen.informatievlaanderen.ldes.ldio.LdioHttpIn adapter: name: be.vlaanderen.informatievlaanderen.ldes.ldi.RdfAdapter . We need a transformation step to turn the linked data state object which we receive into a version object. We need to specify for which object type we need to change it to a version object. We use this type to retrieve that object’s identifier and create the version object ID based on this identifier concatenated with the delimiter and the value of the date-observed-property. We also use the identifier to add a property as specified by versionOf-property to the version object. Finally, we also use the date-observed-property value to add a property as defined by the generatedAt-property to the version object. This sounds way more complicated than it actually is as we will show later. transformers: - name: be.vlaanderen.informatievlaanderen.ldes.ldi.VersionObjectCreator config: member-type: https://example.org/ns/mobility#offStreetParkingGround delimiter: \"/\" date-observed-property: &lt;http://www.w3.org/ns/prov#generatedAtTime&gt; generatedAt-property: http://www.w3.org/ns/prov#generatedAtTime versionOf-property: http://purl.org/dc/terms/isVersionOf . Note that we used the http://www.w3.org/ns/prov#generatedAtTime property for both creating the version object ID as well as for the generatedAt-property. This will prevent creating another property with the same date/time value. Finally, the version object is output to the specified sink. For demo purposes we use a component that simply logs the member to the console, which for a Docker container results in its logs. outputs: - name: be.vlaanderen.informatievlaanderen.ldes.ldio.LdioConsoleOut . ",
    "url": "/pipeline/minimal_workbench#configure-your-first-pipeline",
    
    "relUrl": "/pipeline/minimal_workbench#configure-your-first-pipeline"
  },"37": {
    "doc": "Minimal workbench",
    "title": "Launch the Magic",
    "content": "After this long introduction let’s get our hands dirty and see the magic in action. To start the workbench and wait until it is available: . docker compose up -d while ! docker logs $(docker ps -q -f \"name=ldio-workbench$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done . There is no visual component yet for the LDIO workbench, but you can check its status at http://localhost:9004/actuator/health. ",
    "url": "/pipeline/minimal_workbench#launch-the-magic",
    
    "relUrl": "/pipeline/minimal_workbench#launch-the-magic"
  },"38": {
    "doc": "Minimal workbench",
    "title": "You’ve Got Mail",
    "content": "Now that the workbench is up and running we can send a message through the pipeline and see its version object outputted to the workbench logs. We use the following simple JSON-LD message (clipped to the relevant parts): . { \"@context\": { \"@vocab\": \"https://example.org/ns/mobility#\", \"urllinkaddress\": \"@id\", \"type\": \"@type\", \"lastupdate\": { \"@id\": \"http://www.w3.org/ns/prov#generatedAtTime\", \"@type\": \"http://www.w3.org/2001/XMLSchema#dateTime\" } }, \"lastupdate\": \"2023-11-30T21:45:15+01:00\", \"type\": \"offStreetParkingGround\", \"urllinkaddress\": \"https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/pr-gentbrugge-arsenaal\", ... } . Note: do not worry if you do not understand every detail in the context part. Its main purpose is to add meaning to the state object properties. To send the message into the pipeline: . curl -X POST -H \"Content-Type: application/ld+json\" \"http://localhost:9004/p+r-pipeline\" -d \"@./data/message.jsonld\" . Since it is a small and straight forward message the workbench log will almost immediately contain the version object. To watch the version object appear in the workbench log . docker logs -n 30 $(docker ps -q -f \"name=ldio-workbench$\") . You should see the following (clipped to the relevant parts): . @prefix mobility: &lt;https://example.org/ns/mobility#&gt; . @prefix park-and-ride-pr: &lt;https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/&gt; . @prefix prov: &lt;http://www.w3.org/ns/prov#&gt; . @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . @prefix terms: &lt;http://purl.org/dc/terms/&gt; . &lt;https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/pr-gentbrugge-arsenaal/2023-11-30T21:45:15+01:00&gt; rdf:type mobility:offStreetParkingGround ; terms:isVersionOf park-and-ride-pr:pr-gentbrugge-arsenaal ; prov:generatedAtTime \"2023-11-30T21:45:15+01:00\"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ; ... If you compare the generated member in the container log with the example that we process, you will notice that the state object ID (park-and-ride-pr:pr-gentbrugge-arsenaal or http://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/pr-gentbrugge-arsenaal in full) is a property (terms:isVersionOf or http://purl.org/dc/terms/isVersionOf in full) of the version object and the version ID (https://stad.gent/nl/mobiliteit-openbare-werken/parkeren/park-and-ride-pr/pr-gentbrugge-arsenaal/2023-11-30T21:45:15+01:00) is a combination of that state object ID, the delimiter (/) and the prov:generatedAtTime property (2023-11-30T21:45:15+01:00). ",
    "url": "/pipeline/minimal_workbench#youve-got-mail",
    
    "relUrl": "/pipeline/minimal_workbench#youve-got-mail"
  },"39": {
    "doc": "Minimal workbench",
    "title": "That’s All Folks",
    "content": "You now know how to configure a basic LDIO workbench which takes in RDF messages containing a single state object and turn it into a version object that can be ingested as a LDES member. To bring the containers down and remove the private network: . docker compose down . ",
    "url": "/pipeline/minimal_workbench#thats-all-folks",
    
    "relUrl": "/pipeline/minimal_workbench#thats-all-folks"
  },"40": {
    "doc": "Minimal workbench",
    "title": "Minimal workbench",
    "content": " ",
    "url": "/pipeline/minimal_workbench",
    
    "relUrl": "/pipeline/minimal_workbench"
  },"41": {
    "doc": "Setup a minimal LDES client",
    "title": "Setting Up a Minimal LDES Client",
    "content": "This tutorial will show you how to setup a minimal LDES CLient in order to replicate and synchronize an existing Linked Data Event Stream (LDES). Please see the introduction for the example data set and pre-requisites, as well as an overview of all examples. ",
    "url": "/consuming/consuming#setting-up-a-minimal-ldes-client",
    
    "relUrl": "/consuming/consuming#setting-up-a-minimal-ldes-client"
  },"42": {
    "doc": "Setup a minimal LDES client",
    "title": "LDES in a Nutshell",
    "content": "If you look at the Tree specification you will see that an LDES (which derives from this specification) is a collection of views. A view is in fact the first node of a tree of nodes containing members of the collection as well as relations to other nodes. Wow, let us take a moment here to reflect about this. So, we have a data collection containing collection members. The size of the collection is potentially very big so a LDES allows us to split the collection in pieces. Each view is in fact a collection of connected nodes which may contain a number of collection members as well as links to other nodes which allow us to traverse the whole connected structure. So, by following all the links in the nodes of a view we can collect all the members of the data collection. Each view is organized in some way: the nodes may together form a list, a tree or a graph. This organization comes into existence by a process we call fragmentation: creating a structure of nodes by adding zero or more relations (containing a link to another node) to each node as well as assigning a collection member to one or more nodes. As said, the LDES specification is based on the Tree specification and provides a way to keep the history of changes of a data collection. The result is a dynamic data collection and the members of a LDES are version objects instead of state objects. So, a version object is in essence a state object at some point in time. An LDES allows us to replicate the data collection in addition to synchronize with the changes that occur to this data collection after we have retrieved it. This allows us to keep in sync with the historical state of a system. ",
    "url": "/consuming/consuming#ldes-in-a-nutshell",
    
    "relUrl": "/consuming/consuming#ldes-in-a-nutshell"
  },"43": {
    "doc": "Setup a minimal LDES client",
    "title": "All The Things She Said",
    "content": "Suppose there is a public LDES available out there that you would like to use. We will assume that you know the URL of this LDES. If not, you will need to discover the URL by looking at some metadata catalog such as Metadata Vlaanderen. Once we have the URL of the LDES we can choose one of its views because as stated before each view allows us to retrieve all the members of the LDES by following all the links in the connected nodes. There is no point in following the links in multiple views of an LDES as we end up collecting the same members multiple times. OK, so we have a LDES URL (e.g. http://localhost:9003/ldes/occupancy) and we have selected a view URL by retrieving the LDES itself and selecting one of the tree:views of the ldes:EventStream, e.g. http://localhost:9003/ldes/occupancy/by-page. Now what? For sure we are not going to extract all the members from each node or get all the links to the other nodes by hand. Are we? There can be thousands or millions of nodes and members! Well, of course not. Lucky for you we have already done the hard part of creating a LDES Client, which is a freely available component that allows us to, given a LDES or a view, request the view node or any linked node, extract each member from the node and follow the links in each node in order to collect all members and in fact replicate the LDES. In addition, the LDES Client will also look at the node properties which are lokated in the HTTP headers and re-request the nodes that can change over time, both nodes structure and nodes containing members. By doing this we can synchronize the LDES. The LDES CLient component is packaged as a LDIO input component as well as a NiFi processor and comes as part of their respective docker images (LDIO workbench and Nifi workbench). Both components act as the initial part of a data pipeline and need sink component which receives the extracted collection members which as you already know are version objects. But if you do not need the history of state changes you need a way to revert these version objects back to their corresponding state object. This process of converting such a version object back to a state object we call version materialization, and as it happens we have a component laying around that you can use to do exactly that. Note: currently the LDES Client outputs a stream of version objects and you need to add a transformation step in your data pipeline to do version materialization. In the future we will likely add the option to immediately output state objects instead, or even default to that with the option to output the version objects in you do need the history. Stay tuned. No doubt you are thinking by now: ‘yeah, yeah, enough with that, just show me how!’ OK then, if you insist… . ",
    "url": "/consuming/consuming#all-the-things-she-said",
    
    "relUrl": "/consuming/consuming#all-the-things-she-said"
  },"44": {
    "doc": "Setup a minimal LDES client",
    "title": "Get Your Motor Runnin’",
    "content": "For the LDES we can simply use the parking lot example. For this LDES we know that there is only one view, i.e. http://localhost:9003/ldes/occupancy/by-page. Go ahead and open a command shell at the parking lot example and run it so that we have a LDES to work with. Done? Let’s continue then. Now what do we need for our minimal LDES Client example? Really not that much! We obviouly need a workbench (e.g. LDIO) containing one pipeline which starts with a LDES Client component and outputs the members to a sink. Unless we want to convert to state objects, we do not need a Version Materializer transformer. For the sink we can use a webhook such as https://webhook.site/ or https://public.requestbin.com/ or any other similar online or offline service (e.g. our message sink). You can use any sink for which we have an output component. We will be using the HTTP protocol in this tutorial so we can use a HTTP output component to output member to the sink where we can inspect. So, the input part of our pipeline will look something like this: . name: Ldio:LdesClient config: urls: - http://localhost:9003/ldes/occupancy/by-page . That’s all folks. Short and simple. Now for the output: . name: Ldio:HttpOut config: endpoint: https://webhook.site/287d6ad6-b339-42fc-8b5e-297714ac688c . It is as simple as the input part! . OK, OK, if you look at the workbench configuration you will notice that it is a bit more elaborate than the above: . - name: client-pipeline description: \"Requests all existing members from a public LDES server and keeps following it for changes, sending each member as-is to a webhook\" input: name: Ldio:LdesClient config: urls: - ${LDES_SERVER_URL} sourceFormat: application/n-quads outputs: - name: Ldio:HttpOut config: endpoint: ${SINK_URL} rate-limit: enabled: true max-requests-per-minute: ${MAX_REQUESTS_PER_MINUTE} . But that is because we made it a bit more generic by using environment variables for the LDES Client url and the HTTP output endpoint. In addition, we added the sourceFormat: application/n-quads to request the LDES nodes as N-quads instead of the default format JSON-LD because N-quads can be parsed way faster. Finally we also added the rate-limit section because depending on which online sink service you use, you may be limited into how many HTTP requests you can send to it (usually a free sink service is limited to a minimal amout, e.g. 50 requests/minute). But in the end let’s be honest: it is not that complicated. The docker compose file isn’t all that scary either. We just need one workbench service which looks like this: . ldio-workbench: container_name: basic-client_ldio-workbench image: ldes/ldi-orchestrator:2.0.0-SNAPSHOT # you can safely change this to the latest 1.x.y version environment: - LDES_SERVER_URL=${LDES_SERVER_URL:-http://localhost:9003/ldes/occupancy/by-page} - SINK_URL=${SINK_URL} - MAX_REQUESTS_PER_MINUTE=${MAX_REQUESTS_PER_MINUTE:-50} volumes: - ./application.yml:/ldio/application.yml:ro network_mode: \"host\" . We give the container a name and use a recent LDIO image from docker hub. We also define and pass the required environment variables and provide defaults for all but the sink URL because you will get a unique URL when you use an online service such as Webhook or Pipedream. We provide the workbench configuration to the container by volume mapping it (read-only). Hmmm, what is that last line network_mode: \"host\" all about? Well, as stated before, the links in the LDES nodes will be absolute URLs as defined by the ldes-server.host-name entry in the LDES Server configuration file which in this case is http://localhost:9003/ldes. Because we have exposed the LDES Server to the host system (basically your system) you can request the LDES view and all the nodes from your (host) system just fine. However, the localhost is a special name which always resolves to IP address 127.0.0.1 as it maps the the loopback adapter of your network card. Essentially, if using its own network, when the LDES Client tries to request a LDES node located on localhost it will get back it’s loopback address 127.0.0.1 instead of the IP address of the LDES Server. So, the trick we use here is to let the workbench containing our LDES Client share the network of your system (the host) allowing it to resolve to the host network stack and then because of the docker port mapping, your docker will do its magic and forward the HTTP request to the LDES Server container. This is a bit of hocus-pocus that we need in this tutorial because we run our LDES Server locally and do not expose it using a domain name. In real-life you will not have to do this. In fact, if you would use a LDES which is hosted on the web in this tutorial you could comment out or remove the last line. OK, and now for the missing link: where can you define the mandatory sink URL? Well, that again is docker magic. When you execute docker compose CLI commands docker looks for and uses a file named .env which should contain the required environment variables. Actually you can use any other file but then you have to pass the name and location of that file to the docker CLI command by additing the --env-file option, e.g. --env-file user.env. We have provided a .env file. You can fill in the sink URL there and run all docker CLI commands as-is or copy this file to some other file (e.g. user.env) and pass that file to the docker CLI commands as explained. This is the preferred way to make it explicit that you are providing environment variables which are not provided by default in the docker compose file. So let’s do that. Please open https://webhook.site/ in a browser windows and copy your unique URL. Copy the .env file to user.env and fill in your unique URL as the sink URL. Note that it is a good idea to open the sink in a browser tab which does not store cookies, e.g. Firefox’s private window, Chrome ingocnito tab, etc. just in case you hit a rate limit while trying to run this tutorial. In case you experience this issue you can simply close this browser window and open a new one. That way you will get a new fresh sink unaware of the previous rate limit issue. Now you can run the example using the following command (in a bash shell): . clear docker compose --env-file user.env up . Note that this will start the workbench in the shell and not as a deamon as we did not add the deamon option (-d) to the command. Once started you will see that the LDES Clients starts following the LDES view: . basic-client_ldio-workbench | 2024-01-23T20:07:16.870Z INFO 1 --- [ main] b.v.i.ldes.ldio.Application : Started Application in 6.121 seconds (process running for 6.823) basic-client_ldio-workbench | 2024-01-23T20:07:17.304Z INFO 1 --- [pool-6-thread-1] l.c.s.StartingTreeNodeFinder : determineStartingTreeNode for: http://localhost:9003/ldes/occupancy/by-page basic-client_ldio-workbench | 2024-01-23T20:07:17.685Z INFO 1 --- [pool-6-thread-1] l.c.s.StartingTreeNodeFinder : Parsing response for: http://localhost:9003/ldes/occupancy/by-page . Almost immediately you should see the members start appearing in the online sink. After a little bit, the sink stops receiving members. You have now succesfully replicated the data collection. However, our parking lot workbench polls the source system frequently and generates 5 new version objects (almost) on each run. So, every 2 minutes you can see these new versions appear in the sink. You are now synchronizing the data collection forever, that is, until the LDES Server system is stopped. ",
    "url": "/consuming/consuming#get-your-motor-runnin",
    
    "relUrl": "/consuming/consuming#get-your-motor-runnin"
  },"45": {
    "doc": "Setup a minimal LDES client",
    "title": "And Now, The End Is Near",
    "content": "Because we did not run the docker container as a deamon, in order to stop the container you need to press CTRL-C. To bring the containers down: . docker compose rm ldio-workbench --stop --force --volumes . Of course, you also need to bring down the LDES Server and related containers. Most webhook services will automatically delete the requests after some time but it is best to cleanup yourself. For webhook.site choose the Delete all requests... option under the More menu and confirm the removal of all requests. You can now close the browser window. ",
    "url": "/consuming/consuming#and-now-the-end-is-near",
    
    "relUrl": "/consuming/consuming#and-now-the-end-is-near"
  },"46": {
    "doc": "Setup a minimal LDES client",
    "title": "Setup a minimal LDES client",
    "content": " ",
    "url": "/consuming/consuming",
    
    "relUrl": "/consuming/consuming"
  },"47": {
    "doc": "Publishing a protected LDES",
    "title": "Publishing and Accessing a Protected LDES",
    "content": "This tutorial will show you how to protect a LDES in order to prevent unauthorized access to a proprietary (or a public) data collection. It will also show how to expose the available LDES Server API as well as add and expose some metadata (using DCAT). In addition it will show you how to access such a protected LDES. Please see the introduction for the example data set and pre-requisites, as well as an overview of all examples. ",
    "url": "/protected/publishing_protected_LDES#publishing-and-accessing-a-protected-ldes",
    
    "relUrl": "/protected/publishing_protected_LDES#publishing-and-accessing-a-protected-ldes"
  },"48": {
    "doc": "Publishing a protected LDES",
    "title": "I’ll Protect You From the Hooded Claw",
    "content": "The LDES Server allows you to ingest a data collection and offers one or more views which allows replicating the data collection in whole or a part of it. However, not all data collection can be made publicly available. You need to protect those data collection in some way to prevent unauthorized access. What you will typically do is configure some security system which expects a Data Client to identify itself (authentication) after which the security system verifies it the Data Client has access to the requested data (authorization). The LDES Server does not include such a security system because there are various ways for doing authentication anad authorization and no one-size fits all. You may want to protect the data simply with an API key (secret key shared with a Data Client), using OAuth2 (authorization only) and/or OpenID (based on OAuth2 adding authentication), etc. There are many open-source and commercial implementations available for these and other protocols, so we decided to not include this aspect into the LDES Server. This keeps the server lean and simple for serving open data collections. But, even if you have a open data collection you may want to add a security layer to it to keep usage statistics per Data Client, e.g. to enforce fair usage policies, etc. So, we will look into how you can protect a LDES using a simple API key per Data Client and how the Data Client can access such a LDES. This technique may also apply to a commercial data collection: as a Data Publisher you may also want to offer a part of your data to the public for publicity and marketing reasons but of course you will want to track its usage. We will show how you can protect a LDES and what changes are needed to retrieve such a LDES. We will use existing tutorials to kickstart our setup. To create and feed our LDES we can use the advanced conversion setup and to retrieve it we can use the minimal client. Later we will add a reverse proxy to shield the LDES Server from the outside and configure things in such a way that we have controlled access to both LDES Server administration and LDES replication &amp; synchronization. Obviously we will have to make changes on both the Data Publisher and the Data Client side. But first, let us setup the system without access limitation first to ensure everything works fine. As usual, we start by creating a docker compose file containing the services that we need. At the Data Publisher side we need a database for the LDES Server (ldes-mongodb), the LDES Server itself (ldes-server) and a workbench to feed the LDES Server (server-workbench). We can basically copy/paste the services from the advanced conversion docker compose file: . ldes-mongodb: container_name: protected-setup_ldes-mongodb image: mongo:latest ports: - 27017:27017 networks: - protected-setup ldes-server: container_name: protected-setup_ldes-server image: ldes/ldes-server:2.10.0-SNAPSHOT # you can safely change this to the latest 2.x.y version volumes: - ./ldes-server/application.yml:/application.yml:ro ports: - 9003:80 networks: - protected-setup depends_on: - ldes-mongodb environment: - MANAGEMENT_TRACING_ENABLED=false # TODO: remove this when pull-based tracing implemented - LDES_SERVER_HOST_NAME=${LDES_SERVER_HOST_NAME:-http://localhost:9003/ldes} server-workbench: container_name: protected-setup_server-workbench image: ldes/ldi-orchestrator:2.0.0-SNAPSHOT # you can safely change this to the latest 1.x.y version volumes: - ./server-workbench/config:/ldio/config:ro - ./server-workbench/application.yml:/ldio/application.yml:ro ports: - 9004:80 networks: - protected-setup profiles: - delay-started . Notes: . | for clarity we renamed the network as well as the container names | we also renamed the workbench in order to stress that this is the workbench which feeds the LDES Server | we moved the configuration files to organize the setup a bit | we added an environment variable LDES_SERVER_HOST_NAME to allow changing the ldes-server.host-name in the server configuration easier | . At the Data Client side we only need a workbench (client-workbench) which we can borrow from the minimal client docker compose file: . client-workbench: container_name: protected-setup_client-workbench image: ldes/ldi-orchestrator:2.0.0-SNAPSHOT # you can safely change this to the latest 1.x.y version environment: - LDES_SERVER_URL=${LDES_SERVER_URL:-http://localhost:9003/ldes/occupancy/by-page} - SINK_URL=${SINK_URL} - MAX_REQUESTS_PER_MINUTE=${MAX_REQUESTS_PER_MINUTE:-50} volumes: - ./client-workbench/application.yml:/ldio/application.yml:ro network_mode: \"host\" profiles: - delay-started . Notes . | we renamed the container and the service | we moved the configuration files | we added a profile to prevent the client workbench to start ahead of time | . Note that the client workbench uses the network of the host which is completely disconnected from the internal docker network used by the LDES Server and its database and workbench. The LDES Client component therefore needs to use the host name (localhost) and exposed server port (9003) to access the LDES. That is why we have configured both the LDES_SERVER_HOST_NAME and the LDES_SERVER_URL to start with http://localhost:9003/. What we have now is illustrated in the following system container diagram: . Fig. 1 - Unprotected setup . At this point we can run all the systems and verify that we receive the LDES members in the sink. Please open https://webhook.site/ in a browser windows and copy your unique URL. Copy the .env file to user.env and fill in your unique URL as the sink URL. Now start all systems using (in a bash shell): . clear # start and wait for the server and database systems docker compose up -d while ! docker logs $(docker ps -q -f \"name=ldes-server$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done # define the LDES and the view curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/ldes/admin/api/v1/eventstreams\" -d \"@./ldes-server/definitions/occupancy.ttl\" curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/ldes/admin/api/v1/eventstreams/occupancy/views\" -d \"@./ldes-server/definitions/occupancy.by-page.ttl\" # start and wait for the server workbench docker compose up server-workbench -d while ! docker logs $(docker ps -q -f \"name=server-workbench$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done # start and wait for the client workbench docker compose --env-file user.env up client-workbench -d while ! docker logs $(docker ps -q -f \"name=client-workbench$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done . Once you have verified that the members appear in the sink you can shutdown the systems and remove the private network for now using: . docker compose rm client-workbench --stop --force --volumes docker compose rm server-workbench --stop --force --volumes docker compose down . ",
    "url": "/protected/publishing_protected_LDES#ill-protect-you-from-the-hooded-claw",
    
    "relUrl": "/protected/publishing_protected_LDES#ill-protect-you-from-the-hooded-claw"
  },"49": {
    "doc": "Publishing a protected LDES",
    "title": "It’s a Well Kept Secret",
    "content": "Now that we have a unprotected but working setup we can make the necessary changes to enforce security. We will need to do a few things: . | add a reverse proxy that will accept the LDES requests on the server’s behalf, check authentication &amp; authorization and forward the request or return an access error | not expose the LDES Server outside of the internal docker network so that the only way to access it is through the reverse proxy | change the Data Client (and Data Publisher) configuration to retrieve the LDES through the reverse proxy | . The following illustration shows the setup after adding such a reverse proxy: . Fig. 2 - Protected setup . The first thing we need to asks ourselves is which endpoints we need to protect but before we can answer that question we need to know what endpoints are available. By default, the LDES Server does not make that immediately apparant but by exposing the so called Swagger UI we can make the available API visible. If we add the following to the server configuration and launch it, we should see it: . springdoc: swagger-ui: path: /admin/doc/v1/swagger urlsPrimaryName: base . Note that we expose our swagger UI on the ‘admin’ API so we can protect it together with the rest of the admin API later. The path allows us to define the endpoint where the API information is visualized and the urlsPrimaryName allows us to choose which collection of APIs are displayed by default when we browse to the swagger UI endpoint. Now, in order to see it, we need to launch the server again: . clear docker compose up -d while ! docker logs $(docker ps -q -f \"name=ldes-server$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done . Once started point your browser to http://localhost:9003/ldes/admin/doc/v1/swagger. You will be redirected to http://localhost:9003/ldes/admin/doc/v1/swagger-ui/index.html and in the top right corner you should see the base API collection selected and the base API displayed in the main window. When we look at this base API, we see that there is one endpoint for the ingest that expects a POST to an endpoint. Obviously we do not want anybody else than the server workbench to push members to our ingest endpoint so we need to disallow this through the reverse proxy. We can do that by disallowing POST requests through the reverse proxy but we need to ensure that we can still seed the LDES definitions, which also use POST requests towards the administrative API. Next we see that the LDES Server exposes an endpoint to retrieve metadata at the root / as well as the various data collections at /{collection-name} and their respective views at /{collection-name}/{view-name}, all using GET requests. Most likely we want the metadata to be publicly accessible so that our available data collections can be discovered. In other words, we want metadata crawlers to be able to retrieve the metadata (typically DCAT information) in an unsecured way so that we get some exposure for our data collections. Now, as for the collections and views themselves, we can setup the accessibility as required by our use cases. For this tutorial we will assume that only a few clients can access our LDES and view so we will protect them with an API key. In fact, we will assign one API key per client so we can distinguish them for statistical reasons (e.g. to enforce a fair use policy). Now, if we switch to the admin API (select admin in the top right dropdown) we see a lot more available endpoints for managing the LDES Server. Obviously, we want to protect the whole admin API (including the swagger UI) so that only we are allowed to manage it. We will do this by protecting it with yet another API key. Note that an API key is not a very secure way of protecting an API. If somebody gets a hold of it, it can be misused. Therefore you should keep it a secret and use HTTPS instead of HTTP communication to prevent somebody sniffing the network and gaining access to the API key. Currently we do not expose any metadata for our LDES. Without going into details of DCAT, we will simply add the metadata (catalog, LDES metadata and view metadata) to our LDES Server by means of the admin API. We have kept the DCAT itself to the bare minimum as that is beyond the scope of the tutorial. It will be sufficient for our purpose. To try this please run: . # upload LDES &amp; view definitions curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/ldes/admin/api/v1/eventstreams\" -d \"@./ldes-server/definitions/occupancy.ttl\" curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/ldes/admin/api/v1/eventstreams/occupancy/views\" -d \"@./ldes-server/definitions/occupancy.by-page.ttl\" # upload metadata definitions curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/ldes/admin/api/v1/dcat\" -d \"@./ldes-server/metadata/catalog.ttl\" curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/ldes/admin/api/v1/eventstreams/occupancy/dcat\" -d \"@./ldes-server/metadata/occupancy.ttl\" curl -X POST -H \"content-type: text/turtle\" \"http://localhost:9003/ldes/admin/api/v1/eventstreams/occupancy/views/by-page/dcat\" -d \"@./ldes-server/metadata/occupancy.by-page.ttl\" . Now you can get the full DCAT if you request the root http://localhost:9003/ldes. It is a mix of the metadata definitions which we uploaded and server generated data, resulting in something like this: . @prefix by-page: &lt;http://localhost:9003/ldes/occupancy/by-page/&gt; . @prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; . @prefix ldes: &lt;http://localhost:9003/ldes/&gt; . @prefix occupancy: &lt;http://localhost:9003/ldes/occupancy/&gt; . @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix terms: &lt;http://purl.org/dc/terms/&gt; . @prefix tree: &lt;https://w3id.org/tree/&gt; . ldes:occupancy rdf:type dcat:Dataset ; terms:conformsTo tree:specification , &lt;https://w3id.org/ldes/specification&gt; ; terms:description \"LDES containing the occupancy of the various park+rides in Ghent in real time\"@en ; terms:identifier \"http://localhost:9003/ldes/occupancy\"^^rdfs:Literal ; terms:title \"Real time occupancy P+R (Gent) as LDES\"@en . &lt;https://w3id.org/ldes/specification&gt; rdf:type terms:Standard . occupancy:by-page rdf:type rdfs:Resource . &lt;http://localhost:9003/ldes&gt; rdf:type dcat:Catalog ; terms:description \"Offers an overview of the dataset(s) and data service(s) needed for the tutorial 'Publishing And Accessing a Protected LDES'.\"@en ; terms:identifier \"c403cbbd-9e4d-47a2-8bb5-41a7642701ba\"^^rdfs:Literal ; terms:title \"Catalog for Publishing And Accessing a Protected LDES\"@en ; dcat:dataset ldes:occupancy ; dcat:service by-page:description . by-page:description rdf:type dcat:DataService ; terms:description \"Paged view for the occupancy of the various park+rides in Ghent in real time\"@en ; terms:identifier \"http://localhost:9003/ldes/occupancy/by-page\"^^rdfs:Literal ; terms:title \"Real time occupancy P+R (Gent) by page\"@en ; dcat:endpointDescription &lt;https://semiceu.github.io/LinkedDataEventStreams/&gt; ; dcat:endpointURL occupancy:by-page ; dcat:servesDataset ldes:occupancy . tree:specification rdf:type terms:Standard . &lt;https://semiceu.github.io/LinkedDataEventStreams/&gt; rdf:type rdfs:Resource . As said before, we want this metadata to be publicly available, while limiting access to the admin API only to ourselves and the LDES &amp; the view to a couple of well-known clients, all by means of a unique API key. You can create these keys using one of the free online GUID generators (e.g. https://www.uuidgenerator.net/guid) or a password generator (e.g. https://www.avast.com/random-password-generator), etc. Add a Reverse Proxy . First we need to add the reverse proxy service to the docker compose file: . reverse-proxy: image: nginx:stable container_name: protected-setup_reverse-proxy ports: - 9005:8080 volumes: - ./reverse-proxy/protect-ldes-server.conf:/etc/nginx/conf.d/protect-ldes-server.conf:ro depends_on: - ldes-server networks: - protected-setup . Here we chose a well-known freely available component that we can setup as a reverse proxy. There are many options out there open-source and commercial. The configuration is highly dependent on the component that you use but the principles are mostly the same: you allow or disallow access to some URL for some HTTP verbs (GET, HEAD, POST, etc) based on some conditions. We’ll be using these three API keys for demonstration purposes: . | API key | Purpose | . | admin-secret | admin | . | client-one-secret | client 1 | . | client-two-secret | client 2 | . This translates to a mapping for our reverse proxy. Next we define an API key validation location, meaning that for the URLs we need to protect we use this internal virtual URL for verifying the presence and validity of the given API key. Finally we add our access rules. As previously said we need to: . | allow access to metadata to the public | allow access to admin API (including the swagger API) only to administrators | allow access to the LDES, the view and all view nodes for registered clients only | do not allow access to the ingest endpoint (no POST to LDES endpoint allowed from outside) | . For this particular reverse proxy we end up with this configuration and can start the reverse proxy to test if the rules allow or disallow access correctly: . docker compose up reverse-proxy -d . We have setup the reverse proxy to remap the LDES Server endpoints (all based at /ldes) a bit. The reverse proxy serves: . | the metadata at / | the admin API at /admin | the LDES, the view and its nodes at /feed | . If we do not pass an API key we can retrieve only the metadata and not the LDES, the view, the admin API and the swagger UI: . clear curl -I http://localhost:9005/ curl -I http://localhost:9005/feed/occupancy curl -I http://localhost:9005/feed/occupancy/by-page curl -I http://localhost:9005/admin/api/v1/eventstreams . Public access is only allowed (HTTP 200) for the first call, all other calls are unauthenticated (HTTP 401). Note that we pass -I in order to only retrieve the headers, not the actual content. If we pass a client API key we can retrieve the metadata, the LDES and the view but we cannot use the admin API: . clear curl -I -H \"x-api-key: client-one-secret\" http://localhost:9005/ curl -I -H \"x-api-key: client-one-secret\" http://localhost:9005/feed/occupancy curl -I -H \"x-api-key: client-one-secret\" http://localhost:9005/feed/occupancy/by-page curl -I -H \"x-api-key: client-one-secret\" http://localhost:9005/admin/api/v1/eventstreams . All but the last call should succeed (HTTP 200) while the last one is forbidden (HTTP 403) because we are authenticated but not authorized to use the admin API. Note that we need to pass the API key using the header x-api-key: &lt;well-known-key&gt;. Finally, if we pass the admin API key all calls should be possible: . clear curl -I -H \"x-api-key: admin-secret\" http://localhost:9005/ curl -I -H \"x-api-key: admin-secret\" http://localhost:9005/feed/occupancy curl -I -H \"x-api-key: admin-secret\" http://localhost:9005/feed/occupancy/by-page curl -I -H \"x-api-key: admin-secret\" http://localhost:9005/admin/api/v1/eventstreams . Now all calls succeed. Great! . We need to verify one more rule: nobody (not even an administrator!) should we able to send data to the ingest endpoint of the LDES server: . clear curl -X POST -i -H \"content-type: text/turtle\" -d @./data/member.ttl http://localhost:9005/feed/occupancy curl -X POST -i -H \"content-type: text/turtle\" -d @./data/member.ttl http://localhost:9005/feed/occupancy -H \"x-api-key: client-one-secret\" curl -X POST -i -H \"content-type: text/turtle\" -d @./data/member.ttl http://localhost:9005/feed/occupancy -H \"x-api-key: admin-secret\" . All calls should fail with a forbidden (HTTP 403). Note that it would be better to return method not allowed (HTTP 405) but that seems to be a challenge in this specific reverse proxy configuration. One final thing to test is if we can POST to the admin endpoint, e.g. to define a LDES or add metadata. curl -X POST -d \"@./ldes-server/metadata/catalog.ttl\" -H \"content-type: text/turtle\" \"http://localhost:9005/admin/api/v1/dcat\" -H \"x-api-key: admin-secret\" . Should return an error similar to Resource of type: dcat-catalog with id: c403cbbd-9e4d-47a2-8bb5-41a7642701ba already exists. because we have already defined the catalog. But, this is great news because this means the reverse proxy let the request go through and the LDES Server returns an error response. Do Not Expose the LDES Server . Now that everything is working great we can simply remove (or comment out) the port mapping of the server in the docker compose file as we do not need and do not want any direct access to it: . ldes-server: container_name: protected-setup_ldes-server image: ldes/ldes-server:2.10.0-SNAPSHOT # you can safely change this to the latest 2.x.y version volumes: - ./ldes-server/application.yml:/application.yml:ro # ports: # - 9003:80 networks: - protected-setup depends_on: - ldes-mongodb environment: - MANAGEMENT_TRACING_ENABLED=false # TODO: remove this when pull-based tracing implemented - LDES_SERVER_HOST_NAME=${LDES_SERVER_HOST_NAME:-http://localhost:9003/ldes} . Access the LDES Server Through the Reverse Proxy . Once the LDES Server is not directly accessible anymore, we need to define some environment variables to use the reverse proxy instead: . LDES_SERVER_HOST_NAME=http://localhost:9005/feed LDES_SERVER_URL=http://localhost:9005/feed/occupancy . and we need to pass our user.env file to all our docker compose commands. Of course, we should not forget the most important part: configure the LDES Client to pass a API key when requesting the LDES nodes. In the client workbench we need to change the LDES CLient component configuration to include this API key: . input: name: Ldio:LdesClient config: urls: - ${LDES_SERVER_URL} sourceFormat: application/n-quads auth: type: API_KEY api-key-header: x-api-key api-key: client-two-secret . Show time! But first bring down all systems so we can start with a clean slate: . docker compose down . ",
    "url": "/protected/publishing_protected_LDES#its-a-well-kept-secret",
    
    "relUrl": "/protected/publishing_protected_LDES#its-a-well-kept-secret"
  },"50": {
    "doc": "Publishing a protected LDES",
    "title": "Putting It All Together",
    "content": "To launch all the systems and configure it all you can run the following: . clear # start and wait for the server and database systems docker compose --env-file user.env up -d while ! docker logs $(docker ps -q -f \"name=ldes-server$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done # upload LDES &amp; view definitions curl -X POST -H \"x-api-key: admin-secret\" -H \"content-type: text/turtle\" \"http://localhost:9005/admin/api/v1/eventstreams\" -d \"@./ldes-server/definitions/occupancy.ttl\" curl -X POST -H \"x-api-key: admin-secret\" -H \"content-type: text/turtle\" \"http://localhost:9005/admin/api/v1/eventstreams/occupancy/views\" -d \"@./ldes-server/definitions/occupancy.by-page.ttl\" # upload metadata definitions curl -X POST -H \"x-api-key: admin-secret\" -H \"content-type: text/turtle\" \"http://localhost:9005/admin/api/v1/dcat\" -d \"@./ldes-server/metadata/catalog.ttl\" curl -X POST -H \"x-api-key: admin-secret\" -H \"content-type: text/turtle\" \"http://localhost:9005/admin/api/v1/eventstreams/occupancy/dcat\" -d \"@./ldes-server/metadata/occupancy.ttl\" curl -X POST -H \"x-api-key: admin-secret\" -H \"content-type: text/turtle\" \"http://localhost:9005/admin/api/v1/eventstreams/occupancy/views/by-page/dcat\" -d \"@./ldes-server/metadata/occupancy.by-page.ttl\" # start and wait for the server workbench docker compose --env-file user.env up server-workbench -d while ! docker logs $(docker ps -q -f \"name=server-workbench$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done # start and wait for the client workbench docker compose --env-file user.env up client-workbench -d while ! docker logs $(docker ps -q -f \"name=client-workbench$\") 2&gt; /dev/null | grep 'Started Application in' ; do sleep 1; done . It all goes well (and it should!) you will see the LDES members appear in the sink. ",
    "url": "/protected/publishing_protected_LDES#putting-it-all-together",
    
    "relUrl": "/protected/publishing_protected_LDES#putting-it-all-together"
  },"51": {
    "doc": "Publishing a protected LDES",
    "title": "It’s Been a Long Day",
    "content": "We have shown you how to enable the swagger UI, how to provide metadata for your LDES views and, of course, how to access a protected LDES. In addition we have shown you how you can protect a LDES using a API key but if you require a stronger way of securing access have a look at other authentication and authorization mechanisms. The documentation explains how to configure the LDES client in case you need to access an OAuth2/OpenID protected LDES. Now that you have verified that the members appear in the sink you can shutdown the systems and remove the private network using: . docker compose rm client-workbench --stop --force --volumes docker compose rm server-workbench --stop --force --volumes docker compose down . ",
    "url": "/protected/publishing_protected_LDES#its-been-a-long-day",
    
    "relUrl": "/protected/publishing_protected_LDES#its-been-a-long-day"
  },"52": {
    "doc": "Publishing a protected LDES",
    "title": "Publishing a protected LDES",
    "content": " ",
    "url": "/protected/publishing_protected_LDES",
    
    "relUrl": "/protected/publishing_protected_LDES"
  }
}
